{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!\n",
        "\n",
        "We'll be doing the following in today's notebook:\n",
        "\n",
        "1. Task 1: Simple Assistant\n",
        "2. Task 2: Adding Tools\n",
        "  - Task 2a: Creating an Assistant with File Search Tool\n",
        "  - Task 2b: Creating an Assistant with Code Interpreter Tool\n",
        "  - Task 2c: Creating an Assistant with Function Calling Tool\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Colab Specific Instructions:\n",
        "\n",
        "To get started, please make a copy of this notebook using `File > Save a copy in Drive`\n",
        "\n",
        "![image](https://i.imgur.com/rNzMEfs.png)\n",
        "\n",
        "You will be expected to submit a GitHub link to the completed notebook, so if you're completing the assignment on Colab - you'll want to download the notebook when you have completed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePayyL6at6LS",
        "outputId": "4fb742b3-c650-44c2-8c58-2d83c81a766b"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "9c567820-9633-44fd-dad6-e7dd1ca46d97"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Task 1: Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### ðŸ—ï¸ Build Activity ðŸ—ï¸\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"nisargs_assistant\" # @param {type: \"string\"}\n",
        "instructions = \"You're a cool guy who isnt afraid of anything\" # @param {type: \"string\"}\n",
        "model = \"gpt-4o\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistant\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "86ecb682-a04d-4687-94d3-18a4b99ffc07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_tqFy7M0pW3x5XirM3b3Rvt6u', created_at=1718052328, description=None, instructions=\"You're a cool guy who isnt afraid of anything\", metadata={}, model='gpt-4o', name='nisargs_assistant', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "6ba2a680-a595-40e8-fa5d-5784b9614b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_NpNdSaoXIQmgwaRfFsjJgnhO', created_at=1718052776, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"How old are you?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "dfde63a0-3b6f-4f01-b76c-ec84baefe5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_endj4Xo1wsRlnqmltknTOPCq', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How old are you?'), type='text')], created_at=1718085028, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_NpNdSaoXIQmgwaRfFsjJgnhO')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### ðŸ—ï¸ Build Activity ðŸ—ï¸\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = \"BE COOL\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "56aa670d-8f22-4ef4-b5f4-e568c6f7cefb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Run(id='run_Kvw9U3XqjnrnEcC3xYYKKvNa', assistant_id='asst_tqFy7M0pW3x5XirM3b3Rvt6u', cancelled_at=None, completed_at=None, created_at=1718085178, expires_at=1718085778, failed_at=None, incomplete_details=None, instructions='BE COOL', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_NpNdSaoXIQmgwaRfFsjJgnhO', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={}, parallel_tool_calls=True)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "89f3df27-d5a3-4095-9422-6271db52cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n"
          ]
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "3916ed59-7eba-471f-9eb7-d8d2f4658012"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_SCb27gaBDiHRwSGemJypQQOR', assistant_id='asst_tqFy7M0pW3x5XirM3b3Rvt6u', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"I don't have a specific birthday, but I was first introduced in October 2021. So, you can consider me fairly recent! If you have any questions or need assistance, feel free to ask.\"), type='text')], created_at=1718085179, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_Kvw9U3XqjnrnEcC3xYYKKvNa', status=None, thread_id='thread_NpNdSaoXIQmgwaRfFsjJgnhO')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9_V_XlaVH7"
      },
      "source": [
        "### Streaming Our Runs\n",
        "\n",
        "With recent upgrades to the Assistant API - we can now *stream* our outputs!\n",
        "\n",
        "In order to do this - we'll need something called an `EventHandler` which will help us to decide on what actions to take based on the output of the LLM.\n",
        "\n",
        "Let's build it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YvH-FUbQawcN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBieRYvbAmt"
      },
      "source": [
        "Now we can create our `run` and stream the output as it comes in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ax_0ZfbF_I",
        "outputId": "1bbddea1-a145-4a0d-b662-73f7703a511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > I don't have a specific birthday, but I was first introduced in October 2021. So, you can consider me fairly recent! If you have any questions or need assistance, feel free to ask."
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=EventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Task 2: Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Task 2a: Creating an Assistant with the File Search Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the File Search tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data to Vector Store\n",
        "\n",
        "1. First, we need some data.\n",
        "2. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAHBszIEa1Y",
        "outputId": "c9e86f6b-89b5-426b-ecd8-a6d23cbea92b"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our file to our Vector Store!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/file-search/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6UgUYNTaOpUK"
      },
      "outputs": [],
      "source": [
        "vector_store = client.beta.vector_stores.create(name=\"Paul Graham Essay Compilation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "file_paths = [\"paul_graham_essays.txt\"]\n",
        "file_streams = [open(path, \"rb\") for path in file_paths]\n",
        "\n",
        "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "  vector_store_id=vector_store.id, files=file_streams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_batch` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zJrVLkMpFgwf"
      },
      "outputs": [],
      "source": [
        "while file_batch.status != \"completed\":\n",
        "  time(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Your first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.create(\n",
        "  name=name,\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"file_search\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AhXXOXp1eybd"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.update(\n",
        "  assistant_id=fs_assistant.id,\n",
        "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JM9iJhoMcUfW"
      },
      "outputs": [],
      "source": [
        "fs_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What did Paul Graham say about Silicon Valley?\",\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "We can use an extension of the `EventHandler` that we created above to stream our `run`!\n",
        "\n",
        "Let's add a few things:\n",
        "\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "av5A_tm0bpvf"
      },
      "outputs": [],
      "source": [
        "class FSEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_message_done(self, message) -> None:\n",
        "    message_content = message.content[0].text\n",
        "    annotations = message_content.annotations\n",
        "    citations = []\n",
        "    for index, annotation in enumerate(annotations):\n",
        "      message_content.value = message_content.value.replace(\n",
        "        annotation.text, \n",
        "        f\"[{index}]\"\n",
        "      )\n",
        "      if file_citation := getattr(annotation, \"file_citation\", None):\n",
        "        cited_file = client.files.retrieve(file_citation.file_id)\n",
        "        citations.append(f\"{cited_file.filename}\")\n",
        "        # citations.append(f\"[{index}] {cited_file.filename}\")\n",
        "\n",
        "    print(message_content.value)\n",
        "    print(\"\\n\".join(set(citations)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "b33c4c2d-e8a7-432c-b596-a2169c487f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > Paul Graham attributes the success of Silicon Valley to several key factors:\n",
            "\n",
            "1. **Top Universities**: The presence of leading universities like Stanford and UC Berkeley plays a crucial role in fostering talent and innovation.\n",
            "2. **Risk-Taking Culture**: Silicon Valley has a unique culture that encourages taking risks and tolerates failure, which is essential for innovation.\n",
            "3. **Venture Capital**: The region has a high concentration of venture capital, which provides the necessary funding for startups to grow.\n",
            "4. **Critical Mass of Talent**: Having a large number of like-minded individuals who are passionate about technology and entrepreneurship creates a synergistic environment that fosters innovation[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19].\n",
            "paul_graham_essays.txt\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fs_thread.id,\n",
        "  assistant_id=fs_assistant.id,\n",
        "  event_handler=FSEventHandler(),\n",
        "  temperature=0.4,  # Lower temperature for less randomness\n",
        "  top_p=0.4,        # Lower top-p for less diversity\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Task 2b: Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.create(\n",
        "  name=name + \"+ Code Interpreter\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IPHZswUg6RH"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/ali-ce/datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_a8BLVdog1X8"
      },
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(\"datasets/Y-Combinator/Startups.csv\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "ci_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What kind of file is this?\",\n",
        "      \"attachments\": [\n",
        "          {\n",
        "              \"file_id\" : file.id,\n",
        "              \"tools\" : [{\"type\" : \"code_interpreter\"}]\n",
        "          }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "We'll once again need to create an `EventHandler`, except this time it will have an `on_tool_call_delta` method which will let us see the output of the code interpreter tool as well!\n",
        "\n",
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "class CIEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  def on_tool_call_delta(self, delta, snapshot):\n",
        "    if delta.type == 'code_interpreter':\n",
        "      if delta.code_interpreter.input:\n",
        "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "      if delta.code_interpreter.outputs:\n",
        "        print(f\"\\n\\noutput >\", flush=True)\n",
        "        for output in delta.code_interpreter.outputs:\n",
        "          if output.type == \"logs\":\n",
        "            print(f\"\\n{output.logs}\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPiivot-n5A8"
      },
      "source": [
        "Once again, we can use the streaming interface thanks to creating our `EventHandler`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmBqFfiSvT",
        "outputId": "dbfed50c-4e66-4a01-e36a-9651be586be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > code_interpreter\n",
            "\n",
            "import magic\n",
            "\n",
            "# Determine the file type\n",
            "file_path = '/mnt/data/file-y8FRBrppRgidVspYWLjaPZ23'\n",
            "file_type = magic.from_file(file_path, mime=True)\n",
            "file_type\n",
            "assistant > It seems that the `magic` library is not available in the current environment. I will use an alternative method to determine the file type. Let's read the first few bytes of the file to identify its type.def get_file_signature(file_path, num_bytes=20):\n",
            "    with open(file_path, 'rb') as file:\n",
            "        signature = file.read(num_bytes)\n",
            "    return signature\n",
            "\n",
            "# Get the file signature\n",
            "file_signature = get_file_signature(file_path)\n",
            "file_signature# Define the file path again\n",
            "file_path = '/mnt/data/file-y8FRBrppRgidVspYWLjaPZ23'\n",
            "\n",
            "# Function to get the file signature\n",
            "def get_file_signature(file_path, num_bytes=20):\n",
            "    with open(file_path, 'rb') as file:\n",
            "        signature = file.read(num_bytes)\n",
            "    return signature\n",
            "\n",
            "# Get the file signature\n",
            "file_signature = get_file_signature(file_path)\n",
            "file_signature\n",
            "assistant > The file signature suggests that it starts with the text \"Company,Satus,Year F\". This pattern is indicative of a CSV (Comma-Separated Values) file, which typically starts with column headers.\n",
            "\n",
            "To confirm, I'll read a bit more of the file to examine its contents.# Read the first few lines of the file to confirm its type\n",
            "def read_file_head(file_path, num_lines=5):\n",
            "    with open(file_path, 'r') as file:\n",
            "        lines = [file.readline().strip() for _ in range(num_lines)]\n",
            "    return lines\n",
            "\n",
            "# Read the first few lines\n",
            "file_head = read_file_head(file_path)\n",
            "file_head\n",
            "assistant > The content of the file confirms that it is indeed a CSV (Comma-Separated Values) file. The first line contains the column headers, and the subsequent lines contain data entries.\n",
            "\n",
            "Here are the first few lines of the file:\n",
            "\n",
            "1. `Company,Satus,Year Founded,Mapping Location,Description,Categories,Founders,Y Combinator Year,Y Combinator Session,Investors,Amounts raised in different funding rounds,Office Address,Headquarters (City),Headquarters (US State),Headquarters (Country),Logo,Seed-DB / Mattermark Profile,Crunchbase / Angel List Profile,Website`\n",
            "2. `Curebit,Operating,2010,San Francisco - California - USA,Talkable is a social referral platform for online stories and subscriptions.,\"E-Commerce, Analytics, Internet, Marketing, Social Media\",\"Allan Grant, Dominic Coryell, Jeff Yee, Nori Yoshida\",2011,Winter,\"500 Startups, Dharmesh Shah, Alex Lloyd, Auren Hoffman, Gordon Tucker, Stage One Capital, reinmkr inc., Karl Jacob, Y Combinator\",\"$1200000, undisclosed amount\",\"290 Division St, #405, San Francisco, California, USA\",San Francisco,California,USA,\"http://a5.images.crunchbase.com/image/upload/c_pad,h_98,w_98/v1407434890/sf19b2lsipbo6fazmjzq.png\",http://www.seed-db.com/companies/view?companyid=102020,http://www.crunchbase.com/organization/curebit,https://www.talkable.com`\n",
            "3. `Goldbely,Operating,,San Francisco - California - USA,Goldbely is an e-commerce site for buying and discovering America's best handmade gourmet food & gifts.,\"Social, E-Commerce\",\"Joe Ariel, Joel Gillman, Trevor Stow, Vanessa Torrivilla\",2013,Winter,\"500 Startups, Funders Club, Dave McClure, Tim Draper, Intel Capital, ACE & Company, Y Combinator\",\"$3,000,000\",\"1 Bluxome Street, San Francisco, California, USA\",San Francisco,California,USA,\"http://a3.images.crunchbase.com/image/upload/c_pad,h_98,w_98/v1397180367/99717257bb2a360419f75b586503af93.png\",http://www.seed-db.com/companies/view?companyid=729001,http://www.crunchbase.com/organization/goldbely,http://www.goldbely.com`\n",
            "4. `theDailyMuse,Operating,2011,New York City - New York - USA,The Daily Muse is a job search platform offering multimedia profiles of various companies that are hiring people.,,\"Alex Cavoulacos, Kathryn Minshew, Melissa McCreery\",2012,Winter,\"500 Startups, Great Oaks Venture Capital, Great Oaks Venture Capital, Thomas D. Lehrman, Adam Quinton, Y Combinator\",\"$2800000, $2100000, $1200000, $1200000\",\"33 WEST 26TH STREET, #2, New York, New York, USA\",New York City,New York,USA,\"http://a5.images.crunchbase.com/image/upload/c_pad,h_98,w_98/v1398232938/ffck9mcohrtktbkj3oie.jpg\",http://www.seed-db.com/companies/view?companyid=96040,http://www.crunchbase.com/organization/the-daily-muse,http://www.thedailymuse.com`\n",
            "5. `Zencoder,Exited,,San Francisco - California - USA,Zencoder is web-based video encoding SaaS designed to convert any video into web and mobile-compatible formats in real-time.,\"SaaS, Cloud Computing, Audio, Web Development, Data, Security, Video, Software\",\"Brandon Arbini, Jon Dahl, Steve Heffernan\",2010,Winter,\"500 Startups, Matt Cutts, Ignition Partners, James Lindenbaum, Orion Henry, Neil McClements, Wolfgang Buehler, Justin Yoshimura, Mike Bollinger, Adam Wiggins, SV Angel, Founder Collective, Robert Weber, Lowercase Capital, Andreessen Horowitz, Y Combinator\",\"$2000000, undisclosed amount\",\"San Francisco, CA, 94103, USA\",San Francisco,California,USA,\"http://a1.images.crunchbase.com/image/upload/c_pad,h_98,w_98/v1397206653/62e91e3f1c95f92912be5bd26638ea60.png\",http://www.seed-db.com/companies/view?companyid=98019,http://www.crunchbase.com/organization/zencoder,http://zencoder.com`\n",
            "\n",
            "Would you like to perform any specific operations or analyses on this CSV file?"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=ci_thread.id,\n",
        "  assistant_id=ci_assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=CIEventHandler(),\n",
        "  temperature=0.5,  # Lower temperature for less randomness\n",
        "  top_p=0.9        # Lower top-p for less diversity\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Task 2c: Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "1712c586-0c40-410c-e2e6-07ea7c918c54"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up serpapi_serach as ddg_serach is generating repeating words and letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages (from requests->google-search-results) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages (from requests->google-search-results) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages (from requests->google-search-results) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages (from requests->google-search-results) (2024.2.2)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=071f9869c38b0df78d380810b9e7de116cde4cc8e8dbf2b47f767ea76f3e6b81\n",
            "  Stored in directory: /home/nisargvp/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "# !pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"SERP_API_KEY\"] = getpass(\"SERP_API_KEY:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def serpapi_search(query):\n",
        "    # Set up your SerpAPI key\n",
        "    api_key = os.environ[\"SERP_API_KEY\"]  # Replace with your SerpAPI key\n",
        "    \n",
        "    # Define search parameters\n",
        "    search_params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": api_key,\n",
        "        \"num\": 5  # Requesting top 5 results\n",
        "    }\n",
        "    \n",
        "    # Create the search object\n",
        "    search = GoogleSearch(search_params)\n",
        "    \n",
        "    # Perform the search and get the results dictionary\n",
        "    results = search.get_dict()\n",
        "    \n",
        "    # Check for the 'organic_results' in the response\n",
        "    if \"organic_results\" in results:\n",
        "        organic_results = results[\"organic_results\"]\n",
        "        \n",
        "        # Extract and format the top 5 results\n",
        "        top_results = [result[\"snippet\"] for result in organic_results[:5] if \"snippet\" in result]\n",
        "        return \"\\n\".join(top_results)\n",
        "    else:\n",
        "        return \"No results found.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Harmanpreet Kaur is current captain of the women's team and Rohit Sharma is current captain of men's team in all formats. Contents. 1 Men's. 1.1 Test; 1.2 ODI ...\\nIndia national cricket team ; Association, Board of Control for Cricket in India ; Captain, Rohit Sharma ; Coach, Rahul Dravid ; ICC region, ACC ...\\nCurrent captain of Indian cricket team is Rohit Sharma. He is captaining Indian in all formats. Test , odi , t20i.\\nRohit Sharma will lead the Indian team at the T20 World Cup in USA-West Indies, BCCI secretary Jay Shah said. It will be Rohit Sharma and ...\""
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "serpapi_search(\"Who is the current captain of the Indian Cricket Team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "ea9bb779-f7c3-49e5-b5d4-c12be875a460"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ODI[edit] This is a list of cricketers who have captained the Indian men\\'s cricket team for at least one ODI. 27 players have captained the Indian men\\'s team in ODIs of which MS Dhoni is the most successful captain of men\\'s cricket team with 110 wins. Kapil Dev captained India to win in the 1983 Cricket World Cup, the first ever ICC trophy and ...\\nToday\\'s crossword puzzle clue is a general knowledge one: Current captain of the India national cricket team. We will try to find the right answer to this particular crossword clue. Here are the possible solutions for \"Current captain of the India national cricket team\" clue. It was last seen in British general knowledge crossword. We have 1 ...\\nVirat Kohli has stepped down as captain of India men\\'s Test team. The 33-year-old, who stopped leading India\\'s limited-overs sides last year, had led the Test side since 2015. He captained India ...\\nAnswers for Current captain of the India national cricket team (5,6) crossword clue, 11 letters. Search for crossword clues found in the Daily Celebrity, NY Times, Daily Mirror, Telegraph and major publications. Find clues for Current captain of the India national cricket team (5,6) or most any crossword answer or clues for crossword answers.\\n2008 Pakistan. ICC Under-19 Cricket World Cup. Runner-up. 2006 Sri Lanka. Source: ESPNcricinfo, 14 March 2024. Rohit Gurunath Sharma (born 30 April 1987) is an Indian international cricketer who currently captains the India national cricket team across all formats. He is a right-handed batsman. Considered one of the best batsmen of his ...'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Indian Cricket Team?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [],
      "source": [
        "serp_function = {\n",
        "    \"name\" : \"serpapi_search\",\n",
        "    \"description\" : \"Answer non-technical questions.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions.\",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "####â“ Question\n",
        "\n",
        "Why does the description key-value pair matter? `1. To understand what the function does, 2. Specify the function to use and hence improving function discovery, 3. Ouput error handling by explicit mention of required inputs`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [],
      "source": [
        "fc_assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : serp_function #ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiF_gXksaa"
      },
      "source": [
        "Now we can create our thread, and attach our message to it - just as we've been doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "CtELZ5wFjdo_"
      },
      "outputs": [],
      "source": [
        "fc_thread = client.beta.threads.create()\n",
        "fc_message = client.beta.threads.messages.create(\n",
        "  thread_id=fc_thread.id,\n",
        "  role=\"user\",\n",
        "  content=\"Can you describe the Twitter beef between Elon and LeCun?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KT_dj2YkwyK"
      },
      "source": [
        "Once again, we'll need an `EventHandler` for the streaming response - notice that this time we're utilizing a few new methods:\n",
        "\n",
        "1. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "2. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "To be very clear and explicit - we'll be following this pattern:\n",
        "\n",
        "1. Make a call to the LLM which will decide if a local function call is required.\n",
        "2. If a local function call is required - send a response that indicates a local function call is required.\n",
        "3. Call the function using the arguments provided by the LLM.\n",
        "4. Return the response from the function call with LLM provided arguements.\n",
        "5. Receive a response based on the output of the functional call from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "qgXhMEHbizEX"
      },
      "outputs": [],
      "source": [
        "class FCEventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_event(self, event):\n",
        "        # Check if the event requires an action, which will include tool calls\n",
        "        if event.event == 'thread.run.requires_action':\n",
        "            run_id = event.data.id  # Retrieve the run ID from the event data\n",
        "            self.handle_requires_action(event.data, run_id)\n",
        "\n",
        "    def handle_requires_action(self, data, run_id):\n",
        "        tool_outputs = []\n",
        "\n",
        "        # Iterate over the tool calls to handle the search functionality\n",
        "        for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
        "            print(f\"Tool function arguments: {tool.function.arguments}\")\n",
        "\n",
        "            if tool.function.name == \"serpapi_search\":\n",
        "                # If arguments is a string, assume it is the query directly\n",
        "                query = tool.function.arguments\n",
        "                if isinstance(query, str):\n",
        "                    # Perform the search and capture the output\n",
        "                    search_results = serpapi_search(query)\n",
        "                    tool_outputs.append({\"tool_call_id\": tool.id, \"output\": search_results})\n",
        "                else:\n",
        "                    print(\"Expected a string for query, but got:\", type(query))\n",
        "\n",
        "        # Submit all collected tool outputs\n",
        "        self.submit_tool_outputs(tool_outputs, run_id)\n",
        "\n",
        "    def submit_tool_outputs(self, tool_outputs, run_id):\n",
        "        # Ensure correct thread and run context is used for submission\n",
        "        with client.beta.threads.runs.submit_tool_outputs_stream(\n",
        "            thread_id=self.current_run.thread_id,\n",
        "            run_id=self.current_run.id,\n",
        "            tool_outputs=tool_outputs,\n",
        "            event_handler=EventHandler(),\n",
        "        ) as stream:\n",
        "            for text in stream.text_deltas:\n",
        "                print(text, end=\"\", flush=True)\n",
        "            print()  # Print a new line after all deltas are printed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Gzz4OZnxCQ"
      },
      "source": [
        "Thanks to our event handler - we can stream this process in our notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGInY8fjYNa",
        "outputId": "b6fc88c6-4e58-46f2-c21a-0869fc208c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool function arguments: {\"query\":\"Twitter beef between Elon Musk and Yann LeCun\"}\n",
            "\n",
            "assistant > TheThe Twitter Twitter beef beef between between Elon Elon Musk Musk and and Yann Yann Le LeCCunun,, Meta Meta's's Chief Chief AI AI Scientist Scientist,, primarily primarily revolves revolves around around their their differing differing views views on on AI AI and and technology technology development development.. Here's Here's a a quick quick rundown rundown:\n",
            "\n",
            ":\n",
            "\n",
            "11.. ** **ContextContext****:\n",
            ":\n",
            "     - - Yann Yann Le LeCCunun,, a a recognized recognized AI AI expert expert with with numerous numerous accolades accolades,, expressed expressed his his opinions opinions on on Elon Elon Musk Musk's's stance stance on on AI AI and and his his treatment treatment of of scientists scientists.\n",
            "\n",
            ".\n",
            "\n",
            "22.. ** **TheThe Beef Beef Starts Starts****:\n",
            ":\n",
            "     - - Yann Yann Le LeCCunun critiques critiques Elon Elon Musk Musk's's approach approach,, particularly particularly emphasizing emphasizing that that technology technology and and product product development development might might not not necessarily necessarily need need the the openness openness and and publications publications that that advance advance scientific scientific research research.\n",
            "\n",
            ".\n",
            "\n",
            "33.. ** **ElElon'son's Reaction Reaction****:\n",
            ":\n",
            "     - - Elon Elon Musk Musk,, known known for for his his outspoken outspoken nature nature,, def defendsends his his perspective perspective and and promotes promotes his his visions visions,, such such as as the the X X.A.AII initiative initiative,, encouraging encouraging collaboration collaboration in in his his typical typical assert assertiveive style style.\n",
            "\n",
            ".\n",
            "\n",
            "44.. ** **LeLeCCunun Respond Respondss****:\n",
            ":\n",
            "     - - Le LeCCunun continues continues to to troll troll Musk Musk,, pointing pointing out out inconsist inconsistenciesencies or or flaws flaws in in Musk Musk's's arguments arguments or or methods methods.. Given Given Le LeCCunun's's extensive extensive experience experience and and credentials credentials,, he he lever leveragesages his his authority authority in in the the field field to to underscore underscore his his points points.\n",
            "\n",
            ".\n",
            "\n",
            "55.. ** **PublicPublic Spect Spectacleacle****:\n",
            ":\n",
            "     - - The The exchange exchange attracts attracts significant significant attention attention,, drawing drawing opinions opinions from from various various enthusiasts enthusiasts and and experts experts in in AI AI and and technology technology.\n",
            "\n",
            ".\n",
            "\n",
            "###### Key Key Points Points:\n",
            ":\n",
            "-- ** **YYannann Le LeCCunun****:: Crit Criticicizesizes Musk Musk's's treatment treatment of of scientists scientists and and the the lack lack of of openness openness in in Musk Musk's's approach approach to to technological technological development development.\n",
            ".\n",
            "-- ** **ElElonon Musk Musk****:: Def Defendsends his his methods methods and and promotes promotes X X.A.AII,, asserting asserting his his approach approach to to AI AI development development.\n",
            ".\n",
            "-- ** **OutcomeOutcome****:: A A public public debate debate that that showcases showcases both both sides sides'' strong strong opinions opinions,, with with Le LeCCunun bringing bringing his his vast vast expertise expertise to to bear bear against against Musk Musk's's provoc provocationsations.\n",
            "\n",
            ".\n",
            "\n",
            "ThisThis clash clash reflects reflects broader broader ongoing ongoing debates debates in in the the tech tech world world regarding regarding openness openness,, collaboration collaboration,, and and the the direction direction of of AI AI research research and and development development..\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fc_thread.id,\n",
        "  assistant_id=fc_assistant.id,\n",
        "  event_handler=FCEventHandler()\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
