{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "✋BREAKOUT ROOM #1:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: Basic RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "\n",
        "✋BREAKOUT ROOM #2:\n",
        "- Task 1: Parent Document Retriever\n",
        "- Task 2: Ensemble Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "9cc0c072-1117-4863-8010-ee37e8e33a3d"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "a5794372-be42-46ee-cf7d-4e5628e97e9a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_NpPwk1YAgl"
      },
      "source": [
        "## Task 2: Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWXhsNVYLTA"
      },
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(\n",
        "    model='gpt-3.5-turbo',\n",
        "    tags=[\"base_llm\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiagvgVDYTPn"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDO0XJqbYabb"
      },
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "c10b7cc4-e5c7-4dd8-c689-59d5c356c4d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 219/219 [00:27<00:00,  8.05it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "2c74e338-8ba6-432f-c2da-641d5d55336e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://blog.langchain.dev/langgraph-cloud/'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F79PdFcaYfBL"
      },
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "f5a05c71-c382-42f4-e6a5-58f3b61d66f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUsEc07iYnwj"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's [text-embedding-3-small](https://openai.com/index/new-embedding-models-and-api-updates/) today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoO_2MaY0TS"
      },
      "source": [
        "### Qdrant VectorStore Retriever\n",
        "\n",
        "Now we can use a Qdrant VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    split_documents,\n",
        "    base_embeddings_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"langchainblogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "outputs": [],
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GPhHPAY5yG"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmT5VyLmZAAK"
      },
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "4bcebeb0-37ae-4fb8-9dae-ceba9cc1dc54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"A good way to evaluate agents is by testing their capabilities in common agentic workflows, such as planning/task decomposition, function calling, and the ability to override pre-trained biases when necessary. Creating benchmarks and test environments can help measure an agent's performance in effectively using tools to accomplish tasks.\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "587380f0-7395-4608-aa63-35d117dbd162"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3eoqBtBQERXP",
        "outputId": "727abc25-3510-49b7-9671-98406e672294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM applications. It allows users to easily debug, monitor, test, evaluate, and share their LLM applications.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The screeshot of a trace below shows a `trace` of the entire runnable as executed by the LCEL code from the inut query to using a retriever to fetch documents from the source (documents or vector database), all the way to the output response using the ChatOpenAI function."
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {
        "id": "oMzWpDK369i2"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCKCAhASkXu0"
      },
      "source": [
        "### Synthetic Data Generation (SDG)\n",
        "\n",
        "In order to full test our RAG chain, and the various modifications we'll be using in the following notebook, we'll need to create a small synthetic dataset that is relevant to our task!\n",
        "\n",
        "Let's start by generating a series of questions - which begins with a simple model definition!\n",
        "\n",
        "> NOTE: We're going to be using a purposefully simplified datagen pipeline as an example today - but you could leverage the RAGAS SDG pipeline just as easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-dgAkRzDlEgH"
      },
      "outputs": [],
      "source": [
        "question_model = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-HC4C8lWIS"
      },
      "source": [
        "Next up, we'll create some novel chunks from our source data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2Sz7rw0-lhf8"
      },
      "outputs": [],
      "source": [
        "sdg_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pep-eqUbllrv"
      },
      "source": [
        "Now, let's ask some questions that could be answered from the provided chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e75gwn4Tlt3w"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating questions for an exam. You must create a question for a given piece of context.\n",
        "\n",
        "The question must be answerable only using the provided context.\n",
        "\n",
        "Avoid creating questions that are ambiguous or vague. They should be specifically related to the context.\n",
        "\n",
        "Your output must only be the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Pcb_R-G_odCl"
      },
      "outputs": [],
      "source": [
        "question_chain = question_prompt | question_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YbGi7umtOB"
      },
      "source": [
        "Now we can loop through a subset of our context chunks and create question/context pairs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot0HzAGBms90",
        "outputId": "0417f0df-ea17-4da0-dd3b-a6336b154c4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:21<00:00,  1.01it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "question_context_pairs = []\n",
        "\n",
        "for idx in tqdm(range(0, len(sdg_documents), 40)):\n",
        "  question = question_chain.invoke({\"context\" : sdg_documents[idx].page_content})\n",
        "  question_context_pairs.append({\"question\" : question.content, \"context\" : sdg_documents[idx].page_content, \"idx\" : idx})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhg3AJlmfW-",
        "outputId": "9c32bad2-f2db-471d-e526-ad430e427292"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34zGDKtD9I0A"
      },
      "source": [
        "We'll repeat this process for answers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ws624hGapJ92"
      },
      "outputs": [],
      "source": [
        "answer_prompt_template = \"\"\"\\\n",
        "You are a University Professor creating an exam. You must create a answer for a given piece of context and question.\n",
        "\n",
        "The answer must only rely on the provided context.\n",
        "\n",
        "Your output must only be the answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AEnETuSEqf2R"
      },
      "outputs": [],
      "source": [
        "answer_chain = answer_prompt | question_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89faPBcPqnfT",
        "outputId": "8e9bd52a-b400-44da-d228-34d9234c324e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/22 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [00:23<00:00,  1.07s/it]\n"
          ]
        }
      ],
      "source": [
        "for question_context_pair in tqdm(question_context_pairs):\n",
        "  question_context_pair[\"answer\"] = answer_chain.invoke({\"question\" : question_context_pair[\"question\"], \"context\" : question_context_pair[\"context\"]}).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "usB9PkJWq_0D"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?',\n",
              " 'context': 'Announcing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangGraph v0.1 & LangGraph Cloud: Running agents at scale, reliably\\nOur new infrastructure for running agents at scale, LangGraph Cloud, is available in beta. We also have a new stable release of LangGraph.\\n\\n6 min read\\nJun 27, 2024',\n",
              " 'idx': 0,\n",
              " 'answer': 'LangGraph v0.1'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_context_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in question_context_pairs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746,
          "referenced_widgets": [
            "a179a48558994e3c94b1484cf1f7310f",
            "b46fc6940b004268ac685e7b1d2c4ad6",
            "df0dc6074ebb4cbcb887b1bd123491be",
            "533af6b3e80c4fb7a8a4fb3dd9efe2a6",
            "02e958d09185453587b3fb6106c6a19d",
            "1ad9f6314e06446286368195168e394d",
            "a8afc185bfc245f58ba2388559ca69c8",
            "48ea78bdc6f145f49315f01981af8706",
            "b2f76d283333446abcbf4ceb7f761f44",
            "11355ee61067447ab6f9f640f03e5f8d",
            "283761def08545f2a8edc6f3ab8a0d47"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "d83006a9-6680-4b49-dec8-f3c00e1856fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Base RAG Evaluation-b7b13262' at:\n",
            "https://smith.langchain.com/o/9b340aa0-088e-58ee-bb14-e8347f68d137/datasets/28966892-6a7d-4b2d-924f-65c67f75bbe4/compare?selectedSessions=0a1d9e92-1686-4385-bace-735fc898556a\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77d22941141d4aa3a57a81c4b95f77ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n"
          ]
        }
      ],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_data_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "unlabeled_coherence_evaluator = LangChainStringEvaluator(\"criteria\", config={ \"criteria\": \"coherence\"}, prepare_data=prepare_data_noref)\n",
        "labeled_relevance_evaluator = LangChainStringEvaluator(\"criteria\", config={ \"criteria\": \"relevance\"}, prepare_data=prepare_data_ref)\n",
        "\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    base_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhBxlxYAdno"
      },
      "source": [
        "## Testing Other Retrievers\n",
        "\n",
        "Now we can test our how changing our Retriever impacts our LangSmith evaluation!\n",
        "\n",
        "We'll build this simple qa_chain factory to create standardized qa_chains where the only different component will be the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qnfy4VNkzZi2"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever):\n",
        "  primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "  created_qa_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"context\")\n",
        "      )\n",
        "    | {\n",
        "         \"response\": base_rag_prompt | primary_qa_llm,\n",
        "         \"context\": itemgetter(\"context\"),\n",
        "      }\n",
        "  )\n",
        "  return created_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPp4Xq7AvEx"
      },
      "source": [
        "### Task 1: Parent Document Retriever\n",
        "\n",
        "One of the easier ways we can imagine improving a retriever is to embed our documents into small chunks, and then retrieve a significant amount of additional context that \"surrounds\" the found context.\n",
        "\n",
        "You can read more about this method [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)!\n",
        "\n",
        "The basic outline of this retrieval method is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Retrieve child documents using Dense Vector Retrieval\n",
        "3. Merge the child documents based on their parents. If they have the same parents - they become merged.\n",
        "4. Replace the child documents with their respective parent documents from an in-memory-store.\n",
        "5. Use the parent documents to augment generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "67I6QJAJ0Un7"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=\"split_parents\",\n",
        "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vectorstore = Qdrant(client, collection_name=\"split_parents\", embeddings=base_embeddings_model)\n",
        "\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zfk5RYUt00Pw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=RecursiveCharacterTextSplitter(chunk_size=400),\n",
        "    parent_splitter=RecursiveCharacterTextSplitter(chunk_size=2000),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "68c1t4o104AK"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTH0MDolBndm"
      },
      "source": [
        "Let's create, test, and then evaluate our new chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KMjLfqOC09Iw"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Rv8bAHPN1H4P",
        "outputId": "46487575-438d-4c12-d306-7a4341d7ed83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Model) application development. It involves connecting LLMs to external data sources to retrieve relevant documents based on a user query and generate an answer grounded in the retrieved context.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Key LinksCookbooks for Self-RAG and CRAGVideoMotivationBecause most LLMs are only periodically trained on a large corpus of public data, they lack recent information and / or private data that is inaccessible for training. Retrieval augmented generation (RAG) is a central paradigm in LLM application development to address this by connecting LLMs to external data sources (see our video series and blog post). The basic RAG pipeline involves embedding a user query, retrieving relevant documents to the query, and passing the documents to an LLM for generation of an answer grounded in the retrieved context. Basic RAG flowSelf-Reflective RAGIn practice, many have found that implementing RAG requires logical reasoning around these steps: for example, we can ask when to retrieve (based upon the question and composition of the index), when to re-write the question for better retrieval, or when to discard irrelevant retrieved documents and re-try retrieval? The term self-reflective RAG (paper) has been introduced, which captures the idea of  using an LLM to self-correct poor quality retrieval and / or generations.The basic RAG flow (shown above) simply uses a chain: the LLM determines what to generate based upon the retrieved documents. Some RAG flows use routing, where an LLM decides between, for example, different retrievers based on the question. But self-reflective RAG usually requires some kind of feedback, re-generating the question and / or re-retrieving documents. State machines are a third kind of cognitive architecture that supports loops and it well suited for this: a state machine simply lets us define a set of steps (e.g., retrieval, grade documents, re-write query) and set the transitions options between them; e.g., if our retrieved docs are not relevant, then re-write the query and re-retrieve new documents. Cognitive architectures for RAGSelf-Reflective RAG with LangGraphWe recently launched LangGraph, which is an easy way to implement LLM state machines.', metadata={'source': 'https://blog.langchain.dev/agentic-rag-with-langgraph/', 'loc': 'https://blog.langchain.dev/agentic-rag-with-langgraph/', 'lastmod': '2024-02-15T00:21:59.000Z'}),\n",
              " Document(page_content='{\\n    \\t# Use the above retrieval logic to get documents and then format them\\n        \"context\": retriever_chain | format_docs,\\n        \"question\": itemgetter(\"question\"),\\n        \"chat_history\": itemgetter(\"chat_history\"),\\n    }\\n)\\nresponse_synthesizer = prompt | llm | StrOutputParser()\\nchain = _context | response_synthesizerThis is a relatively simple chain, where we first fetch the relevant context and then pass it to a single LLM call. ConclusionWe hope that this has helped prepare you to build your own RAG application in a few ways. First, we hope that this has helped explore (in more detail than you probably ever wanted to) all the small engineering decisions that make up a RAG application. Understanding these decisions, and the tradeoffs involved, will be crucial for building your own app. Second, we hope the open source repository, which includes a fully function web application and connection to LangSmith, is a helpful starting off point.The underlying application logic is EXTREMELY similar (basically the same) as the ChatLangChain app we released last week. This is no accident. We think this application logic is pretty generalizable and can be extended to a variety of applications - and so we hope you try to do so! While we hope this serves as an easy getting started point, we\\'re looking forward to improvements next week that will make it even easier to customize.', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}),\n",
              " Document(page_content='Deconstructing RAG\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDeconstructing RAG\\n\\n7 min read\\nNov 30, 2023', metadata={'source': 'https://blog.langchain.dev/deconstructing-rag/', 'loc': 'https://blog.langchain.dev/deconstructing-rag/', 'lastmod': '2023-11-30T16:07:12.000Z'}),\n",
              " Document(page_content=\"your RAG pipeline. It can often involving connecting to a variety of sources. Airbyte's recent release can help with this!Building long context RAG with RAPTOR. RAPTOR (Sarthi et al) is a novel indexing strategy: docs are clustered and clusters are summarized to capture higher-level information across similar docs.🤝 From the CommunityLangSmith: The LLM Ops Breakthrough. Check out this 15 part series highlighting LangSmith from AI Accelera.Build a Perplexity-Inspired Answer Engine. Create your own AI search engine powered by Groq, Mistral, LangChain, Brave, and OpenAI embeddings. Repo here and Youtube tutorial here by Developers Digest.Chat with MySQL Database with Python | LangChain Tutorial. Discover how to interact with a MySQL database using Python and LangChain in this tutorial by Alejandro AO.LangGraph: An intuitive framework for AI agents workflow. Read this review of LangGraph by Mario Bonilla.\", metadata={'source': 'https://blog.langchain.dev/week-of-3-04-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-3-04-langchain-release-notes/', 'lastmod': '2024-05-29T19:52:03.000Z'})]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents = parent_document_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"context\"]\n",
        "contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Key LinksCookbooks for Self-RAG and CRAGVideoMotivationBecause most LLMs are only periodically trained on a large corpus of public data, they lack recent information and / or private data that is inaccessible for training. Retrieval augmented generation (RAG) is a central paradigm in LLM application development to address this by connecting LLMs to external data sources (see our video series and blog post). The basic RAG pipeline involves embedding a user query, retrieving relevant documents to the query, and passing the documents to an LLM for generation of an answer grounded in the retrieved context. Basic RAG flowSelf-Reflective RAGIn practice, many have found that implementing RAG requires logical reasoning around these steps: for example, we can ask when to retrieve (based upon the question and composition of the index), when to re-write the question for better retrieval, or when to discard irrelevant retrieved documents and re-try retrieval? The term self-reflective RAG (paper) has been introduced, which captures the idea of  using an LLM to self-correct poor quality retrieval and / or generations.The basic RAG flow (shown above) simply uses a chain: the LLM determines what to generate based upon the retrieved documents. Some RAG flows use routing, where an LLM decides between, for example, different retrievers based on the question. But self-reflective RAG usually requires some kind of feedback, re-generating the question and / or re-retrieving documents. State machines are a third kind of cognitive architecture that supports loops and it well suited for this: a state machine simply lets us define a set of steps (e.g., retrieval, grade documents, re-write query) and set the transitions options between them; e.g., if our retrieved docs are not relevant, then re-write the query and re-retrieve new documents. Cognitive architectures for RAGSelf-Reflective RAG with LangGraphWe recently launched LangGraph, which is an easy way to implement LLM state machines.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n    \\t# Use the above retrieval logic to get documents and then format them\\n        \"context\": retriever_chain | format_docs,\\n        \"question\": itemgetter(\"question\"),\\n        \"chat_history\": itemgetter(\"chat_history\"),\\n    }\\n)\\nresponse_synthesizer = prompt | llm | StrOutputParser()\\nchain = _context | response_synthesizerThis is a relatively simple chain, where we first fetch the relevant context and then pass it to a single LLM call. ConclusionWe hope that this has helped prepare you to build your own RAG application in a few ways. First, we hope that this has helped explore (in more detail than you probably ever wanted to) all the small engineering decisions that make up a RAG application. Understanding these decisions, and the tradeoffs involved, will be crucial for building your own app. Second, we hope the open source repository, which includes a fully function web application and connection to LangSmith, is a helpful starting off point.The underlying application logic is EXTREMELY similar (basically the same) as the ChatLangChain app we released last week. This is no accident. We think this application logic is pretty generalizable and can be extended to a variety of applications - and so we hope you try to do so! While we hope this serves as an easy getting started point, we\\'re looking forward to improvements next week that will make it even easier to customize.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents[1].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6H7vHCt2HJi"
      },
      "source": [
        "#### Evaluating the Parent Document Retrieval Pipeline\n",
        "\n",
        "Now that we've created a new retriever - let's try evaluating it on the same dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "142e5756a6c840aeb25b733ece015ed2",
            "4777a17d7d9c444a8d44e070faef0ad0",
            "dad645f3259c41b5a50e0e94906ca3e3",
            "c67428191d074e4d80cb0f28fb65cd41",
            "8db8082c5ab741b5b4fd78027e65135d",
            "2ab8452345f24b46b172515ddc77fdb0",
            "d5ad942ac5ed41c6a581b4022f177114",
            "27e2bff47a084ed6bc7d96ac09af9ebc",
            "d097ee02256d40d5b395d37cd0face05",
            "13c75ca5ee9d42eb8476a307f30b7528",
            "e8143f8ceee6468c98433734ff59b835"
          ]
        },
        "id": "Z-0WFCtx2N4n",
        "outputId": "341b1af3-cd74-46a9-d9d1-5ab2190ec96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Parent Document Retrieval RAG Evaluation-11fe2fe7' at:\n",
            "https://smith.langchain.com/o/9b340aa0-088e-58ee-bb14-e8347f68d137/datasets/28966892-6a7d-4b2d-924f-65c67f75bbe4/compare?selectedSessions=c4997fbf-abac-4920-9dd8-b93798cc2681\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb8e843d6b2a4e75b02fbad25dce6d38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n"
          ]
        }
      ],
      "source": [
        "pdr_rag_results = evaluate(\n",
        "    parent_document_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Parent Document Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaNk6o7_BqX8"
      },
      "source": [
        "### Task 2: Ensemble Retrieval\n",
        "\n",
        "Next let's look at ensemble retrieval!\n",
        "\n",
        "You can read more about this [here](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble)!\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Obtain User Question\n",
        "2. Hit the Retriever Pair\n",
        "    - Retrieve Documents with BM25 Sparse Vector Retrieval\n",
        "    - Retrieve Documents with Dense Vector Retrieval Method\n",
        "3. Collect and \"fuse\" the retrieved docs based on their weighting using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm into a single ranked list.\n",
        "4. Use those documents to augment our generation.\n",
        "\n",
        "Ensure your `weights` list - the relative weighting of each retriever - sums to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zz7dl1GD5-L-"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Vs8wxT9b5pRA"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=75)\n",
        "split_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(split_documents)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Qdrant.from_documents(split_documents, embedding, location=\":memory:\")\n",
        "qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        bm25_retriever,\n",
        "        qdrant_retriever\n",
        "    ],\n",
        "    weights=[\n",
        "        0.5,\n",
        "        0.5\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cv69YDpF6PrJ"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6lSszzrf6UmP",
        "outputId": "ea13ffbc-df0f-4191-f873-6c2f0405d874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval Augmented Generation, which is a central paradigm in LLM (Large Language Models) application development. It involves connecting LLMs to external data sources to address the lack of recent or private information during training.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retriever_qa_chain.invoke({\"question\" : \"What is RAG?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e99b5317081e42c199a297f3a483bbf5",
            "42bfd25ca7a14d629e4070b093a197d5",
            "fa9f9fc79b344271adc94acc487aef59",
            "a603cf6ec7d544a1a951fb8defccb976",
            "7448f6f5d9dc431fba689d8821285ca3",
            "1d0589c63fd1461b93b403c594b6ccfa",
            "5e500672dafc4529806e4e5f72fe97fa",
            "ece363bdc8b54631beba2628e3175e0b",
            "46a7a454e9bb486588ac33156abddecc",
            "f79f87ac933b45fa9e2c17871d4f2e44",
            "a888bb92387549fea8843f20e3b4c50f"
          ]
        },
        "id": "GVBY5lhm4KG7",
        "outputId": "d48d2604-1ac3-4b8a-c4aa-93214b4c0e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Hybrid Retrieval RAG Evaluation-715a8292' at:\n",
            "https://smith.langchain.com/o/9b340aa0-088e-58ee-bb14-e8347f68d137/datasets/28966892-6a7d-4b2d-924f-65c67f75bbe4/compare?selectedSessions=125395c9-feb4-46af-9040-aea0300cf4ba\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06f74701e6be463a8a11705019988c1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n",
            "/home/nisargvp/anaconda3/envs/axo/lib/python3.10/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in CriteriaEvalChain, as it is not expected.\n",
            "To use references, use the labeled_criteria instead.\n",
            "  warn(self._skip_reference_warning)\n"
          ]
        }
      ],
      "source": [
        "pdre_rag_results = evaluate(\n",
        "    ensemble_retriever_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        unlabeled_coherence_evaluator,\n",
        "        labeled_relevance_evaluator\n",
        "        ],\n",
        "    experiment_prefix=\"Hybrid Retrieval RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'run': RunTree(id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 217628, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 201389, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.869823+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 73.0, 'mem': {'rss': 1286811648.0}, 'cpu': {'time': {'sys': 300.18, 'user': 141.44}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, 'config': None}, outputs={'response': AIMessage(content='The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-41239289-aaff-4785-9064-0e9131ec6043-0', usage_metadata={'input_tokens': 1494, 'output_tokens': 46, 'total_tokens': 1540}), 'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=UUID('86e59b2a-8ce7-48ea-be30-b825169663ae'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('56abb6a3-1426-463b-a77e-c1e03c011885'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 469380, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 198261, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 469380, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 198261, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'response': AIMessage(content='The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-41239289-aaff-4785-9064-0e9131ec6043-0', usage_metadata={'input_tokens': 1494, 'output_tokens': 46, 'total_tokens': 1540}), 'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), tags=[], child_runs=[Run(id=UUID('761fab5b-b17d-46a6-84af-74507f012b47'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 782534, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 124343, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 782534, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 124343, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, reference_example_id=None, parent_run_id=UUID('56abb6a3-1426-463b-a77e-c1e03c011885'), tags=['seq:step:1'], child_runs=[Run(id=UUID('ffb5fcc9-c8c3-4c46-911c-2bc0a5d09395'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 153924, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 81644, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 153924, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 81644, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=None, parent_run_id=UUID('761fab5b-b17d-46a6-84af-74507f012b47'), tags=['map:key:context'], child_runs=[Run(id=UUID('1e629151-6a68-4853-8a04-8e7e45f544fb'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 367732, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 569860, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 367732, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 569860, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, reference_example_id=None, parent_run_id=UUID('ffb5fcc9-c8c3-4c46-911c-2bc0a5d09395'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002651782534Z761fab5b-b17d-46a6-84af-74507f012b47.20240628T002652153924Zffb5fcc9-c8c3-4c46-911c-2bc0a5d09395.20240628T002652367732Z1e629151-6a68-4853-8a04-8e7e45f544fb'), Run(id=UUID('2f9f24c3-89cc-4266-bfa1-955f4cd6c5bd'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 592123, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 68047, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 592123, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 68047, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'documents': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=None, parent_run_id=UUID('ffb5fcc9-c8c3-4c46-911c-2bc0a5d09395'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002651782534Z761fab5b-b17d-46a6-84af-74507f012b47.20240628T002652153924Zffb5fcc9-c8c3-4c46-911c-2bc0a5d09395.20240628T002652592123Z2f9f24c3-89cc-4266-bfa1-955f4cd6c5bd')], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002651782534Z761fab5b-b17d-46a6-84af-74507f012b47.20240628T002652153924Zffb5fcc9-c8c3-4c46-911c-2bc0a5d09395'), Run(id=UUID('e99bf743-82a5-4172-85ba-5e310b9ffaf3'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 321750, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 672965, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 321750, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 672965, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, reference_example_id=None, parent_run_id=UUID('761fab5b-b17d-46a6-84af-74507f012b47'), tags=['map:key:question'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002651782534Z761fab5b-b17d-46a6-84af-74507f012b47.20240628T002652321750Ze99bf743-82a5-4172-85ba-5e310b9ffaf3')], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002651782534Z761fab5b-b17d-46a6-84af-74507f012b47'), Run(id=UUID('98cec532-d171-4834-9e5c-eebb7d4ac920'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 128642, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 128642, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('56abb6a3-1426-463b-a77e-c1e03c011885'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002717128642Z98cec532-d171-4834-9e5c-eebb7d4ac920'), Run(id=UUID('6145b6b2-26d7-4848-abc0-d169f96dd5db'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 187829, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 196521, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 187829, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 196521, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'response': AIMessage(content='The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-41239289-aaff-4785-9064-0e9131ec6043-0', usage_metadata={'input_tokens': 1494, 'output_tokens': 46, 'total_tokens': 1540}), 'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=None, parent_run_id=UUID('56abb6a3-1426-463b-a77e-c1e03c011885'), tags=['seq:step:3'], child_runs=[Run(id=UUID('43d2f355-9c56-4ae4-af2f-f982fbb5d69b'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 787430, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 194407, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 787430, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 194407, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': AIMessage(content='The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-41239289-aaff-4785-9064-0e9131ec6043-0', usage_metadata={'input_tokens': 1494, 'output_tokens': 46, 'total_tokens': 1540})}, reference_example_id=None, parent_run_id=UUID('6145b6b2-26d7-4848-abc0-d169f96dd5db'), tags=['map:key:response'], child_runs=[Run(id=UUID('aa47f6e6-5b36-4bac-afa3-146eddc10bea'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 987110, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 365664, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 987110, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 365664, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use\\', metadata={\\'source\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'loc\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'lastmod\\': \\'2023-11-07T18:32:54.000Z\\'}), Document(page_content=\"Editor\\'s note: We\\'re excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={\\'source\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'loc\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'lastmod\\': \\'2023-11-07T18:32:54.000Z\\'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you\\'ll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/\\', \\'lastmod\\': \\'2023-12-01T16:08:08.000Z\\'})]\\n\\nQuestion:\\nWhat are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?\\n')])}, reference_example_id=None, parent_run_id=UUID('43d2f355-9c56-4ae4-af2f-f982fbb5d69b'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002721187829Z6145b6b2-26d7-4848-abc0-d169f96dd5db.20240628T002721787430Z43d2f355-9c56-4ae4-af2f-f982fbb5d69b.20240628T002721987110Zaa47f6e6-5b36-4bac-afa3-146eddc10bea'), Run(id=UUID('41239289-aaff-4785-9064-0e9131ec6043'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 571126, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 193310, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 571126, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 193310, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use\\', metadata={\\'source\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'loc\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'lastmod\\': \\'2023-11-07T18:32:54.000Z\\'}), Document(page_content=\"Editor\\'s note: We\\'re excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={\\'source\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'loc\\': \\'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/\\', \\'lastmod\\': \\'2023-11-07T18:32:54.000Z\\'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you\\'ll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/\\', \\'lastmod\\': \\'2023-12-01T16:08:08.000Z\\'})]\\n\\nQuestion:\\nWhat are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template are:\\n1. Typical RAG\\n2. Parent retriever\\n3. Hypothetical Questions\\n4. Summaries', 'response_metadata': {'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-41239289-aaff-4785-9064-0e9131ec6043-0', 'usage_metadata': {'input_tokens': 1494, 'output_tokens': 46, 'total_tokens': 1540}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1494, 'total_tokens': 1540}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('43d2f355-9c56-4ae4-af2f-f982fbb5d69b'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002721187829Z6145b6b2-26d7-4848-abc0-d169f96dd5db.20240628T002721787430Z43d2f355-9c56-4ae4-af2f-f982fbb5d69b.20240628T002722571126Z41239289-aaff-4785-9064-0e9131ec6043')], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002721187829Z6145b6b2-26d7-4848-abc0-d169f96dd5db.20240628T002721787430Z43d2f355-9c56-4ae4-af2f-f982fbb5d69b'), Run(id=UUID('452a4f16-ba63-410b-b0e0-d891b1c064f9'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 824494, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 95201, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 824494, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 95201, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})], 'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'output': [Document(page_content=')Neo4j Advanced RAG templateLangChain Templates offers a collection of easily deployable reference architectures that anyone can use. This is a new way to create, share, maintain, download, and customize chains and agents. They are all in a standard format that allows them to easily be deployed with LangServe, allowing you to easily get production-ready APIs and a playground for free.The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.Available Strategies1. Typical RAG:- Traditional method where the exact data indexed is the data retrieved.2. Parent retriever:- Instead of indexing entire documents, data is divided into smaller chunks, referred to as Parent and Child documents.- Child documents are indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.3. Hypothetical Questions:- Documents are processed to generate potential questions they might answer.- These questions are then indexed for better representation of specific concepts, while parent documents are retrieved to ensure context retention.4. Summaries:- Instead of indexing the entire document, a summary of the document is created and indexed.- Similarly, the parent document is retrieved in a RAG application.To be able to use LangChain templates, you should first install the LangChain CLI:pip install -U \"langchain-cli[serve]\"Retrieving the LangChain template is then as simple as executing the following line of code:langchain app new my-app --package neo4j-advanced-ragThis code will create a new folder called my-app, and store all the relevant code in it. Think of it as a “git clone” equivalent for LangChain templates. This will construct the following structure in your filesystem.There are two top-level folders created:App: stores the FastAPI server codePackages: stores all the templates that you selected to use in this application. Remember, you can use', metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"Editor's note: We're excited to share this blogpost as it covers several of the advanced retrieval strategies we introduced in the past month, specifically a lot of the ones that rely on changing the ingestion step. A lot of these advanced retrieval strategies can be summarized as changing how indexing of documents is done to retain some concept of hierarchy. Neo4j is an exciting database to use for these tasks since it can represent these hierarchies as part of the graph. This also allows you to switch between indexing strategies pretty easily.Tomaz has implemented a single LangChain template that contains four different RAG strategies. Check it out here:Neo4j Advanced RAG TemplateRetrieval-augmented generation applications seem to be the “Hello World” of AI applications. Nowadays, you can implement a “Chat with your PDF” application in only a couple of minutes due to the help of LLM framework libraries like LangChain.“Chat with your PDF” applications typically rely on vector similarity search to retrieve relevant information, which are then fed to an LLM to generate a final answer that is returned to a user.Vector similarity search is used to retrieve relevant information.Lately, it is becoming more and more obvious that naive vector similarity search might not be accurate enough for all use cases. For example, we have seen the introduction of step-back approach to prompting, which emphasizes the importance of taking a step back from the immediate details of a task to focus on a higher-level abstraction.Step-back prompting. Image from research paper licensed under CC BY 4.0.The step-back prompting technique is based on the observation that directly addressing intricate tasks can lead to errors, especially when there are numerous specifics to consider. Instead of plunging straight into the complexities, the model first prompts itself to ask a more generic question that encapsulates the core essence of the original query. By focusing on this broader concept or\", metadata={'source': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'loc': 'https://blog.langchain.dev/implementing-advanced-retrieval-rag-strategies-with-neo4j/', 'lastmod': '2023-11-07T18:32:54.000Z'}), Document(page_content=\"New to LangChain Templates 📁LangChain Templates are the easiest way to get started building GenAI applications. We have dozens of examples that you can adopt for your own use cases, giving you starter code that’s easy to customize. As a big bonus, LangChain Templates integrate seamlessly with LangServe and LangSmith, so you deploy them and monitor them too ❤️.Check out https://templates.langchain.com/ to see them all!A few of our most recent favorites are:Research assistant: build a research assistant with a retriever of your choice! Inspired by the great gpt-researcher 🙇.Advanced RAG with Neo4j: sometimes basic RAG doesn’t cut it 😏. We highly recommend this template if you want to experiment with different retrieval strategies. You’ll learn about different techniques: traditional, parent retriever, hypothetical questions, and summarization.Extraction with Anthropic Models: use prompting techniques to do extraction with Anthropic models.Coming soon: Hosted LangServe 🦜🏓We’re hard at work building Hosted LangServe. This service will make it super easy to deploy LangServe apps and get tracking in LangSmith. Sign up to get notified for early access here. We’re also eager to hear about what kind of deployment challenges you’re having with LLM-powered apps. Feel free to hit reply, and let us know how we can support you better!We also collaborated with Google to make it dead simple to deploy LangServe apps on Cloud Run — read about it here.7 Day of LangSmith 🦜⚒️In case you missed it, we put together a LangSmith demo series to highlight how the platform can help you build production-ready gen AI applications faster and more reliably!Check out these 7 videos, each <10 min, and you'll be a LangSmith pro in under an hour 💪.LangChain Benchmarks 📊LangChain benchmarks is a python package and associated datasets to facilitate experimentation and benchmarking of different cognitive architectures. Each benchmark task targets key functionality within common LLM applications, such as\", metadata={'source': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-11-27-langchain-release-notes/', 'lastmod': '2023-12-01T16:08:08.000Z'})]}, reference_example_id=None, parent_run_id=UUID('6145b6b2-26d7-4848-abc0-d169f96dd5db'), tags=['map:key:context'], child_runs=[], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002721187829Z6145b6b2-26d7-4848-abc0-d169f96dd5db.20240628T002721824494Z452a4f16-ba63-410b-b0e0-d891b1c064f9')], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885.20240628T002721187829Z6145b6b2-26d7-4848-abc0-d169f96dd5db')], trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706'), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706.20240628T002651469380Z56abb6a3-1426-463b-a77e-c1e03c011885')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651217628Z4cd6cd41-10d3-4045-b089-30a29130f706', trace_id=UUID('4cd6cd41-10d3-4045-b089-30a29130f706')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What are the four available strategies for balancing precise embeddings and context retention in the neo4j-advanced-rag template?'}, outputs={'answer': 'The neo4j-advanced-rag template allows you to balance precise embeddings and context retention by implementing advanced retrieval strategies.'}, metadata={'dataset_split': ['base']}, id=UUID('86e59b2a-8ce7-48ea-be30-b825169663ae'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 869823, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 869823, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('38da35a3-c0bf-4f9e-9eec-efde460e297f'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8ae09040-7130-4c2b-bd27-919c34ed7cb2'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a3f2ecb1-98ba-453c-95cd-09924940a4dd'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6d648d29-98a2-4221-8c06-085c0f3004a4'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0bf3090a-9862-4d3c-b1c2-fa8bad36d1f9'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 501667, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 989870, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.174800+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What were the two different code generation architectures built and compared in the study?'}, 'config': None}, outputs={'response': AIMessage(content='The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb016570-b566-45b0-9b18-17846ef112a6-0', usage_metadata={'input_tokens': 966, 'output_tokens': 50, 'total_tokens': 1016}), 'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=UUID('5dde399b-5bca-466b-aaee-0e0e86f57385'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('cc1a9dd8-6a9f-430e-a029-8c828f9f7438'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 769176, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 988463, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 769176, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 988463, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'response': AIMessage(content='The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb016570-b566-45b0-9b18-17846ef112a6-0', usage_metadata={'input_tokens': 966, 'output_tokens': 50, 'total_tokens': 1016}), 'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), tags=[], child_runs=[Run(id=UUID('fc1ae8c9-8ad8-434b-b0f1-bcecc098c561'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 144788, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 486409, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 144788, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 486409, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, reference_example_id=None, parent_run_id=UUID('cc1a9dd8-6a9f-430e-a029-8c828f9f7438'), tags=['seq:step:1'], child_runs=[Run(id=UUID('9cc1d1d9-8c57-4633-b345-f4ae6fc75d28'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 548333, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 389123, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 548333, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 389123, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=None, parent_run_id=UUID('fc1ae8c9-8ad8-434b-b0f1-bcecc098c561'), tags=['map:key:context'], child_runs=[Run(id=UUID('785f0b62-dfab-4d83-9378-21a42f7760e3'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 738876, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 924964, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 738876, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 924964, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': 'What were the two different code generation architectures built and compared in the study?'}, reference_example_id=None, parent_run_id=UUID('9cc1d1d9-8c57-4633-b345-f4ae6fc75d28'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002652144788Zfc1ae8c9-8ad8-434b-b0f1-bcecc098c561.20240628T002652548333Z9cc1d1d9-8c57-4633-b345-f4ae6fc75d28.20240628T002652738876Z785f0b62-dfab-4d83-9378-21a42f7760e3'), Run(id=UUID('984b2959-489f-4855-ac6b-ca55bc757647'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 926371, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 276822, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 926371, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 276822, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'documents': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9cc1d1d9-8c57-4633-b345-f4ae6fc75d28'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002652144788Zfc1ae8c9-8ad8-434b-b0f1-bcecc098c561.20240628T002652548333Z9cc1d1d9-8c57-4633-b345-f4ae6fc75d28.20240628T002652926371Z984b2959-489f-4855-ac6b-ca55bc757647')], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002652144788Zfc1ae8c9-8ad8-434b-b0f1-bcecc098c561.20240628T002652548333Z9cc1d1d9-8c57-4633-b345-f4ae6fc75d28'), Run(id=UUID('73374393-8566-4c45-ae8a-71e4003efef1'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 918074, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 314063, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 918074, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 314063, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': 'What were the two different code generation architectures built and compared in the study?'}, reference_example_id=None, parent_run_id=UUID('fc1ae8c9-8ad8-434b-b0f1-bcecc098c561'), tags=['map:key:question'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002652144788Zfc1ae8c9-8ad8-434b-b0f1-bcecc098c561.20240628T002652918074Z73374393-8566-4c45-ae8a-71e4003efef1')], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002652144788Zfc1ae8c9-8ad8-434b-b0f1-bcecc098c561'), Run(id=UUID('87509299-4671-4e8b-91dd-f9884fb411ee'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 638709, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 638709, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('cc1a9dd8-6a9f-430e-a029-8c828f9f7438'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002720638709Z87509299-4671-4e8b-91dd-f9884fb411ee'), Run(id=UUID('c192d14e-a949-4723-b7d1-54f9f0a526cd'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 931542, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 987643, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 931542, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 987643, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'response': AIMessage(content='The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb016570-b566-45b0-9b18-17846ef112a6-0', usage_metadata={'input_tokens': 966, 'output_tokens': 50, 'total_tokens': 1016}), 'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=None, parent_run_id=UUID('cc1a9dd8-6a9f-430e-a029-8c828f9f7438'), tags=['seq:step:3'], child_runs=[Run(id=UUID('5fd7e0d4-f00f-418a-9b17-5e7b74822f3f'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 559579, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 986315, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 559579, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 986315, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': AIMessage(content='The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eb016570-b566-45b0-9b18-17846ef112a6-0', usage_metadata={'input_tokens': 966, 'output_tokens': 50, 'total_tokens': 1016})}, reference_example_id=None, parent_run_id=UUID('c192d14e-a949-4723-b7d1-54f9f0a526cd'), tags=['map:key:response'], child_runs=[Run(id=UUID('99960b0c-a675-4559-8e88-1dce6e4a0f80'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 931784, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 296216, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 931784, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 296216, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to\\', metadata={\\'source\\': \\'https://blog.langchain.dev/code-execution-with-langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/code-execution-with-langgraph/\\', \\'lastmod\\': \\'2024-03-01T21:52:16.000Z\\'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don\\'t necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain\\'s documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={\\'source\\': \\'https://blog.langchain.dev/public-langsmith-benchmarks/\\', \\'loc\\': \\'https://blog.langchain.dev/public-langsmith-benchmarks/\\', \\'lastmod\\': \\'2023-11-22T17:04:34.000Z\\'})]\\n\\nQuestion:\\nWhat were the two different code generation architectures built and compared in the study?\\n')])}, reference_example_id=None, parent_run_id=UUID('5fd7e0d4-f00f-418a-9b17-5e7b74822f3f'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002721931542Zc192d14e-a949-4723-b7d1-54f9f0a526cd.20240628T002722559579Z5fd7e0d4-f00f-418a-9b17-5e7b74822f3f.20240628T002722931784Z99960b0c-a675-4559-8e88-1dce6e4a0f80'), Run(id=UUID('eb016570-b566-45b0-9b18-17846ef112a6'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 435380, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 985745, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 435380, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 985745, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to\\', metadata={\\'source\\': \\'https://blog.langchain.dev/code-execution-with-langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/code-execution-with-langgraph/\\', \\'lastmod\\': \\'2024-03-01T21:52:16.000Z\\'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don\\'t necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain\\'s documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={\\'source\\': \\'https://blog.langchain.dev/public-langsmith-benchmarks/\\', \\'loc\\': \\'https://blog.langchain.dev/public-langsmith-benchmarks/\\', \\'lastmod\\': \\'2023-11-22T17:04:34.000Z\\'})]\\n\\nQuestion:\\nWhat were the two different code generation architectures built and compared in the study?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The two different code generation architectures built and compared in the study were:\\n1. Code generation via prompting and context stuffing\\n2. Code generation flow that involved checking and running the code, and then if there was some error passing it back to correct itself', 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-eb016570-b566-45b0-9b18-17846ef112a6-0', 'usage_metadata': {'input_tokens': 966, 'output_tokens': 50, 'total_tokens': 1016}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 966, 'total_tokens': 1016}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('5fd7e0d4-f00f-418a-9b17-5e7b74822f3f'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002721931542Zc192d14e-a949-4723-b7d1-54f9f0a526cd.20240628T002722559579Z5fd7e0d4-f00f-418a-9b17-5e7b74822f3f.20240628T002723435380Zeb016570-b566-45b0-9b18-17846ef112a6')], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002721931542Zc192d14e-a949-4723-b7d1-54f9f0a526cd.20240628T002722559579Z5fd7e0d4-f00f-418a-9b17-5e7b74822f3f'), Run(id=UUID('ccaea091-cfdc-404b-b0d4-d2487f9ffbcd'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 680814, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 974387, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 680814, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 974387, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})], 'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'output': [Document(page_content='Key LinksLangGraph cookbookVideoMotivationCode generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like GitHub co-pilot and popularity of projects like GPT-engineer. The recent AlphaCodium work showed that code generation can be improved by using a flow paradigm rather than a naive prompt:answer paradigm: answers can be iteratively constructed by (1) testing answers and (2) reflecting on the results of these tests in order to improve the solution. Flow for AlphaCodiumWe recently launched LangGraph to support flow engineering, giving the user the ability to represent flows as a graph. Inspired by the AlphaCodium and Reflexion work, we wanted to demonstrate that LangGraph can be used to implement code generation with the same types of cycles and decisions points as shown above.Specifically we wanted to build and compare two different architectures:Code generation via prompting and context stuffingCode generation flow that involved checking and running the code, and then if there was some error passing it back to correct itselfThis would attempt to answer: how much can these code checks improve performance of a code generation system?The answer? 💡The system that checks the code and attempts to fix it yielded a sizable improvement over the baseline of a single generation (81% vs 55%)ProblemTo demonstrate code generation on a narrow corpus of documentation, we chose a sub-set of LangChain docs focused on LangChain Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to LangSmith.Workflow to', metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z'}), Document(page_content=\"superior performance characteristics. The LangChain community has implemented many of these, from simple prompting techniques like chain of density and step-back prompting, to advanced RAG techniques all the way to RL chains, generative agents, and autonomous agents like BabyAGI. For structured generation alone, we have (thankfully) evolved from emotion prompting techniques to fine-tuned APIs like function calling and grammar-based sampling.With the launch of Hub and LangChain Templates, the release rate of new architectures continues to accelerate. But which of these approaches will translate to performance gains in YOUR application? What tradeoffs are assumed by each technique?It can be hard to separate the signal from the noise, and the abundance of options makes reliable and relevant benchmarks that much more important. When it comes to grading language models on general tasks, public benchmarks like HELM or EleutherAI’s Test Harness are great options. For measuring LLM inference speed and throughput, AnyScale’s LLMPerf benchmarks can be a guiding light. These tools are excellent for comparing the underlying capability of language models, but they don't necessarily reflect their real-world behavior within your application.LangChain’s mission is to make it as easy as possible to build with LLMs, and that means helping you stay up to date on the relevant advancements in the field. LangSmith’s evaluation and tracing experience helps you easily compare approaches in aggregate and on a sample level, and it makes it easy to drill down into each step to identify the root cause for changes in behavior.With public datasets and evals, you can see the performance characteristics of any reference architecture on a relevant dataset so you can easily separate the signal from the noise.📑 LangChain Docs Q&A DatasetThe first benchmark task we are including is a Q&A dataset over LangChain's documentation. This is a set of hand-crafted question-answer pairs we wrote over\", metadata={'source': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'loc': 'https://blog.langchain.dev/public-langsmith-benchmarks/', 'lastmod': '2023-11-22T17:04:34.000Z'})]}, reference_example_id=None, parent_run_id=UUID('c192d14e-a949-4723-b7d1-54f9f0a526cd'), tags=['map:key:context'], child_runs=[], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002721931542Zc192d14e-a949-4723-b7d1-54f9f0a526cd.20240628T002722680814Zccaea091-cfdc-404b-b0d4-d2487f9ffbcd')], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438.20240628T002721931542Zc192d14e-a949-4723-b7d1-54f9f0a526cd')], trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5'), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5.20240628T002651769176Zcc1a9dd8-6a9f-430e-a029-8c828f9f7438')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651501667Z9ae09789-5210-4573-bc96-1df36c0154b5', trace_id=UUID('9ae09789-5210-4573-bc96-1df36c0154b5')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What were the two different code generation architectures built and compared in the study?'}, outputs={'answer': '81%'}, metadata={'dataset_split': ['base']}, id=UUID('5dde399b-5bca-466b-aaee-0e0e86f57385'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 174800, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 174800, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f7f4b949-c4c6-4b6a-b212-8c5322b8c48e'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2f665d09-4213-425e-8883-9d1e6f9d4f64'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('09807064-76f6-4068-a95b-2c659f7a0949'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('18e41c2b-08dc-41dc-ad63-a7cef850f68e'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('dacddb56-b2cc-4cd7-9919-2d1a834f65c2'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 359479, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 831260, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.420688+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, 'config': None}, outputs={'response': AIMessage(content='The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d5463a0-2721-4dcd-98ff-6e9ecde8ab84-0', usage_metadata={'input_tokens': 1069, 'output_tokens': 26, 'total_tokens': 1095}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=UUID('1bc51d82-e378-433f-8826-af05ef54ab03'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('ce4a6be0-4272-4dbb-8554-1a4db668ca62'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 76644, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 829361, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 76644, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 829361, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'response': AIMessage(content='The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d5463a0-2721-4dcd-98ff-6e9ecde8ab84-0', usage_metadata={'input_tokens': 1069, 'output_tokens': 26, 'total_tokens': 1095}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), tags=[], child_runs=[Run(id=UUID('5fd442e2-38cf-45c5-94cc-ea7b295cb8f3'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 310313, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 500472, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 310313, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 500472, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, reference_example_id=None, parent_run_id=UUID('ce4a6be0-4272-4dbb-8554-1a4db668ca62'), tags=['seq:step:1'], child_runs=[Run(id=UUID('df03c1c8-733c-49d1-a955-0b7eae9d802d'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 749364, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 415679, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 749364, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 415679, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('5fd442e2-38cf-45c5-94cc-ea7b295cb8f3'), tags=['map:key:context'], child_runs=[Run(id=UUID('6f9aa251-b19e-4254-8510-d1f040df15ce'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 886974, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 316957, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 886974, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 316957, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, reference_example_id=None, parent_run_id=UUID('df03c1c8-733c-49d1-a955-0b7eae9d802d'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002652310313Z5fd442e2-38cf-45c5-94cc-ea7b295cb8f3.20240628T002652749364Zdf03c1c8-733c-49d1-a955-0b7eae9d802d.20240628T002652886974Z6f9aa251-b19e-4254-8510-d1f040df15ce'), Run(id=UUID('d827d4bb-64d4-4e33-b521-a055f0ce33f9'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 318425, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 343223, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 318425, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 343223, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'documents': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('df03c1c8-733c-49d1-a955-0b7eae9d802d'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002652310313Z5fd442e2-38cf-45c5-94cc-ea7b295cb8f3.20240628T002652749364Zdf03c1c8-733c-49d1-a955-0b7eae9d802d.20240628T002653318425Zd827d4bb-64d4-4e33-b521-a055f0ce33f9')], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002652310313Z5fd442e2-38cf-45c5-94cc-ea7b295cb8f3.20240628T002652749364Zdf03c1c8-733c-49d1-a955-0b7eae9d802d'), Run(id=UUID('a9a32394-2f3f-4d03-9094-2b07ae17a8fc'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 37688, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 545317, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 37688, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 545317, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, reference_example_id=None, parent_run_id=UUID('5fd442e2-38cf-45c5-94cc-ea7b295cb8f3'), tags=['map:key:question'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002652310313Z5fd442e2-38cf-45c5-94cc-ea7b295cb8f3.20240628T002653037688Za9a32394-2f3f-4d03-9094-2b07ae17a8fc')], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002652310313Z5fd442e2-38cf-45c5-94cc-ea7b295cb8f3'), Run(id=UUID('ef07e439-26ee-4869-a7b5-7151e6210481'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 749611, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 749611, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('ce4a6be0-4272-4dbb-8554-1a4db668ca62'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002720749611Zef07e439-26ee-4869-a7b5-7151e6210481'), Run(id=UUID('9f3c3103-63b3-43e3-8ed5-a5ec6268b98b'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 864796, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 807914, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 864796, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 807914, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'response': AIMessage(content='The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d5463a0-2721-4dcd-98ff-6e9ecde8ab84-0', usage_metadata={'input_tokens': 1069, 'output_tokens': 26, 'total_tokens': 1095}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('ce4a6be0-4272-4dbb-8554-1a4db668ca62'), tags=['seq:step:3'], child_runs=[Run(id=UUID('4ba56bcf-9cc9-4b2e-8f07-df5deea12051'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 171274, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 798728, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 171274, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 798728, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': AIMessage(content='The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d5463a0-2721-4dcd-98ff-6e9ecde8ab84-0', usage_metadata={'input_tokens': 1069, 'output_tokens': 26, 'total_tokens': 1095})}, reference_example_id=None, parent_run_id=UUID('9f3c3103-63b3-43e3-8ed5-a5ec6268b98b'), tags=['map:key:response'], child_runs=[Run(id=UUID('6077200a-3572-4ced-bb11-6c09077f86cc'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 688158, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 411821, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 688158, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 411821, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content=\"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\\\n\\\\nBy LangChain\\\\n4 min read\\\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\\\nBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Case Accelerant: Extraction Service\\\\n\\\\n\\\\nBy LangChain\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 2/19] LangChain Release Notes\\\\n\\\\n\\\\nBy LangChain\\\\n1 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReflection Agents\\\\n\\\\n\\\\nagents\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\\\nBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Case Accelerant: Extraction Service\\\\n\\\\n\\\\nBy LangChain\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 2/19] LangChain Release Notes\\\\n\\\\n\\\\nBy LangChain\\\\n1 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReflection Agents\\\\n\\\\n\\\\nagents\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]\\n\\nQuestion:\\nWhat is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?\\n\")])}, reference_example_id=None, parent_run_id=UUID('4ba56bcf-9cc9-4b2e-8f07-df5deea12051'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002721864796Z9f3c3103-63b3-43e3-8ed5-a5ec6268b98b.20240628T002722171274Z4ba56bcf-9cc9-4b2e-8f07-df5deea12051.20240628T002722688158Z6077200a-3572-4ced-bb11-6c09077f86cc'), Run(id=UUID('6d5463a0-2721-4dcd-98ff-6e9ecde8ab84'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 558829, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 798257, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 558829, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 798257, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\\\n\\\\nBy LangChain\\\\n4 min read\\\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\\\nBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Case Accelerant: Extraction Service\\\\n\\\\n\\\\nBy LangChain\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 2/19] LangChain Release Notes\\\\n\\\\n\\\\nBy LangChain\\\\n1 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReflection Agents\\\\n\\\\n\\\\nagents\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\\\nBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Case Accelerant: Extraction Service\\\\n\\\\n\\\\nBy LangChain\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 2/19] LangChain Release Notes\\\\n\\\\n\\\\nBy LangChain\\\\n1 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReflection Agents\\\\n\\\\n\\\\nagents\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]\\n\\nQuestion:\\nWhat is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?\\n\", 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The title of the article is \"How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x.\"', 'response_metadata': {'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-6d5463a0-2721-4dcd-98ff-6e9ecde8ab84-0', 'usage_metadata': {'input_tokens': 1069, 'output_tokens': 26, 'total_tokens': 1095}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1069, 'total_tokens': 1095}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('4ba56bcf-9cc9-4b2e-8f07-df5deea12051'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002721864796Z9f3c3103-63b3-43e3-8ed5-a5ec6268b98b.20240628T002722171274Z4ba56bcf-9cc9-4b2e-8f07-df5deea12051.20240628T002723558829Z6d5463a0-2721-4dcd-98ff-6e9ecde8ab84')], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002721864796Z9f3c3103-63b3-43e3-8ed5-a5ec6268b98b.20240628T002722171274Z4ba56bcf-9cc9-4b2e-8f07-df5deea12051'), Run(id=UUID('1e752fab-a65f-4356-9d13-1326702f1886'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 211011, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 842225, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 211011, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 842225, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})], 'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'output': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/opengpts/', 'loc': 'https://blog.langchain.dev/opengpts/', 'lastmod': '2024-01-31T16:48:41.000Z'}), Document(page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Integrates NVIDIA NIM for GPU-optimized LLM Inference in RAG\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse Case Accelerant: Extraction Service\\n\\n\\nBy LangChain\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 2/19] LangChain Release Notes\\n\\n\\nBy LangChain\\n1 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReflection Agents\\n\\n\\nagents\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/callbacks/', 'loc': 'https://blog.langchain.dev/callbacks/', 'lastmod': '2023-08-18T22:00:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9f3c3103-63b3-43e3-8ed5-a5ec6268b98b'), tags=['map:key:context'], child_runs=[], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002721864796Z9f3c3103-63b3-43e3-8ed5-a5ec6268b98b.20240628T002722211011Z1e752fab-a65f-4356-9d13-1326702f1886')], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62.20240628T002721864796Z9f3c3103-63b3-43e3-8ed5-a5ec6268b98b')], trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24'), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24.20240628T002652076644Zce4a6be0-4272-4dbb-8554-1a4db668ca62')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651359479Zfed70037-e7e5-4258-8785-7ecc917b7a24', trace_id=UUID('fed70037-e7e5-4258-8785-7ecc917b7a24')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the title of the article that discusses using LangSmith to automate a feedback loop and improve iteration speed?'}, outputs={'answer': 'The LangChain team and community provide updates through a newsletter. How can one subscribe to it?\\n\\nEnter your email and click the \"Subscribe\" button. Then check your inbox and click the link to confirm your subscription.'}, metadata={'dataset_split': ['base']}, id=UUID('1bc51d82-e378-433f-8826-af05ef54ab03'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 420688, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 420688, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3fbf6ef3-846f-4978-9a3f-8d7f88067407'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3f012568-b19b-4a0f-9528-54f9e01fd6dd'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a8fa131e-ca9e-4306-b6e4-005af1a0c09e'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('abb23f4d-da7b-4d94-ba15-d4e856f89649'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('adb5573c-e0fc-468b-938a-58c792ae78b8'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 52431, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 347953, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.315616+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 87.0, 'mem': {'rss': 1290809344.0}, 'cpu': {'time': {'sys': 300.04, 'user': 140.98}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What does synthetic data prevent in the context of privacy and data security?'}, 'config': None}, outputs={'response': AIMessage(content='Synthetic data prevents re-identification attacks in the context of privacy and data security.', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5abc4e0a-08e5-4a0b-9b04-6b30e053711a-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 17, 'total_tokens': 1050}), 'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=UUID('76c3cdf9-a5a4-4462-b37e-1d4ecace07ba'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('24699697-1ccd-4976-b35c-cd8d35d767e5'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 283224, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 469, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 283224, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 469, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'response': AIMessage(content='Synthetic data prevents re-identification attacks in the context of privacy and data security.', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5abc4e0a-08e5-4a0b-9b04-6b30e053711a-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 17, 'total_tokens': 1050}), 'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=None, parent_run_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), tags=[], child_runs=[Run(id=UUID('90189df0-19ca-48bf-b815-a004a2d2cf1b'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 464125, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 16, 959622, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 464125, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 16, 959622, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, reference_example_id=None, parent_run_id=UUID('24699697-1ccd-4976-b35c-cd8d35d767e5'), tags=['seq:step:1'], child_runs=[Run(id=UUID('09415192-29c0-4431-8d72-2748b558dd68'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 853277, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 15, 614561, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 853277, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 15, 614561, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=None, parent_run_id=UUID('90189df0-19ca-48bf-b815-a004a2d2cf1b'), tags=['map:key:context'], child_runs=[Run(id=UUID('6b08ca7f-2645-4a0c-b189-a084acae4e69'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 148268, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 273374, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 148268, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 273374, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': 'What does synthetic data prevent in the context of privacy and data security?'}, reference_example_id=None, parent_run_id=UUID('09415192-29c0-4431-8d72-2748b558dd68'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002651464125Z90189df0-19ca-48bf-b815-a004a2d2cf1b.20240628T002651853277Z09415192-29c0-4431-8d72-2748b558dd68.20240628T002652148268Z6b08ca7f-2645-4a0c-b189-a084acae4e69'), Run(id=UUID('8523c4d1-471d-4090-8113-fceaeebce855'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 274017, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 15, 613519, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 274017, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 15, 613519, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'documents': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=None, parent_run_id=UUID('09415192-29c0-4431-8d72-2748b558dd68'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002651464125Z90189df0-19ca-48bf-b815-a004a2d2cf1b.20240628T002651853277Z09415192-29c0-4431-8d72-2748b558dd68.20240628T002652274017Z8523c4d1-471d-4090-8113-fceaeebce855')], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002651464125Z90189df0-19ca-48bf-b815-a004a2d2cf1b.20240628T002651853277Z09415192-29c0-4431-8d72-2748b558dd68'), Run(id=UUID('47f61927-6428-4cc5-ac61-f4e8901e11cd'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 35716, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 267676, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 35716, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 267676, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': 'What does synthetic data prevent in the context of privacy and data security?'}, reference_example_id=None, parent_run_id=UUID('90189df0-19ca-48bf-b815-a004a2d2cf1b'), tags=['map:key:question'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002651464125Z90189df0-19ca-48bf-b815-a004a2d2cf1b.20240628T002652035716Z47f61927-6428-4cc5-ac61-f4e8901e11cd')], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002651464125Z90189df0-19ca-48bf-b815-a004a2d2cf1b'), Run(id=UUID('3f5d17cb-0741-4e16-aa29-1544da1aae20'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 16, 965427, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 16, 965427, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('24699697-1ccd-4976-b35c-cd8d35d767e5'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002716965427Z3f5d17cb-0741-4e16-aa29-1544da1aae20'), Run(id=UUID('0db2190e-9660-4e1c-86b2-6f5a528335a9'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 119840, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 882706, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 19, 119840, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 882706, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'response': AIMessage(content='Synthetic data prevents re-identification attacks in the context of privacy and data security.', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5abc4e0a-08e5-4a0b-9b04-6b30e053711a-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 17, 'total_tokens': 1050}), 'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=None, parent_run_id=UUID('24699697-1ccd-4976-b35c-cd8d35d767e5'), tags=['seq:step:3'], child_runs=[Run(id=UUID('aa2a2faf-836f-4170-bfdf-1328a29af0bd'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 668116, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 36481, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 668116, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 36481, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})]}, reference_example_id=None, parent_run_id=UUID('0db2190e-9660-4e1c-86b2-6f5a528335a9'), tags=['map:key:context'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002719119840Z0db2190e-9660-4e1c-86b2-6f5a528335a9.20240628T002720668116Zaa2a2faf-836f-4170-bfdf-1328a29af0bd'), Run(id=UUID('e1f2075b-b0ac-43f2-99c7-73158b884df4'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 707122, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 691541, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 707122, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 691541, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': AIMessage(content='Synthetic data prevents re-identification attacks in the context of privacy and data security.', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5abc4e0a-08e5-4a0b-9b04-6b30e053711a-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 17, 'total_tokens': 1050})}, reference_example_id=None, parent_run_id=UUID('0db2190e-9660-4e1c-86b2-6f5a528335a9'), tags=['map:key:response'], child_runs=[Run(id=UUID('94151c5c-8ac5-4407-8f6d-ecfb3f9af7f7'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 998001, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 326031, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 998001, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 326031, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you'll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user's input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you're asking for.Generating synthetic tabular dataBefore diving into the example, let's talk about synthetic data. With Gretel's models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we'll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel's quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'}), Document(page_content='run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\'s data-centric world, adopting these technologies isn\\'t just an option; it\\'s a must.If you\\'re ready to up your data game, sign up for Gretel today and start synthesizing.', metadata={'source': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'loc': 'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/', 'lastmod': '2023-09-18T15:58:13.000Z'})], 'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you\\'ll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user\\'s input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you\\'re asking for.Generating synthetic tabular dataBefore diving into the example, let\\'s talk about synthetic data. With Gretel\\'s models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we\\'ll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel\\'s quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={\\'source\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'loc\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'lastmod\\': \\'2023-09-18T15:58:13.000Z\\'}), Document(page_content=\\'run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\\\\\'s data-centric world, adopting these technologies isn\\\\\\'t just an option; it\\\\\\'s a must.If you\\\\\\'re ready to up your data game, sign up for Gretel today and start synthesizing.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'loc\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'lastmod\\': \\'2023-09-18T15:58:13.000Z\\'})]\\n\\nQuestion:\\nWhat does synthetic data prevent in the context of privacy and data security?\\n')])}, reference_example_id=None, parent_run_id=UUID('e1f2075b-b0ac-43f2-99c7-73158b884df4'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002719119840Z0db2190e-9660-4e1c-86b2-6f5a528335a9.20240628T002720707122Ze1f2075b-b0ac-43f2-99c7-73158b884df4.20240628T002720998001Z94151c5c-8ac5-4407-8f6d-ecfb3f9af7f7'), Run(id=UUID('5abc4e0a-08e5-4a0b-9b04-6b30e053711a'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 436602, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 567225, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 436602, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 567225, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"generative models that can offer strong privacy guarantees to data. Gretel offers generative models for working with tabular data based on Transformer, GAN, and graph-based architectures.SQL Databases: The backbone holding the data you\\'ll be querying. For today, we’ll use a SQLite database.What is an Agent in LangChain?Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user\\'s input, too. In these types of chains, there is an “agent” that has access to a suite of tools — for example math, or the ability to query a SQL database. Depending on the user input, the agent can then decide which, if any, of these tools to call.Under the hood, the LangChain SQL Agent uses a MRKL (pronounced Miracle)-based approach, and queries the database schema and example rows and uses these to generate SQL queries, which it then executes to pull back the results you\\'re asking for.Generating synthetic tabular dataBefore diving into the example, let\\'s talk about synthetic data. With Gretel\\'s models, you can make an artificial but statistically similar version of your sensitive data. This synthetic data is safe to use, thanks to math-backed privacy features like differential privacy. In our example, we\\'ll use both real and synthetic data to show why this privacy is crucial when letting language models access sensitive info.To generate your own synthetic data for this example, grab the IBM HR Employee Attrition dataset (or your own) and an API key from https://console.gretel.ai. You can run Gretel\\'s quickstart notebook or console-based workflow to create a synthetic version of the data.For this example, I used the Gretel Tabular DP model (notebook, docs) with an epsilon value of 5 for strong privacy guarantees that are great for regulated environments. For maximum accuracy while still maintaining privacy, you can also try the Gretel ACTGAN model (docs), which excels at working with highly\", metadata={\\'source\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'loc\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'lastmod\\': \\'2023-09-18T15:58:13.000Z\\'}), Document(page_content=\\'run_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)Figure 3. Comparing real and synthetic results for query #3.In this case, we get a perfect match.Importance of privacy: Re-identification attack exampleHere, we illustrate a \"re-identification attack\" where vulnerabilities in even de-identified datasets can allow an attacker to re-identify individuals by combining known attributes. Such risks emphasize the danger of sharing data stripped of direct identifiers yet containing attributes that, when combined, can lead to identification — such as the combination of an attacker who knew someone’s age, gender, and department in the example below.Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.prompt = \"Is there an employee who is Age 46, Female, and who works in Human Resources. If so, what is their monthly income, performance rating, and years since their last promotion?\"\\\\nrun_and_compare_queries(synthetic=agent_synthetic_db, real=agent_real_db, query=prompt)\\\\nConclusionBy using synthetic data, you not only protect privacy but also gain actionable insights—essential for any data-driven organization. When you blend this with agent-based approaches and large language models, you open the door for more and better stakeholder collaborations. No SQL expertise needed; simply use natural language to engage with your data across all levels of your organization.This scalable solution democratizes data access and ushers in a new era of smart, privacy-conscious data interaction. For businesses eager to maintain a competitive edge in today\\\\\\'s data-centric world, adopting these technologies isn\\\\\\'t just an option; it\\\\\\'s a must.If you\\\\\\'re ready to up your data game, sign up for Gretel today and start synthesizing.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'loc\\': \\'https://blog.langchain.dev/how-to-safely-query-enterprise-data-with-langchain-agents-sql-openai-gretel/\\', \\'lastmod\\': \\'2023-09-18T15:58:13.000Z\\'})]\\n\\nQuestion:\\nWhat does synthetic data prevent in the context of privacy and data security?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'Synthetic data prevents re-identification attacks in the context of privacy and data security.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Synthetic data prevents re-identification attacks in the context of privacy and data security.', 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-5abc4e0a-08e5-4a0b-9b04-6b30e053711a-0', 'usage_metadata': {'input_tokens': 1033, 'output_tokens': 17, 'total_tokens': 1050}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 1033, 'total_tokens': 1050}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('e1f2075b-b0ac-43f2-99c7-73158b884df4'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002719119840Z0db2190e-9660-4e1c-86b2-6f5a528335a9.20240628T002720707122Ze1f2075b-b0ac-43f2-99c7-73158b884df4.20240628T002721436602Z5abc4e0a-08e5-4a0b-9b04-6b30e053711a')], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002719119840Z0db2190e-9660-4e1c-86b2-6f5a528335a9.20240628T002720707122Ze1f2075b-b0ac-43f2-99c7-73158b884df4')], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5.20240628T002719119840Z0db2190e-9660-4e1c-86b2-6f5a528335a9')], trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c'), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c.20240628T002651283224Z24699697-1ccd-4976-b35c-cd8d35d767e5')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651052431Ze2f0781c-3563-4f1a-afcd-7e9a0bc4c69c', trace_id=UUID('e2f0781c-3563-4f1a-afcd-7e9a0bc4c69c')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What does synthetic data prevent in the context of privacy and data security?'}, outputs={'answer': 'Synthetic data prevents direct linking of individual information as no record in the output is based on a single user’s data, effectively thwarting re-identification attacks and upholding privacy.'}, metadata={'dataset_split': ['base']}, id=UUID('76c3cdf9-a5a4-4462-b37e-1d4ecace07ba'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 315616, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 315616, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('801c760c-6d4e-46ca-8b59-8fa20617e0cb'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a0b2e2cb-8583-4e14-bb0c-210879b07f2f'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=6, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8c29c455-5022-4815-926e-5ebc72349509'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ded89834-e8b2-40cc-9211-58294e9674fd'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d0389c61-ecbd-4f91-bf8d-83b38a0357db'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 393274, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 351887, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.314214+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, 'config': None}, outputs={'response': AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-82a6514d-6efc-49d2-92cb-a6a1e3016c8f-0', usage_metadata={'input_tokens': 1579, 'output_tokens': 5, 'total_tokens': 1584}), 'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=UUID('a3ff8b4d-fef5-4713-9baa-0f56375ad587'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('d1c7ee47-02af-4ed8-811a-1f6287b9fc23'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 703254, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 349759, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 703254, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 349759, tzinfo=datetime.timezone.utc)}], inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'response': AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-82a6514d-6efc-49d2-92cb-a6a1e3016c8f-0', usage_metadata={'input_tokens': 1579, 'output_tokens': 5, 'total_tokens': 1584}), 'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=None, parent_run_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), tags=[], child_runs=[Run(id=UUID('2bfd108d-b211-429d-8e9f-69da60e6ac9c'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 961170, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 752353, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 961170, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 752353, tzinfo=datetime.timezone.utc)}], inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, reference_example_id=None, parent_run_id=UUID('d1c7ee47-02af-4ed8-811a-1f6287b9fc23'), tags=['seq:step:1'], child_runs=[Run(id=UUID('f03ece22-6e69-4333-8981-904e191006f0'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 313317, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 488168, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 313317, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 488168, tzinfo=datetime.timezone.utc)}], inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=None, parent_run_id=UUID('2bfd108d-b211-429d-8e9f-69da60e6ac9c'), tags=['map:key:context'], child_runs=[Run(id=UUID('8b131254-7505-4c2b-8a58-25749e2591c6'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 508564, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 716497, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 508564, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 716497, tzinfo=datetime.timezone.utc)}], inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, reference_example_id=None, parent_run_id=UUID('f03ece22-6e69-4333-8981-904e191006f0'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002651961170Z2bfd108d-b211-429d-8e9f-69da60e6ac9c.20240628T002652313317Zf03ece22-6e69-4333-8981-904e191006f0.20240628T002652508564Z8b131254-7505-4c2b-8a58-25749e2591c6'), Run(id=UUID('6d558988-2e81-48ba-b286-1467ab8c8b9d'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 774902, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 432267, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 774902, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 432267, tzinfo=datetime.timezone.utc)}], inputs={'query': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'documents': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=None, parent_run_id=UUID('f03ece22-6e69-4333-8981-904e191006f0'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002651961170Z2bfd108d-b211-429d-8e9f-69da60e6ac9c.20240628T002652313317Zf03ece22-6e69-4333-8981-904e191006f0.20240628T002652774902Z6d558988-2e81-48ba-b286-1467ab8c8b9d')], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002651961170Z2bfd108d-b211-429d-8e9f-69da60e6ac9c.20240628T002652313317Zf03ece22-6e69-4333-8981-904e191006f0'), Run(id=UUID('71c2f637-3418-43e4-8127-853202aa2b26'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 558053, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 840854, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 558053, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 840854, tzinfo=datetime.timezone.utc)}], inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, reference_example_id=None, parent_run_id=UUID('2bfd108d-b211-429d-8e9f-69da60e6ac9c'), tags=['map:key:question'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002651961170Z2bfd108d-b211-429d-8e9f-69da60e6ac9c.20240628T002652558053Z71c2f637-3418-43e4-8127-853202aa2b26')], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002651961170Z2bfd108d-b211-429d-8e9f-69da60e6ac9c'), Run(id=UUID('3ac55d59-dc0a-4675-9573-502a04dd5f95'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 872132, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 872132, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs=None, reference_example_id=None, parent_run_id=UUID('d1c7ee47-02af-4ed8-811a-1f6287b9fc23'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002720872132Z3ac55d59-dc0a-4675-9573-502a04dd5f95'), Run(id=UUID('4a184880-f23d-487b-ab4d-200bded8a0e0'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 606017, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 348807, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 606017, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 348807, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'response': AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-82a6514d-6efc-49d2-92cb-a6a1e3016c8f-0', usage_metadata={'input_tokens': 1579, 'output_tokens': 5, 'total_tokens': 1584}), 'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=None, parent_run_id=UUID('d1c7ee47-02af-4ed8-811a-1f6287b9fc23'), tags=['seq:step:3'], child_runs=[Run(id=UUID('b54784e9-8d3a-4583-96ed-bdffe0e0235c'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 144516, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 347623, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 144516, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 347623, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-82a6514d-6efc-49d2-92cb-a6a1e3016c8f-0', usage_metadata={'input_tokens': 1579, 'output_tokens': 5, 'total_tokens': 1584})}, reference_example_id=None, parent_run_id=UUID('4a184880-f23d-487b-ab4d-200bded8a0e0'), tags=['map:key:response'], child_runs=[Run(id=UUID('1c0d63a1-2686-4532-9241-5069eae4f82b'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 400389, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 552508, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 400389, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 552508, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content=\"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]\\n\\nQuestion:\\nList the titles of the works authored by Aniket Maurya and Carter Rabasa.\\n\")])}, reference_example_id=None, parent_run_id=UUID('b54784e9-8d3a-4583-96ed-bdffe0e0235c'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002722606017Z4a184880-f23d-487b-ab4d-200bded8a0e0.20240628T002723144516Zb54784e9-8d3a-4583-96ed-bdffe0e0235c.20240628T002723400389Z1c0d63a1-2686-4532-9241-5069eae4f82b'), Run(id=UUID('82a6514d-6efc-49d2-92cb-a6a1e3016c8f'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 624892, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 346951, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 624892, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 346951, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]\\n\\nQuestion:\\nList the titles of the works authored by Aniket Maurya and Carter Rabasa.\\n\", 'type': 'human'}}]]}, outputs={'generations': [[{'text': \"I don't know.\", 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"I don't know.\", 'response_metadata': {'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-82a6514d-6efc-49d2-92cb-a6a1e3016c8f-0', 'usage_metadata': {'input_tokens': 1579, 'output_tokens': 5, 'total_tokens': 1584}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 5, 'prompt_tokens': 1579, 'total_tokens': 1584}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('b54784e9-8d3a-4583-96ed-bdffe0e0235c'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002722606017Z4a184880-f23d-487b-ab4d-200bded8a0e0.20240628T002723144516Zb54784e9-8d3a-4583-96ed-bdffe0e0235c.20240628T002723624892Z82a6514d-6efc-49d2-92cb-a6a1e3016c8f')], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002722606017Z4a184880-f23d-487b-ab4d-200bded8a0e0.20240628T002723144516Zb54784e9-8d3a-4583-96ed-bdffe0e0235c'), Run(id=UUID('765dbd23-a722-43d5-8077-94d418e77476'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 181459, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 475568, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 181459, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 475568, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})], 'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'output': [Document(page_content='__Jay__0xcha050xcrusher0xDTE0xJordan134ARGAaaCabbageAarav BorthakurAaron PhamAashish SainiAashutoshPathakShorthillsAIAayush ShahAbdelsalam ElTamawyAbdulla Al BlooshiAbhijeet MalamkarAbhik SinglaAbhinav UpadhyayAbhishekYadavShorthillsAIAbi RajaAbonia SojasingarayarAce EldeibAckermann YuriyAdam DemjenAdam GutglickAdam McCabeAdam Quigleyadam91holtAdarsh ShirawalmathAdarsh ShrivastavAdheeban ManoharanAdil AnsariadilkhanAdilkhan SarsenAdilzhan IsmailovAditi ViswanathanAditya Sadrienohanaaerickson-cltAeroXiAhmad BunniAI-ChefAidan BolandAidan HollandAiden LeAivin V. SolatorioAkashAkash NPAkash SamantAkash SharmaAkhil VempaliAkio NishimuraakmhmgcAkshayAkshay TripathiAkshaya AnnavajhalaAlan ChaAlan deLevieAlbert CastellanaAlbert ZieglerAlec FlettAlejandra De LunaAlejandro Garrido MotaalekhyablueAlexAlex GambleAlex IribarrenAlex LeeAlex RadAlex RothbergAlex StachowiakAlex Strick van LinschotenAlex TelonAlexander DibrovAlexander HoyleAlexander Miasoiedov (Myasoedov)Alexander WeichartAlexandre PesantAlexandros MavrogiannisAlexey NominasAlfrick OpidiAli SolimanAlon DiamentAlon RothAlonso Silva AllendeAlpri ElseAltay SansalaltryneAlvaro BartolomeAmbuj PawarAmélieAmir KarimiAmitSinghShorthillsAIAmos NgAnam HiraAnarAnatolii KmetiukandersenchenAndre ElizondoAndrea PintoAndreas LiebschnerAndreiAndreLCanadaAndrew GleaveAndrew GrangaardAndrew LeiAndrew SwitlykAndrew WangAndrew WhiteandrewmelisAndrey AvtomonovAndrey E. VedishchevAndrey VasnetsovAndriy MulyarAngel LuisAni peter benjaminanifortAnirudh SureshAnish ShahAnkit AryaAnkur AgarwalAnsil M BAnthony MahannaAnton DanylchenkoAnton TroynikovAnubhav BindlishAnujMauryaShorthillsAIanupam-tiwariAnuragAprilApurv AgarwalapurvsibalAratakoArchimedesFTWArchonArjun AravindanarjunbansalArpan PokharelArthur TeldersArttiiAsh VardanianAshish Talatiashish-dahalAshutosh SanzgiriAsif Ahmadat-b612Ati SharmaAttila TőkésAugustine TheodoreAurélien SCHILTZausbossAustinAustin WalkerAvinash Rajaxa99axiangcodingAyan BandyopadhyayAymen FurterAzam', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'}), Document(page_content='MarellaRaymond YuanRaynor ChavezRaza HabibRenze YuresearchonlyRian DolphinRicardo ReisRichard AdamsRichard HeRichard WangRiche AkparuorjiRichy Wangricki-epsillaRishabh RaizadaRithwik Ediga LakhamsaniRiyadh Rahmanrjanardhan3rjarun8rkeshwaniRobert LewisRobert PerrottaRobert YiRodrigo Siqueirarodrigo-clickupRoger YuRoger ZurawickiRogério ChavesrogerserperRohit GuptaRohit Kumar SinghRomaRoman ShaptalaRonald LiRosário P. FernandesRostyslav KinashRounak DattaRoy WilliamsRoy XueRubén BarragánRubén MartínezRubens MauRui FerreiraRuiqi GuoRuixi FanRukmal WeerawaranaRukmaniRussruzeRyan CulliganRyan DaoRyan SloanRyan WalkerRyan ZottiRyo KanazawaSaarthak MainiSaba SturuaSachin VargheseSai Vinay GSajal SharmaSamSam ChingSam ChouSam Cordner-MatthewsSam CowardSam GroenjesSam HoganSam ParteeSam WeaverSamantha WhitmoreSamhita AllaSami LiedesSamuel BertheSamuel Dion-GirardeauSamuel ROZESamuli RauatmaaSangamSwadiKSanskar TanwarSantiago DelgadoSasmitha ManathungaSasonSatheesh ValluruSathindusatoriohSatoru SakamotoSaurabh ChaturvediSaurabh MisraSaurav MaheshkarSaverio ProtoscadEfUrschop-robScott LeibrandScottyseamuspSean MorganSean ShengSean ZhengseanaedmistonSebastianSebastien KerbratSeifsergerdnSergey KozlovSergio MorenosergiolrinditexSertaç ÖzercanShahriar TajbakhshShantanu NairSharath RajasekarShashankShashank DeshpandeShawn91Sheik Irfan BashaShelby JenkinsSheldonSheng Han LimsherylZhaoCodeshibuiwilliamShinya MaedaShishin Moshiyu22Shobith AlvaShorthills AIShota TerashitaShotaro KohamaShreya RajpalShreyas SShrinedShuShubham KushwahaShuchang ZhouShukriShuqianShyamal H AnadkatSian CaoSidchat95Simba KhadderSimFGSimon CheungSimon DaiSimon ZhouSiraj AizlewoodsksparkSlapDroneSlawomir GonetSmit ShahSnehil Kumarso2liuSoos3DSparsh Jainsqrsseidest01csStan Girardstandby24x7Stanko KuveljicstaoxiaoStav SapirStefano LottiniStéphane BussoStephaneBereuxStephen HankinsonSteve KimSteven HoelscherstonekimstopdropandrewSubsegmentsudolongsudrangasumandengSumanth DonthulaSun binSunish ShethSurav', metadata={'source': 'https://blog.langchain.dev/langchains-first-birthday/', 'loc': 'https://blog.langchain.dev/langchains-first-birthday/', 'lastmod': '2023-10-25T18:21:36.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4a184880-f23d-487b-ab4d-200bded8a0e0'), tags=['map:key:context'], child_runs=[], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002722606017Z4a184880-f23d-487b-ab4d-200bded8a0e0.20240628T002723181459Z765dbd23-a722-43d5-8077-94d418e77476')], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23.20240628T002722606017Z4a184880-f23d-487b-ab4d-200bded8a0e0')], trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c'), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c.20240628T002651703254Zd1c7ee47-02af-4ed8-811a-1f6287b9fc23')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651393274Ze4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c', trace_id=UUID('e4b2f4b8-aa97-4f5f-9d00-a58b9ab5041c')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'List the titles of the works authored by Aniket Maurya and Carter Rabasa.'}, outputs={'answer': 'Kristian Freeman, and Jacob Lee'}, metadata={'dataset_split': ['base']}, id=UUID('a3ff8b4d-fef5-4713-9baa-0f56375ad587'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 314214, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 314214, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0795990e-22a0-4db7-a2bc-7340285e1f2f'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c6217866-a224-4eea-a4e6-bdaa4d178bfa'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8d53622d-63df-4fdd-9edf-1ad254cffdbf'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b40e478c-ac31-4af1-8083-221b98257b28'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('90cbdbcc-c0de-4e04-985f-a89d73524194'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 658779, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 906685, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:19.152025+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 51.0, 'mem': {'rss': 1279164416.0}, 'cpu': {'time': {'sys': 300.3, 'user': 142.08}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, 'config': None}, outputs={'response': AIMessage(content=\"I don't know the answer to that question based on the provided context.\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2095a2ab-e374-4dbe-8854-3cf6685aabeb-0', usage_metadata={'input_tokens': 1470, 'output_tokens': 15, 'total_tokens': 1485}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=UUID('88cc51d1-02c6-403a-8491-739d36c1db79'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('73a8ef04-fc5e-4b4b-81b8-bd25fa52e474'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 207788, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 903813, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 207788, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 903813, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'response': AIMessage(content=\"I don't know the answer to that question based on the provided context.\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2095a2ab-e374-4dbe-8854-3cf6685aabeb-0', usage_metadata={'input_tokens': 1470, 'output_tokens': 15, 'total_tokens': 1485}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), tags=[], child_runs=[Run(id=UUID('95fb5794-7f5e-451d-8efb-7b9a78cde44c'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 602590, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 390141, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 602590, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 390141, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, reference_example_id=None, parent_run_id=UUID('73a8ef04-fc5e-4b4b-81b8-bd25fa52e474'), tags=['seq:step:1'], child_runs=[Run(id=UUID('03f5cbf6-7a15-4132-8104-8346500e56fc'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 911773, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 224126, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 911773, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 224126, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('95fb5794-7f5e-451d-8efb-7b9a78cde44c'), tags=['map:key:context'], child_runs=[Run(id=UUID('9ef76555-5644-4059-a178-a84c603e63a7'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 184189, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 450336, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 184189, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 450336, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, reference_example_id=None, parent_run_id=UUID('03f5cbf6-7a15-4132-8104-8346500e56fc'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002720602590Z95fb5794-7f5e-451d-8efb-7b9a78cde44c.20240628T002720911773Z03f5cbf6-7a15-4132-8104-8346500e56fc.20240628T002721184189Z9ef76555-5644-4059-a178-a84c603e63a7'), Run(id=UUID('1cb3f60b-7704-4f69-894f-5537fb62643d'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 520328, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 142551, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 520328, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 142551, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'documents': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('03f5cbf6-7a15-4132-8104-8346500e56fc'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002720602590Z95fb5794-7f5e-451d-8efb-7b9a78cde44c.20240628T002720911773Z03f5cbf6-7a15-4132-8104-8346500e56fc.20240628T002721520328Z1cb3f60b-7704-4f69-894f-5537fb62643d')], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002720602590Z95fb5794-7f5e-451d-8efb-7b9a78cde44c.20240628T002720911773Z03f5cbf6-7a15-4132-8104-8346500e56fc'), Run(id=UUID('5916f2c1-30dd-4e71-a197-17dd3022092c'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 945766, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 110621, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 945766, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 110621, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, reference_example_id=None, parent_run_id=UUID('95fb5794-7f5e-451d-8efb-7b9a78cde44c'), tags=['map:key:question'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002720602590Z95fb5794-7f5e-451d-8efb-7b9a78cde44c.20240628T002720945766Z5916f2c1-30dd-4e71-a197-17dd3022092c')], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002720602590Z95fb5794-7f5e-451d-8efb-7b9a78cde44c'), Run(id=UUID('a37bba94-ef1a-4779-b2db-fec3fa68b507'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 481891, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 481891, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('73a8ef04-fc5e-4b4b-81b8-bd25fa52e474'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002723481891Za37bba94-ef1a-4779-b2db-fec3fa68b507'), Run(id=UUID('53f65092-88d8-46c5-9a12-276241276108'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 7844, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 902504, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 7844, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 902504, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'response': AIMessage(content=\"I don't know the answer to that question based on the provided context.\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2095a2ab-e374-4dbe-8854-3cf6685aabeb-0', usage_metadata={'input_tokens': 1470, 'output_tokens': 15, 'total_tokens': 1485}), 'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('73a8ef04-fc5e-4b4b-81b8-bd25fa52e474'), tags=['seq:step:3'], child_runs=[Run(id=UUID('90e136aa-282d-4ba3-9b83-faf715d5caab'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 46810, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 899259, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 46810, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 899259, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': AIMessage(content=\"I don't know the answer to that question based on the provided context.\", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2095a2ab-e374-4dbe-8854-3cf6685aabeb-0', usage_metadata={'input_tokens': 1470, 'output_tokens': 15, 'total_tokens': 1485})}, reference_example_id=None, parent_run_id=UUID('53f65092-88d8-46c5-9a12-276241276108'), tags=['map:key:response'], child_runs=[Run(id=UUID('47e537e3-9b68-4bb1-9f3d-f21f8f579ed6'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 55481, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 57433, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 55481, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 57433, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content=\"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\\\n\\\\nBy LangChain\\\\n4 min read\\\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]\\n\\nQuestion:\\nWhat are the two main announcements made in the LangChain article dated June 27, 2024?\\n\")])}, reference_example_id=None, parent_run_id=UUID('90e136aa-282d-4ba3-9b83-faf715d5caab'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002724007844Z53f65092-88d8-46c5-9a12-276241276108.20240628T002724046810Z90e136aa-282d-4ba3-9b83-faf715d5caab.20240628T002724055481Z47e537e3-9b68-4bb1-9f3d-f21f8f579ed6'), Run(id=UUID('2095a2ab-e374-4dbe-8854-3cf6685aabeb'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 58854, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 891583, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 58854, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 891583, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\\\n\\\\nBy LangChain\\\\n4 min read\\\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\\\nRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/15] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/18] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n3 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 3/4] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n2 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]\\n\\nQuestion:\\nWhat are the two main announcements made in the LangChain article dated June 27, 2024?\\n\", 'type': 'human'}}]]}, outputs={'generations': [[{'text': \"I don't know the answer to that question based on the provided context.\", 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"I don't know the answer to that question based on the provided context.\", 'response_metadata': {'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-2095a2ab-e374-4dbe-8854-3cf6685aabeb-0', 'usage_metadata': {'input_tokens': 1470, 'output_tokens': 15, 'total_tokens': 1485}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1470, 'total_tokens': 1485}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('90e136aa-282d-4ba3-9b83-faf715d5caab'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002724007844Z53f65092-88d8-46c5-9a12-276241276108.20240628T002724046810Z90e136aa-282d-4ba3-9b83-faf715d5caab.20240628T002724058854Z2095a2ab-e374-4dbe-8854-3cf6685aabeb')], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002724007844Z53f65092-88d8-46c5-9a12-276241276108.20240628T002724046810Z90e136aa-282d-4ba3-9b83-faf715d5caab'), Run(id=UUID('7122efa0-a440-4e5d-a891-ee4a100a8db0'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 49968, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 60749, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 49968, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 60749, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})], 'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'output': [Document(page_content='How Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\nHow Factory AI uses LangSmith to debug issues and close the product feedback loop, resulting in a 2x improvement in iteration speed.\\n\\nBy LangChain\\n4 min read\\nJun 19, 2024', metadata={'source': 'https://blog.langchain.dev/customers-factory/', 'loc': 'https://blog.langchain.dev/customers-factory/', 'lastmod': '2024-06-19T17:22:09.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-27-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:42.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-6-10-langchain-release-notes/', 'lastmod': '2024-06-23T00:16:49.000Z'}), Document(page_content='Tags\\nRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/15] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/18] LangChain Release Notes\\n\\n\\nRelease Notes\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 3/4] LangChain Release Notes\\n\\n\\nRelease Notes\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-5-13-langchain-release-notes/', 'lastmod': '2024-05-31T16:43:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('53f65092-88d8-46c5-9a12-276241276108'), tags=['map:key:context'], child_runs=[], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002724007844Z53f65092-88d8-46c5-9a12-276241276108.20240628T002724049968Z7122efa0-a440-4e5d-a891-ee4a100a8db0')], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474.20240628T002724007844Z53f65092-88d8-46c5-9a12-276241276108')], trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754'), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754.20240628T002720207788Z73a8ef04-fc5e-4b4b-81b8-bd25fa52e474')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002719658779Zc3f4dc03-1998-4dd7-945f-c26c7f2e3754', trace_id=UUID('c3f4dc03-1998-4dd7-945f-c26c7f2e3754')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What are the two main announcements made in the LangChain article dated June 27, 2024?'}, outputs={'answer': 'LangGraph v0.1'}, metadata={'dataset_split': ['base']}, id=UUID('88cc51d1-02c6-403a-8491-739d36c1db79'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 152025, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 152025, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0f878f07-4212-464f-92dc-2de886d76a37'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d288736b-4857-4a57-85ce-1c7477cd72dd'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ef04c5c7-d181-4d85-8dad-f6bafe4f61dc'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('98e779bd-00f2-4c10-bb61-bea42fd2fdee'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8a423a65-89d3-49c9-882a-69430e552f03'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 38418, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 668232, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.432912+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 73.0, 'mem': {'rss': 1286811648.0}, 'cpu': {'time': {'sys': 300.18, 'user': 141.44}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, 'config': None}, outputs={'response': AIMessage(content='The built-in type offered by Xata that can be used to store and query vectors is the vector type.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3b49710-e6b8-46d6-a880-f03dfb4fb2c7-0', usage_metadata={'input_tokens': 1310, 'output_tokens': 22, 'total_tokens': 1332}), 'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=UUID('d7d42ad1-db72-4642-aec3-58818b26262e'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('85ec4aa8-91b1-43f7-ba47-90c8aaec140f'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 320850, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 665785, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 320850, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 665785, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'response': AIMessage(content='The built-in type offered by Xata that can be used to store and query vectors is the vector type.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3b49710-e6b8-46d6-a880-f03dfb4fb2c7-0', usage_metadata={'input_tokens': 1310, 'output_tokens': 22, 'total_tokens': 1332}), 'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=None, parent_run_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), tags=[], child_runs=[Run(id=UUID('da629479-49a5-463a-8cd3-73605115db5f'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 778178, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 436114, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 778178, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 436114, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, reference_example_id=None, parent_run_id=UUID('85ec4aa8-91b1-43f7-ba47-90c8aaec140f'), tags=['seq:step:1'], child_runs=[Run(id=UUID('9c038611-3825-4497-bb93-45ebd527fb60'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 263705, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 345659, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 263705, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 345659, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=None, parent_run_id=UUID('da629479-49a5-463a-8cd3-73605115db5f'), tags=['map:key:context'], child_runs=[Run(id=UUID('7c53218d-d85d-44a3-9d0e-67cbe2fcf282'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 396591, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 713273, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 396591, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 713273, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, reference_example_id=None, parent_run_id=UUID('9c038611-3825-4497-bb93-45ebd527fb60'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002651778178Zda629479-49a5-463a-8cd3-73605115db5f.20240628T002652263705Z9c038611-3825-4497-bb93-45ebd527fb60.20240628T002652396591Z7c53218d-d85d-44a3-9d0e-67cbe2fcf282'), Run(id=UUID('7b838d44-f38e-482f-8207-3e5257b4bbd6'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 714239, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 149417, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 714239, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 149417, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'documents': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9c038611-3825-4497-bb93-45ebd527fb60'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002651778178Zda629479-49a5-463a-8cd3-73605115db5f.20240628T002652263705Z9c038611-3825-4497-bb93-45ebd527fb60.20240628T002652714239Z7b838d44-f38e-482f-8207-3e5257b4bbd6')], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002651778178Zda629479-49a5-463a-8cd3-73605115db5f.20240628T002652263705Z9c038611-3825-4497-bb93-45ebd527fb60'), Run(id=UUID('ffe0348b-271b-494e-af6f-e956d05dfeca'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 435540, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 788551, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 435540, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 788551, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, reference_example_id=None, parent_run_id=UUID('da629479-49a5-463a-8cd3-73605115db5f'), tags=['map:key:question'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002651778178Zda629479-49a5-463a-8cd3-73605115db5f.20240628T002652435540Zffe0348b-271b-494e-af6f-e956d05dfeca')], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002651778178Zda629479-49a5-463a-8cd3-73605115db5f'), Run(id=UUID('5dc19f2d-f88a-46f7-962e-914e24965a90'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 518570, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 518570, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('85ec4aa8-91b1-43f7-ba47-90c8aaec140f'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002720518570Z5dc19f2d-f88a-46f7-962e-914e24965a90'), Run(id=UUID('7bba3e80-bd7a-478d-9305-7a88fe085de7'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 48355, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 664357, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 48355, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 664357, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'response': AIMessage(content='The built-in type offered by Xata that can be used to store and query vectors is the vector type.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3b49710-e6b8-46d6-a880-f03dfb4fb2c7-0', usage_metadata={'input_tokens': 1310, 'output_tokens': 22, 'total_tokens': 1332}), 'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=None, parent_run_id=UUID('85ec4aa8-91b1-43f7-ba47-90c8aaec140f'), tags=['seq:step:3'], child_runs=[Run(id=UUID('67aac986-a65b-4cf4-b579-7cfb58393fae'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 362118, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 662889, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 362118, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 662889, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': AIMessage(content='The built-in type offered by Xata that can be used to store and query vectors is the vector type.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3b49710-e6b8-46d6-a880-f03dfb4fb2c7-0', usage_metadata={'input_tokens': 1310, 'output_tokens': 22, 'total_tokens': 1332})}, reference_example_id=None, parent_run_id=UUID('7bba3e80-bd7a-478d-9305-7a88fe085de7'), tags=['map:key:response'], child_runs=[Run(id=UUID('ca651d62-7d2f-494e-aa5b-5c826b9f030c'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 852079, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 188921, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 852079, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 188921, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Editor\\'s Note: This post was written in collaboration with the Xata team. We\\'re excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\\\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\\\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'/* Clear both the vector store and the memory store */\\\\nawait vectorStore.delete({ ids });\\\\nawait memory.clear();\\\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\\\nconst table = \"docs\";\\\\nconst embeddings = new OpenAIEmbeddings();\\\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\\\n\\\\n/* Add documents to the vector store */\\\\nconst docs = [\\\\n  new Document({\\\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\\\n  }),\\\\n  new Document({\\\\n    pageContent:\\\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\\\n  }),\\\\n  new Document({\\\\n    pageContent: \"Xata includes similarity search\",\\\\n  }),\\\\n];\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'})]\\n\\nQuestion:\\nWhat is the built-in type offered by Xata that can be used to store and query vectors?\\n')])}, reference_example_id=None, parent_run_id=UUID('67aac986-a65b-4cf4-b579-7cfb58393fae'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002722048355Z7bba3e80-bd7a-478d-9305-7a88fe085de7.20240628T002722362118Z67aac986-a65b-4cf4-b579-7cfb58393fae.20240628T002722852079Zca651d62-7d2f-494e-aa5b-5c826b9f030c'), Run(id=UUID('e3b49710-e6b8-46d6-a880-f03dfb4fb2c7'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 298464, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 662182, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 298464, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 662182, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Editor\\'s Note: This post was written in collaboration with the Xata team. We\\'re excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\\\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\\\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'/* Clear both the vector store and the memory store */\\\\nawait vectorStore.delete({ ids });\\\\nawait memory.clear();\\\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\\\nconst table = \"docs\";\\\\nconst embeddings = new OpenAIEmbeddings();\\\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\\\n\\\\n/* Add documents to the vector store */\\\\nconst docs = [\\\\n  new Document({\\\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\\\n  }),\\\\n  new Document({\\\\n    pageContent:\\\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\\\n  }),\\\\n  new Document({\\\\n    pageContent: \"Xata includes similarity search\",\\\\n  }),\\\\n];\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'})]\\n\\nQuestion:\\nWhat is the built-in type offered by Xata that can be used to store and query vectors?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The built-in type offered by Xata that can be used to store and query vectors is the vector type.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The built-in type offered by Xata that can be used to store and query vectors is the vector type.', 'response_metadata': {'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-e3b49710-e6b8-46d6-a880-f03dfb4fb2c7-0', 'usage_metadata': {'input_tokens': 1310, 'output_tokens': 22, 'total_tokens': 1332}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1310, 'total_tokens': 1332}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('67aac986-a65b-4cf4-b579-7cfb58393fae'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002722048355Z7bba3e80-bd7a-478d-9305-7a88fe085de7.20240628T002722362118Z67aac986-a65b-4cf4-b579-7cfb58393fae.20240628T002723298464Ze3b49710-e6b8-46d6-a880-f03dfb4fb2c7')], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002722048355Z7bba3e80-bd7a-478d-9305-7a88fe085de7.20240628T002722362118Z67aac986-a65b-4cf4-b579-7cfb58393fae'), Run(id=UUID('d27db8d0-29a7-4ce0-af9f-b6297b2fc301'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 409232, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 260449, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 409232, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 260449, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})], 'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'output': [Document(page_content=\"Editor's Note: This post was written in collaboration with the Xata team. We're excited about their new integrations and really enjoyed their deepdive on implementation a Q&A chat bot with them. \\xa0Over the past few weeks, we’ve merged four Xata integrations to the LangChain repositories, and today we’re happy to unveil them as part of Xata’s launch week! In this blog post, we’ll take a brief look at what Xata is and why it is a good data companion for AI applications. We’ll also show a code example for implementing a Q&A chat bot that answers questions based on the info in a Xata database (as a vector store) and has long-term memory stored in Xata (as a memory store).What is Xata?Xata is a database platform powered by PostgreSQL. It stores the source-of-truth data in PostgreSQL, but also replicates it automatically to Elasticsearch. This means that it offers functionality from both Postgres (ACID transactions, joins, constraints, etc.) and from Elasticsearch (BM25 full-text search, vector search, hybrid search), behind the same simple serverless API. This covers the functionality needed by the majority of AI applications and because it’s based on PostgreSQL and Elasticsearch, it is reliable and scalable.Xata has client SDKs for both TypeScript/JavaScript and Python and built-in integrations with platforms like GitHub, Vercel, and Netlify.In the AI space, beside the LangChain integrations announced here, Xata offers a deep integration with OpenAI for the “ChatGPT on your data” use case.The integrationsAs of today, the following integrations are available :Xata as a vector store in LangChain. This allows one to store documents with embeddings in a Xata table and perform vector search on them. The integration takes advantage of the newly GA-ed Python SDK. The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your\", metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='The integration supports filtering by metadata, which is represented in Xata columns for the maximum performance.Xata as a vector store in LangChain.js. Same as the Python integration, but for your TypeScript/JavaScript applications.Xata as a memory store in LangChain. This allows storing the chat message history for AI chat sessions in Xata, making it work as “memory” for LLM applications.The messages are stored inXata as a memory store in LangChain.js. \\xa0Same as the Python integration, but for TypeScript/JavaScript.Each integration comes with one or two code examples in the doc pages linked above.The four integrations already make Xata one of the most comprehensive data solutions for LangChain, and we’re just getting started! For the near future, we’re planning to add custom retrievers for the Xata keyword and hybrid search and the Xata Ask AI endpoint.Example: Conversational Q&A with memoryWhile each LangChain integration comes with at least one minimal code example, in this blog post we’ll look at a more complex example that uses Xata both as a vector store and as a memory store. The application implements the “chat with your data” use case, and allows for follow-up questions. The full code can be found in this repo, which you can also use as a starter-kit for LangChain + Xata applications.While the example application here is written in TypeScript, a similar example using the Python LangChain can be found in this Jupyter notebook.The main part of the code looks like this:import * as dotenv from \"dotenv\";', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='/* Clear both the vector store and the memory store */\\nawait vectorStore.delete({ ids });\\nawait memory.clear();\\nLet’s take it piece by piece and see what it does:First, we use Xata as a vector store. In this vector store, we index a few sample documents, but in a real application you can index tens of thousands of documents. These are the documents that our chatbot will use to find answers for user questions. While not shown here, it’s also possible to add custom metadata columns to these documents. You can see the examples on the integration page./* Create the vector store */\\nconst table = \"docs\";\\nconst embeddings = new OpenAIEmbeddings();\\nconst vectorStore = new XataVectorSearch(embeddings, { client, table });\\n\\n/* Add documents to the vector store */\\nconst docs = [\\n  new Document({\\n    pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\\n  }),\\n  new Document({\\n    pageContent:\\n      \"Xata offers a built-in vector type that can be used to store and query vectors\",\\n  }),\\n  new Document({\\n    pageContent: \"Xata includes similarity search\",\\n  }),\\n];', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'})]}, reference_example_id=None, parent_run_id=UUID('7bba3e80-bd7a-478d-9305-7a88fe085de7'), tags=['map:key:context'], child_runs=[], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002722048355Z7bba3e80-bd7a-478d-9305-7a88fe085de7.20240628T002722409232Zd27db8d0-29a7-4ce0-af9f-b6297b2fc301')], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f.20240628T002722048355Z7bba3e80-bd7a-478d-9305-7a88fe085de7')], trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263'), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263.20240628T002651320850Z85ec4aa8-91b1-43f7-ba47-90c8aaec140f')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651038418Z574df557-25df-4dd4-8307-87ea5771f263', trace_id=UUID('574df557-25df-4dd4-8307-87ea5771f263')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the built-in type offered by Xata that can be used to store and query vectors?'}, outputs={'answer': 'Xata is a Serverless Data platform based on PostgreSQL.'}, metadata={'dataset_split': ['base']}, id=UUID('d7d42ad1-db72-4642-aec3-58818b26262e'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 432912, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 432912, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('78af8906-e28b-409d-8039-1eb3d8b0775d'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6cec8d8f-90ee-4bd6-98d6-0a924389f2b9'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=8, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('85c1a39b-42da-4230-94e5-97f59b432bb7'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a0858556-4c0f-4355-9c77-4d952a3de36b'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4001f49f-e4d7-45e2-ba35-277b882fece9'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('618a35aa-f285-476e-a918-7973f504e169'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 132176, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 873408, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.086799+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, 'config': None}, outputs={'response': AIMessage(content='The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9d39f293-07af-4f2c-a571-90e53e765d0a-0', usage_metadata={'input_tokens': 1388, 'output_tokens': 48, 'total_tokens': 1436}), 'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=UUID('f8f6c88b-e427-4c2b-9b56-6b35063fcf23'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('080ba756-6acc-408d-8070-360c127ad8c5'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 708376, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 871749, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 708376, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 871749, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'response': AIMessage(content='The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9d39f293-07af-4f2c-a571-90e53e765d0a-0', usage_metadata={'input_tokens': 1388, 'output_tokens': 48, 'total_tokens': 1436}), 'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), tags=[], child_runs=[Run(id=UUID('92830a40-0d7d-49c4-aacd-e0192c7fee8b'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 155304, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 470025, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 155304, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 470025, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, reference_example_id=None, parent_run_id=UUID('080ba756-6acc-408d-8070-360c127ad8c5'), tags=['seq:step:1'], child_runs=[Run(id=UUID('179741d5-f676-4ab5-8a10-207b9e0c69ef'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 516747, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 372527, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 516747, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 372527, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('92830a40-0d7d-49c4-aacd-e0192c7fee8b'), tags=['map:key:context'], child_runs=[Run(id=UUID('570ebed2-717f-4935-8821-d4ce71b73004'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 746483, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 958208, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 746483, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 958208, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, reference_example_id=None, parent_run_id=UUID('179741d5-f676-4ab5-8a10-207b9e0c69ef'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002652155304Z92830a40-0d7d-49c4-aacd-e0192c7fee8b.20240628T002652516747Z179741d5-f676-4ab5-8a10-207b9e0c69ef.20240628T002652746483Z570ebed2-717f-4935-8821-d4ce71b73004'), Run(id=UUID('2a2d0543-aea4-4e60-85cc-dc61052c3361'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 985768, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 249547, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 985768, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 249547, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'documents': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('179741d5-f676-4ab5-8a10-207b9e0c69ef'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002652155304Z92830a40-0d7d-49c4-aacd-e0192c7fee8b.20240628T002652516747Z179741d5-f676-4ab5-8a10-207b9e0c69ef.20240628T002652985768Z2a2d0543-aea4-4e60-85cc-dc61052c3361')], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002652155304Z92830a40-0d7d-49c4-aacd-e0192c7fee8b.20240628T002652516747Z179741d5-f676-4ab5-8a10-207b9e0c69ef'), Run(id=UUID('b07cd760-b736-4041-8c01-bd904d13364c'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 838282, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 241988, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 838282, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 241988, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, reference_example_id=None, parent_run_id=UUID('92830a40-0d7d-49c4-aacd-e0192c7fee8b'), tags=['map:key:question'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002652155304Z92830a40-0d7d-49c4-aacd-e0192c7fee8b.20240628T002652838282Zb07cd760-b736-4041-8c01-bd904d13364c')], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002652155304Z92830a40-0d7d-49c4-aacd-e0192c7fee8b'), Run(id=UUID('32bf12e2-98b8-4dbc-927c-fe17130665c7'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 554318, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 554318, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('080ba756-6acc-408d-8070-360c127ad8c5'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002720554318Z32bf12e2-98b8-4dbc-927c-fe17130665c7'), Run(id=UUID('787f4b47-c1f9-49e6-9196-55d6ea79a9f8'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 790985, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 870707, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 790985, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 870707, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'response': AIMessage(content='The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9d39f293-07af-4f2c-a571-90e53e765d0a-0', usage_metadata={'input_tokens': 1388, 'output_tokens': 48, 'total_tokens': 1436}), 'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('080ba756-6acc-408d-8070-360c127ad8c5'), tags=['seq:step:3'], child_runs=[Run(id=UUID('05d42d1b-6f91-43fe-80fc-f9914492fffc'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 134017, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 865849, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 134017, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 865849, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': AIMessage(content='The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9d39f293-07af-4f2c-a571-90e53e765d0a-0', usage_metadata={'input_tokens': 1388, 'output_tokens': 48, 'total_tokens': 1436})}, reference_example_id=None, parent_run_id=UUID('787f4b47-c1f9-49e6-9196-55d6ea79a9f8'), tags=['map:key:response'], child_runs=[Run(id=UUID('cdc92dc1-20fa-4fd9-becd-9ceae1bb2f87'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 611396, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 258112, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 611396, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 258112, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Bringing Free OSS Models to the Playground with Fireworks AI\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBringing Free OSS Models to the Playground with Fireworks AI\\\\n\\\\n3 min read\\\\nOct 2, 2023\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'}), Document(page_content=\\'A year ago, the only real LLM people were using was OpenAI\\\\\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\\\\\'ve seen more and more people wanting to try them out. We\\\\\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\\\\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\\\\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\\\\\'s walk through an example of this!First, let\\\\\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\\\\\'s a good start.Let\\\\\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'}), Document(page_content=\\'to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\\\\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'})]\\n\\nQuestion:\\nWhat is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?\\n')])}, reference_example_id=None, parent_run_id=UUID('05d42d1b-6f91-43fe-80fc-f9914492fffc'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002721790985Z787f4b47-c1f9-49e6-9196-55d6ea79a9f8.20240628T002722134017Z05d42d1b-6f91-43fe-80fc-f9914492fffc.20240628T002722611396Zcdc92dc1-20fa-4fd9-becd-9ceae1bb2f87'), Run(id=UUID('9d39f293-07af-4f2c-a571-90e53e765d0a'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 339271, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 864902, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 339271, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 864902, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Bringing Free OSS Models to the Playground with Fireworks AI\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSkip to content\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy LangChain\\\\n\\\\n\\\\n\\\\n\\\\nRelease Notes\\\\n\\\\n\\\\n\\\\n\\\\nCase Studies\\\\n\\\\n\\\\n\\\\n\\\\nLangChain\\\\n\\\\n\\\\n\\\\n\\\\nGitHub\\\\n\\\\n\\\\n\\\\n\\\\nDocs\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign in\\\\nSubscribe\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBringing Free OSS Models to the Playground with Fireworks AI\\\\n\\\\n3 min read\\\\nOct 2, 2023\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'}), Document(page_content=\\'A year ago, the only real LLM people were using was OpenAI\\\\\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\\\\\'ve seen more and more people wanting to try them out. We\\\\\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\\\\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\\\\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\\\\\'s walk through an example of this!First, let\\\\\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\\\\\'s a good start.Let\\\\\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'}), Document(page_content=\\'to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\\\\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key\\', metadata={\\'source\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'loc\\': \\'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/\\', \\'lastmod\\': \\'2023-10-15T22:27:03.000Z\\'})]\\n\\nQuestion:\\nWhat is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI is \"Bringing Free OSS Models to the Playground with Fireworks AI\".', 'response_metadata': {'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-9d39f293-07af-4f2c-a571-90e53e765d0a-0', 'usage_metadata': {'input_tokens': 1388, 'output_tokens': 48, 'total_tokens': 1436}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 48, 'prompt_tokens': 1388, 'total_tokens': 1436}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('05d42d1b-6f91-43fe-80fc-f9914492fffc'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002721790985Z787f4b47-c1f9-49e6-9196-55d6ea79a9f8.20240628T002722134017Z05d42d1b-6f91-43fe-80fc-f9914492fffc.20240628T002723339271Z9d39f293-07af-4f2c-a571-90e53e765d0a')], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002721790985Z787f4b47-c1f9-49e6-9196-55d6ea79a9f8.20240628T002722134017Z05d42d1b-6f91-43fe-80fc-f9914492fffc'), Run(id=UUID('70202716-a3d0-4009-9368-b892a79395b5'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 160045, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 642204, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 160045, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 642204, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})], 'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'output': [Document(page_content='Bringing Free OSS Models to the Playground with Fireworks AI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBringing Free OSS Models to the Playground with Fireworks AI\\n\\n3 min read\\nOct 2, 2023', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='A year ago, the only real LLM people were using was OpenAI\\'s GPT-3. Fast forward to now, and there are a multitude of models to choose from - including a wide variety of open source models. These open source models have seen large performance gains over the past six months in particular. As these models get better, we\\'ve seen more and more people wanting to try them out. We\\'ve teamed up with Fireworks AI to bring these models to the LangSmith playground - completely free of cost (for now, we\\'ll see how expensive this gets).What does mean exactly?Concretely, we have integrated Fireworks AI into the playground, joining the ranks of OpenAI, Anthropic and Vertex AI as supported model providers. Read more about Fireworks AI below, but at a high level they provide API access to a plethora of OSS models. While other model providers in the playground require an API key to use, we\\'ve worked with Fireworks AI to enable anyone to use this integration regardless of whether they have an API key or not (note: you need to be signed into the LangSmith platform in order for this to work).This now means it is easier than ever to try out prompts with an OSS model. Let\\'s walk through an example of this!First, let\\'s go the LangSmith Hub. We can filter existing prompts in the hub to ones that are meant for Llama-2. Note: this is a manual tagging, so it could be incorrect, but it\\'s a good start.Let\\'s choose the hwchase17/llama-rag prompt. Once on this page, we can click on \"Try it\" to open it in the playground.The playground defaults to OpenAI, but we can click on the model provider to change it up.From here, we can select the Fireworks option.We can now select the model we want to use, and then plug in some inputs and hit run!What is Fireworks?Fireworks.ai provides a platform to enable developers to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'}), Document(page_content='to run, fine-tune, and share large language models (LLMs) to best solve product problems. The Fireworks.ai Generative AI platform provides developers access to lightning-fast OSS models, LLM inference, and state-of-the-art foundation models for fine-tuning. The platform provides state-of-the-art machine performance for latency-optimized and throughput-optimized settings and cost reduction (up to 20–120x lower) for affordable serving.Integrating Fireworks.ai models in the LangChain Playground means giving the developer community easy access to the best high-performing open-source and fine-tuned models. The LangChain Prompt Hub already makes it simple to try different prompts, models, and parameters without any coding. The availability of faster inference or faster LLMs helps to further boost productivity in building LLM workflows.A big part of the LLM workflow requires testing and optimizing prompts which is a highly iterative and time-consuming process. This integration makes it possible for LangChain Prompt Hub users to more efficiently test and optimize prompts for state-of-the-art open-source and fine-tuned LLMs like Llama 2 70B. Trying Fireworks in the Playground:Logged-in users can try Fireworks in the playground without an API key, for free!If you’re not logged in or don’t have an account, but want to try Fireworks, you can get one directly from FireworksBelow are the instructions to set up an account with Fireworks.ai:Step 1: Visit app.fireworks.ai.Step 2: Click the \"Sign In\" button in the top navigation bar.Step 3: Click \"Continue with Google\" and authenticate with your Google account. A new Fireworks developer account will be provisioned for you the first time you sign in.Step 4: Next, we\\'ll provision a new API key. Click on \"API Keys\" in the left navigation bar then Click on \"New API Key\" and give your new API key a name.Step 5: Now open-source models like Llama 2 13B Chat are ready to be used in the LangChain Playground.Step 6: You can enter you API key', metadata={'source': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'loc': 'https://blog.langchain.dev/bringing-free-oss-models-to-the-playground-with-fireworks-ai/', 'lastmod': '2023-10-15T22:27:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('787f4b47-c1f9-49e6-9196-55d6ea79a9f8'), tags=['map:key:context'], child_runs=[], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002721790985Z787f4b47-c1f9-49e6-9196-55d6ea79a9f8.20240628T002722160045Z70202716-a3d0-4009-9368-b892a79395b5')], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5.20240628T002721790985Z787f4b47-c1f9-49e6-9196-55d6ea79a9f8')], trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169'), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169.20240628T002651708376Z080ba756-6acc-408d-8070-360c127ad8c5')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651132176Z618a35aa-f285-476e-a918-7973f504e169', trace_id=UUID('618a35aa-f285-476e-a918-7973f504e169')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the title of the article written by LangChain on October 2, 2023, which discusses the introduction of free OSS models to the Playground with Fireworks AI?'}, outputs={'answer': 'Fireworks AI brings free OSS models to the playground.'}, metadata={'dataset_split': ['base']}, id=UUID('f8f6c88b-e427-4c2b-9b56-6b35063fcf23'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 86799, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 86799, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4862545b-9b59-4af9-a649-77fcab34974f'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1a9bd0ab-99aa-4cd0-9c0f-47ae5366403e'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0fc55cb1-6955-4bd8-b7a3-bdc66768c84d'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2ecf58f9-7938-414a-85e0-adf86c4f21ad'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5f305333-625d-459d-9f29-dc326b0584d1'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 64962, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 655217, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.197190+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 73.0, 'mem': {'rss': 1286811648.0}, 'cpu': {'time': {'sys': 300.18, 'user': 141.44}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, 'config': None}, outputs={'response': AIMessage(content='The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e38cb180-aad7-4971-977b-92928a57ecb4-0', usage_metadata={'input_tokens': 1788, 'output_tokens': 29, 'total_tokens': 1817}), 'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=UUID('825a1d0b-4801-456d-9bf0-a580a0a9b16f'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('f53ea10e-9b08-46f3-ba3d-d1188dd3ca47'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 753016, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 635477, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 753016, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 635477, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'response': AIMessage(content='The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e38cb180-aad7-4971-977b-92928a57ecb4-0', usage_metadata={'input_tokens': 1788, 'output_tokens': 29, 'total_tokens': 1817}), 'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=None, parent_run_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), tags=[], child_runs=[Run(id=UUID('58e97b27-fd47-4160-9f69-579b089db02a'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 889824, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 483973, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 889824, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 483973, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, reference_example_id=None, parent_run_id=UUID('f53ea10e-9b08-46f3-ba3d-d1188dd3ca47'), tags=['seq:step:1'], child_runs=[Run(id=UUID('624548fe-7ef2-45e0-a0d0-c351330175ff'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 272245, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 391420, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 272245, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 391420, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=None, parent_run_id=UUID('58e97b27-fd47-4160-9f69-579b089db02a'), tags=['map:key:context'], child_runs=[Run(id=UUID('98fbd72e-6559-4d42-8f37-ccdad779887f'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 407348, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 776257, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 407348, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 776257, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, reference_example_id=None, parent_run_id=UUID('624548fe-7ef2-45e0-a0d0-c351330175ff'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002651889824Z58e97b27-fd47-4160-9f69-579b089db02a.20240628T002652272245Z624548fe-7ef2-45e0-a0d0-c351330175ff.20240628T002652407348Z98fbd72e-6559-4d42-8f37-ccdad779887f'), Run(id=UUID('b97d4b01-0daf-4359-9fc6-491eca677580'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 777193, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 175787, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 777193, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 175787, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'documents': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=None, parent_run_id=UUID('624548fe-7ef2-45e0-a0d0-c351330175ff'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002651889824Z58e97b27-fd47-4160-9f69-579b089db02a.20240628T002652272245Z624548fe-7ef2-45e0-a0d0-c351330175ff.20240628T002652777193Zb97d4b01-0daf-4359-9fc6-491eca677580')], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002651889824Z58e97b27-fd47-4160-9f69-579b089db02a.20240628T002652272245Z624548fe-7ef2-45e0-a0d0-c351330175ff'), Run(id=UUID('8159991c-15ca-435c-8611-08201f235383'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 467396, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 782623, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 467396, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 782623, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, reference_example_id=None, parent_run_id=UUID('58e97b27-fd47-4160-9f69-579b089db02a'), tags=['map:key:question'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002651889824Z58e97b27-fd47-4160-9f69-579b089db02a.20240628T002652467396Z8159991c-15ca-435c-8611-08201f235383')], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002651889824Z58e97b27-fd47-4160-9f69-579b089db02a'), Run(id=UUID('11dac3ef-2996-47b6-9ccd-d1a9c54b6ba2'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 642897, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 642897, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('f53ea10e-9b08-46f3-ba3d-d1188dd3ca47'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002720642897Z11dac3ef-2996-47b6-9ccd-d1a9c54b6ba2'), Run(id=UUID('d12ce9a0-69ae-4d99-be5a-02352f4ef17b'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 974980, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 632601, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 974980, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 632601, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'response': AIMessage(content='The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e38cb180-aad7-4971-977b-92928a57ecb4-0', usage_metadata={'input_tokens': 1788, 'output_tokens': 29, 'total_tokens': 1817}), 'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=None, parent_run_id=UUID('f53ea10e-9b08-46f3-ba3d-d1188dd3ca47'), tags=['seq:step:3'], child_runs=[Run(id=UUID('423a7d5c-d077-4f8f-9184-22fa9932d668'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 202672, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 631086, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 202672, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 631086, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': AIMessage(content='The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e38cb180-aad7-4971-977b-92928a57ecb4-0', usage_metadata={'input_tokens': 1788, 'output_tokens': 29, 'total_tokens': 1817})}, reference_example_id=None, parent_run_id=UUID('d12ce9a0-69ae-4d99-be5a-02352f4ef17b'), tags=['map:key:response'], child_runs=[Run(id=UUID('594ad463-1bca-4289-b242-0bcd14b53d20'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 808471, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 255084, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 808471, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 255084, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company\\'s app development process. Along these lines, we don\\'t believe that prompts should be treated as traditional code–it\\'s simply not the best way to facilitate this kind of collaboration.We\\'re aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn\\'t quite there today–this first iteration only supports personal accounts–but we\\'re actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren\\'t out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-prompt-hub/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-prompt-hub/\\', \\'lastmod\\': \\'2023-09-27T19:34:45.000Z\\'}), Document(page_content=\\'New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain\\', metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/\\', \\'lastmod\\': \\'2023-09-22T16:54:44.000Z\\'}), Document(page_content=\\'others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'loc\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'lastmod\\': \\'2024-06-19T21:53:50.000Z\\'}), Document(page_content=\\'the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run\\', metadata={\\'source\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'loc\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'lastmod\\': \\'2024-06-19T21:53:50.000Z\\'})]\\n\\nQuestion:\\nWhat feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?\\n')])}, reference_example_id=None, parent_run_id=UUID('423a7d5c-d077-4f8f-9184-22fa9932d668'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002721974980Zd12ce9a0-69ae-4d99-be5a-02352f4ef17b.20240628T002722202672Z423a7d5c-d077-4f8f-9184-22fa9932d668.20240628T002722808471Z594ad463-1bca-4289-b242-0bcd14b53d20'), Run(id=UUID('e38cb180-aad7-4971-977b-92928a57ecb4'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 336762, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 630532, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 336762, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 630532, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company\\'s app development process. Along these lines, we don\\'t believe that prompts should be treated as traditional code–it\\'s simply not the best way to facilitate this kind of collaboration.We\\'re aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn\\'t quite there today–this first iteration only supports personal accounts–but we\\'re actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren\\'t out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-prompt-hub/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-prompt-hub/\\', \\'lastmod\\': \\'2023-09-27T19:34:45.000Z\\'}), Document(page_content=\\'New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain\\', metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/\\', \\'lastmod\\': \\'2023-09-22T16:54:44.000Z\\'}), Document(page_content=\\'others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'loc\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'lastmod\\': \\'2024-06-19T21:53:50.000Z\\'}), Document(page_content=\\'the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run\\', metadata={\\'source\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'loc\\': \\'https://blog.langchain.dev/customers-rakuten/\\', \\'lastmod\\': \\'2024-06-19T21:53:50.000Z\\'})]\\n\\nQuestion:\\nWhat feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The feature of LangSmith designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation is the LangChain Hub.', 'response_metadata': {'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-e38cb180-aad7-4971-977b-92928a57ecb4-0', 'usage_metadata': {'input_tokens': 1788, 'output_tokens': 29, 'total_tokens': 1817}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 29, 'prompt_tokens': 1788, 'total_tokens': 1817}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('423a7d5c-d077-4f8f-9184-22fa9932d668'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002721974980Zd12ce9a0-69ae-4d99-be5a-02352f4ef17b.20240628T002722202672Z423a7d5c-d077-4f8f-9184-22fa9932d668.20240628T002723336762Ze38cb180-aad7-4971-977b-92928a57ecb4')], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002721974980Zd12ce9a0-69ae-4d99-be5a-02352f4ef17b.20240628T002722202672Z423a7d5c-d077-4f8f-9184-22fa9932d668'), Run(id=UUID('0c17baf4-4499-4581-84f7-9b07958b90cf'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 237136, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 603774, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 237136, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 603774, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})], 'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'output': [Document(page_content=\"that non-technical team members are participating in the process of editing and refining prompts. We wanted to make it much easier for more team members to get involved in what we believe is going to become a core part of every company's app development process. Along these lines, we don't believe that prompts should be treated as traditional code–it's simply not the best way to facilitate this kind of collaboration.We're aiming to make LangChain Hub the best place for teams to write and manage prompts, together. The product isn't quite there today–this first iteration only supports personal accounts–but we're actively looking for organizations that are excited to explore an Alpha with us so if you want organizational support for the Hub, please reach out to us directly at support@langchain.dev with the subject [Hub: Orgs]4. Artifact Management and LangSmithFrom partnering with early LangSmith users, the tie-in between debugging, logging, testing, and evaluation and artifact management has become increasingly obvious. By making LangChain Hub a part of LangSmith, we knew we could help teams not only identify and collaborate on prompts, but also make informed decisions about how to implement them. Testing integrations with prompts aren't out yet but they are coming soon!Favorite FeaturesHome PageWe want to make discoverability and navigability as easy as possible. You should be able to go from curiosity to coding in just a few clicks. You can view sort prompts by:Most favoritesMost viewedMost downloadedRecently uploadedYou can filter prompts by:Use cases (chatbots, extraction, summarization, etc)Type (prompt template, etc)Language (English, Chinese, etc)Model (OpenAI, Anthropic, Llama2, VertexAI, etc)Downloading and Uploading PromptsWe have released an SDK to enable easy programatic downloading of prompts:from langchain import hub\", metadata={'source': 'https://blog.langchain.dev/langchain-prompt-hub/', 'loc': 'https://blog.langchain.dev/langchain-prompt-hub/', 'lastmod': '2023-09-27T19:34:45.000Z'}), Document(page_content='New in LangSmithOrg Support in LangChain Hub: share and collaborate on prompts across your team. Easily pull in organizationally-approved prompts into your LangChain code.Need access to LangSmith to collaborate on prompts with your team? Fill out this form.Collapsable Traces: makes it way easier to navigate errors (especially for long runs).New Cookbooks:Prompt Tracking on Hub. Prototype and share LLM prompts. An example of how to use it within a RAG chain here.Edit and commit directly from the playground. Jump between prototyping and debugging directly in the browser. Save and use within any custom chain. Example here.Prompt Versioning for consistent, stable deployments: Deploy and test specific prompt versions so you can be consistent in prod without slowing down your experimentation. Cookbook here.Custom run (trace) naming. Navigate and query traces better with customized chain names. Cookbook here.New in Open SourceRouting in LCEL: Allows for flexibility in choosing what to do, but keeps you in control over a finite number of potential paths. Python docs. JS docs.Stream, batch, async model calls: For all OpenAI, Anthropic, Azure OpenAI, AWS Bedrock, and Google VertexAI LLMs and chat models, we now take advantage of native support for streaming, batching and asynchronous calls wherever possible. To see the full feature list by model integration, see LLMs and Chat models.OpenAI InstructGPT 3.5 model: very easy to use in LangChain–just specify the new model gpt-3.5-turbo-instruct in the OpenAI class (we recommend using \"old\" prompts that are NOT in the ChatMessage format)New summarization technique–Chain of Density–works with Anthropic, too: Produces 5 summaries iteratively, each one denser and more informative than the prior. Great overview thread by Victor Mota here. Try it out on Prompt Hub here.Customizable Agents: default agents make it easy to prototype but agents in production require more customization. So, we rewrote all 8 agent types using LangChain', metadata={'source': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-9-18-langchain-release-notes/', 'lastmod': '2023-09-22T16:54:44.000Z'}), Document(page_content='others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run experiments on multiple approaches (models, cognitive architecture, etc.) and measure the results. By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches for using LLMs in an enterprise-setting faster.”LangSmith also provides Rakuten with the enterprise assurances that they need. Data stays within Rakuten’s environment, and with LangSmith, Rakuten can separate access for development workflows from production ones, maintaining the high bar that Rakuten requires when dealing with user data.The Road AheadRakuten started with two ambitious projects, impacting both their e-commerce marketplace and their employee base, but that’s par for the course at Rakuten whose mission is to empower people and society through innovation and entrepreneurship. Rakuten plans to distribute Rakuten AI for Business across its customer base, focusing particularly on merchants, hotels, retail stores, and local economies, with the aim to improve productivity by 20%. The team will continue to design and build technical architectures for large-scale AI applications across its suite of 70 businesses, and LangChain and LangSmith will enable them to achieve their goals faster and more safely than they could have otherwise.', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'}), Document(page_content='the client’s documentation to answer questions from the client’s end customers and prospects in real time.\\xa0Rakuten AI: Empowering RakuteniansRakuten recently leveraged LangChain’s OpenGPTs package to deliver an employee empowerment experience, in which teams could build their own chatbots over internal documentation to help with knowledge management and employee enablement. It only took three engineers one week to get the initial platform up and running.\\xa0The team at Rakuten was excited about LangChain OpenGPTs because it gave them maximal flexibility and control on designing the cognitive architecture and user experience. Additionally, they didn’t want to gate the experience to only a subset of employees and intend to roll the product to 32k employees, and at this scale, the Data Science & ML team must keep a variety of options open and be able to control the cost / performance tradeoffs. LangChain’s framework makes the ability to customize and abstract away vendor lock-in possible.Why LangChain and LangSmithRakuten was an early adopter of LangChain, and they first started building with the framework in January of 2023. LangChain was helpful in providing common, successful interaction patterns for building with LLMs, and many of the off-the-shelf chain and agent architectures allowed Rakuten to iterate quickly.As the team got serious about scaling up their LangChain usage and allowing more users to interact with the applications, they looked to LangSmith to harden their work and provide visibility into what’s happening and why.General Manager of AI for Business at Rakuten, Yusuke Kaji says, “LangSmith allows us to get things done scientifically. At a large company, usually multiple teams develop their ideas independently. Some teams find good approaches, while others don’t. By using LangSmith Hub, we could distribute the best prompts and promote collaboration across teams. By using LangSmith Testing and Eval with our custom evaluation metrics, we can run', metadata={'source': 'https://blog.langchain.dev/customers-rakuten/', 'loc': 'https://blog.langchain.dev/customers-rakuten/', 'lastmod': '2024-06-19T21:53:50.000Z'})]}, reference_example_id=None, parent_run_id=UUID('d12ce9a0-69ae-4d99-be5a-02352f4ef17b'), tags=['map:key:context'], child_runs=[], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002721974980Zd12ce9a0-69ae-4d99-be5a-02352f4ef17b.20240628T002722237136Z0c17baf4-4499-4581-84f7-9b07958b90cf')], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47.20240628T002721974980Zd12ce9a0-69ae-4d99-be5a-02352f4ef17b')], trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5'), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5.20240628T002651753016Zf53ea10e-9b08-46f3-ba3d-d1188dd3ca47')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651064962Ze34f021f-b5b8-423a-82ad-9c2313ef4aa5', trace_id=UUID('e34f021f-b5b8-423a-82ad-9c2313ef4aa5')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What feature of LangSmith is designed to help teams identify and collaborate on prompts as well as make informed decisions about their implementation?'}, outputs={'answer': 'from langchain import hub\\n```\\n\\nQuestion:\\nWhat features does the Home Page offer to enhance user experience?\\n\\nAnswer:\\nThe Home Page offers features to enhance user experience by making discoverability and navigability easy, allowing users to go from curiosity to coding in just a few clicks. Users can view and sort prompts by most favorites, most viewed, most downloaded, and recently uploaded. They can also filter prompts by use cases, type, language, and model.'}, metadata={'dataset_split': ['base']}, id=UUID('825a1d0b-4801-456d-9bf0-a580a0a9b16f'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 197190, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 197190, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4a496616-1363-4489-9161-abef8f04445f'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('483c3bc4-72fc-4eaf-9d7d-c819df1ae735'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=2, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f2e9866e-a67c-490e-9c77-72ab8467b660'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9d8b20cf-9eef-4f93-85f2-0ab4f93ff1ee'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('facb49f8-4a50-449a-9358-8f2095865555'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 324768, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 53667, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.632844+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 73.0, 'mem': {'rss': 1286811648.0}, 'cpu': {'time': {'sys': 300.18, 'user': 141.44}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, 'config': None}, outputs={'response': AIMessage(content='The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-028d69d2-7abf-4a4c-8ec6-49b15931d312-0', usage_metadata={'input_tokens': 1949, 'output_tokens': 61, 'total_tokens': 2010}), 'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=UUID('22fba3eb-3089-451a-858e-961fbfe85a95'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('0b9878e7-06f0-4e38-97a1-d0951784c61d'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 546918, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 52315, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 546918, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 52315, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'response': AIMessage(content='The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-028d69d2-7abf-4a4c-8ec6-49b15931d312-0', usage_metadata={'input_tokens': 1949, 'output_tokens': 61, 'total_tokens': 2010}), 'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), tags=[], child_runs=[Run(id=UUID('c0aa196a-a306-4ac6-9ba9-930741f73543'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 766677, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 38776, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 766677, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 38776, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, reference_example_id=None, parent_run_id=UUID('0b9878e7-06f0-4e38-97a1-d0951784c61d'), tags=['seq:step:1'], child_runs=[Run(id=UUID('4d010f93-e976-4141-92c3-1e12d8f61dba'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 74061, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 16, 998454, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 74061, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 16, 998454, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('c0aa196a-a306-4ac6-9ba9-930741f73543'), tags=['map:key:context'], child_runs=[Run(id=UUID('5ee93d83-76a1-4851-ab57-00be59f78542'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 222246, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 315515, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 222246, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 315515, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, reference_example_id=None, parent_run_id=UUID('4d010f93-e976-4141-92c3-1e12d8f61dba'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002651766677Zc0aa196a-a306-4ac6-9ba9-930741f73543.20240628T002652074061Z4d010f93-e976-4141-92c3-1e12d8f61dba.20240628T002652222246Z5ee93d83-76a1-4851-ab57-00be59f78542'), Run(id=UUID('94b5caf4-3705-4f86-b42f-1bdb74ea4360'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 316659, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 16, 996807, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 316659, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 16, 996807, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'documents': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4d010f93-e976-4141-92c3-1e12d8f61dba'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002651766677Zc0aa196a-a306-4ac6-9ba9-930741f73543.20240628T002652074061Z4d010f93-e976-4141-92c3-1e12d8f61dba.20240628T002652316659Z94b5caf4-3705-4f86-b42f-1bdb74ea4360')], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002651766677Zc0aa196a-a306-4ac6-9ba9-930741f73543.20240628T002652074061Z4d010f93-e976-4141-92c3-1e12d8f61dba'), Run(id=UUID('212fb024-b686-49fd-8e1a-410796cc3358'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 231927, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 361185, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 231927, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 361185, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, reference_example_id=None, parent_run_id=UUID('c0aa196a-a306-4ac6-9ba9-930741f73543'), tags=['map:key:question'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002651766677Zc0aa196a-a306-4ac6-9ba9-930741f73543.20240628T002652231927Z212fb024-b686-49fd-8e1a-410796cc3358')], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002651766677Zc0aa196a-a306-4ac6-9ba9-930741f73543'), Run(id=UUID('88213d00-bfca-4a45-a055-f265f49fb945'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 17, 44571, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 17, 44571, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('0b9878e7-06f0-4e38-97a1-d0951784c61d'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002717044571Z88213d00-bfca-4a45-a055-f265f49fb945'), Run(id=UUID('be608b29-8288-4744-92c3-36ba622b2219'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 991796, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 51300, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 991796, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 51300, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'response': AIMessage(content='The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-028d69d2-7abf-4a4c-8ec6-49b15931d312-0', usage_metadata={'input_tokens': 1949, 'output_tokens': 61, 'total_tokens': 2010}), 'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('0b9878e7-06f0-4e38-97a1-d0951784c61d'), tags=['seq:step:3'], child_runs=[Run(id=UUID('68e1781a-9b6f-47d3-ac8c-7be3d6c432b2'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 334709, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 42807, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 334709, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 42807, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': AIMessage(content='The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-028d69d2-7abf-4a4c-8ec6-49b15931d312-0', usage_metadata={'input_tokens': 1949, 'output_tokens': 61, 'total_tokens': 2010})}, reference_example_id=None, parent_run_id=UUID('be608b29-8288-4744-92c3-36ba622b2219'), tags=['map:key:response'], child_runs=[Run(id=UUID('5a1a1570-255f-4e4b-9a24-f716cbc34ad2'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 708603, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 981102, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 708603, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 981102, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\\\\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\\\\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\\\\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\\\\\'s worth looking something up or not. Another way to do\\', metadata={\\'source\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'loc\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'lastmod\\': \\'2023-10-15T19:19:25.000Z\\'}), Document(page_content=\\'when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\\\\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\\\\\'ve seen in practice as we\\\\\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for\\', metadata={\\'source\\': \\'https://blog.langchain.dev/langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/langgraph/\\', \\'lastmod\\': \\'2024-02-09T21:45:03.000Z\\'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We\\'ve also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={\\'source\\': \\'https://blog.langchain.dev/the-prompt-landscape/\\', \\'loc\\': \\'https://blog.langchain.dev/the-prompt-landscape/\\', \\'lastmod\\': \\'2024-03-14T20:34:52.000Z\\'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={\\'source\\': \\'https://blog.langchain.dev/langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/langgraph/\\', \\'lastmod\\': \\'2024-02-09T21:45:03.000Z\\'})]\\n\\nQuestion:\\nWhat are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?\\n')])}, reference_example_id=None, parent_run_id=UUID('68e1781a-9b6f-47d3-ac8c-7be3d6c432b2'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002720991796Zbe608b29-8288-4744-92c3-36ba622b2219.20240628T002721334709Z68e1781a-9b6f-47d3-ac8c-7be3d6c432b2.20240628T002721708603Z5a1a1570-255f-4e4b-9a24-f716cbc34ad2'), Run(id=UUID('028d69d2-7abf-4a4c-8ec6-49b15931d312'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 141588, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 41879, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 141588, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 41879, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\\\\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\\\\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\\\\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\\\\\'s worth looking something up or not. Another way to do\\', metadata={\\'source\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'loc\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'lastmod\\': \\'2023-10-15T19:19:25.000Z\\'}), Document(page_content=\\'when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\\\\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\\\\\'ve seen in practice as we\\\\\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for\\', metadata={\\'source\\': \\'https://blog.langchain.dev/langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/langgraph/\\', \\'lastmod\\': \\'2024-02-09T21:45:03.000Z\\'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We\\'ve also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={\\'source\\': \\'https://blog.langchain.dev/the-prompt-landscape/\\', \\'loc\\': \\'https://blog.langchain.dev/the-prompt-landscape/\\', \\'lastmod\\': \\'2024-03-14T20:34:52.000Z\\'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we\\'ve implemented already. We will then highlight a few of the common modifications to these runtimes we\\'ve heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We\\'ve invested heavily in the functionality for this with LangChain Expression Language. However, so far we\\'ve lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={\\'source\\': \\'https://blog.langchain.dev/langgraph/\\', \\'loc\\': \\'https://blog.langchain.dev/langgraph/\\', \\'lastmod\\': \\'2024-02-09T21:45:03.000Z\\'})]\\n\\nQuestion:\\nWhat are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context are:\\n1. Do we ALWAYS look something up?\\n2. Do we look up the raw user search query or a derived one?\\n3. What do we do for follow up questions?', 'response_metadata': {'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-028d69d2-7abf-4a4c-8ec6-49b15931d312-0', 'usage_metadata': {'input_tokens': 1949, 'output_tokens': 61, 'total_tokens': 2010}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 61, 'prompt_tokens': 1949, 'total_tokens': 2010}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('68e1781a-9b6f-47d3-ac8c-7be3d6c432b2'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002720991796Zbe608b29-8288-4744-92c3-36ba622b2219.20240628T002721334709Z68e1781a-9b6f-47d3-ac8c-7be3d6c432b2.20240628T002722141588Z028d69d2-7abf-4a4c-8ec6-49b15931d312')], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002720991796Zbe608b29-8288-4744-92c3-36ba622b2219.20240628T002721334709Z68e1781a-9b6f-47d3-ac8c-7be3d6c432b2'), Run(id=UUID('e796f43f-c6e2-42a0-8e8a-2058c5b4e318'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 338786, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 561781, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 338786, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 561781, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})], 'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'output': [Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content='when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed to an LLM to generate a final answer. While this is often effective, it breaks down in cases when the first retrieval step fails to return any useful results. In this case, it\\'s often ideal if the LLM can reason that the results returned from the retriever are poor, and maybe issue a second (more refined) query to the retriever, and use those results instead. Essentially, running an LLM in a loop helps create applications that are more flexible and thus can accomplish more vague use-cases that may not be predefined.These types of applications are often called agents. The simplest - but at the same time most ambitious - form of these is a loop that essentially has two steps:Call the LLM to determine either (a) what actions to take, or (b) what response to give the userTake given actions, and pass back to step 1These steps are repeated until a final response is generated. This is essentially the loop that powers our core AgentExecutor, and is the same logic that caused projects like AutoGPT to rise in prominence. This is simple because it is a relatively simple loop. It is the most ambitious because it offloads pretty much ALL of the decision making and reasoning ability to the LLM.One thing we\\'ve seen in practice as we\\'ve worked the community and companies to put agents into production is that often times more control is needed. You may want to always force an agent to call particular tool first. You may want have more control over how tools are called. You may want to have different prompts for the agent, depending on that state it is in.When talking about these more controlled flows, we internally refer to them as \"state machines\". See the below diagram from our blog on cognitive architectures.These state machines have the power of being able to loop - allowing for', metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'}), Document(page_content=\"LangChain has numerous tools for querying SQL databases (see our use case guide and cookbook). Exampleshttps://smith.langchain.com/hub/rlm/text-to-sqlBrainstormingMany people have had instructive and / or entertaining conversations with LLMs. LLMs have proven broadly useful for brainstorming: one trick is to create multiple user personas that work through an idea collectively, as shown by @mattshumer_ business plan ideation prompt. The principle can be adapted broadly. As an example, BIDARA (Bio-inspired Design and Research Assistant) is a GPT-4 chatbot  instructed to help scientists and engineers understand, learn from, and emulate the strategies used by living things for new designs and technologies.Exampleshttps://smith.langchain.com/hub/hwchase17/matt-shumer-validate-business-ideahttps://smith.langchain.com/hub/bruffridge/bidaraExtractionLLMs can be powerful tools for extracting text in particular formats, often aided by function calling. This is a rich area with frameworks developed to support it, such as @jxnlco’s Instructor (see their guide on prompt engineering). We've also seen prompts designed for specific extraction tasks, such as knowledge graph triple extraction (as shown by tools like Instagraph or the text-to-graph playground).Knowledge graph triple visualizations from @yoheinakajimaExampleshttps://smith.langchain.com/hub/langchain/knowledge-triple-extractorhttps://smith.langchain.com/hub/homanp/superagentRAG Retrieval augmented generation (RAG) is a popular LLM application: it passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.RAG overviewExampleshttps://smith.langchain.com/hub/rlm/rag-promptInstruction-tuned LLMsThe landscape of open source instruction-tuned LLMs has exploded over the past year. With this has come a variety of popular LLMs that each have specific\", metadata={'source': 'https://blog.langchain.dev/the-prompt-landscape/', 'loc': 'https://blog.langchain.dev/the-prompt-landscape/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content=\"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes.Python RepoPython YouTube PlaylistJS RepoIntroductionOne of the things we highlighted in our LangChain v0.1 announcement was the introduction of a new library: LangGraph. LangGraph is built on top of LangChain and completely interoperable with the LangChain ecosystem. It adds new value primarily through the introduction of an easy way to create cyclical graphs. This is often useful when creating agent runtimes.In this blog post, we will first walk through the motivations for LangGraph. We will then cover the basic functionality it provides. We will then spotlight two agent runtimes we've implemented already. We will then highlight a few of the common modifications to these runtimes we've heard requests for, and examples of implementing those. We will finish with a preview of what we will be releasing next.MotivationOne of the big value props of LangChain is the ability to easily create custom chains. We've invested heavily in the functionality for this with LangChain Expression Language. However, so far we've lacked a method for easily introducing cycles into these chains. Effectively, these chains are directed acyclic graphs (DAGs) - as are most data orchestration frameworks.One of the common patterns we see when people are creating more complex LLM applications is the introduction of cycles into the runtime. These cycles often use the LLM to reason about what to do next in the cycle. A big unlock of LLMs is the ability to use them for these reasoning tasks. This can essentially be thought of as running an LLM in a for-loop. These types of systems are often called agents.An example of why this agentic behavior can be so powerful can be found when considering a typical retrieval augmented generation (RAG) application. In a typical RAG application, a call to a retriever is made that returns some documents. These documents are then passed\", metadata={'source': 'https://blog.langchain.dev/langgraph/', 'loc': 'https://blog.langchain.dev/langgraph/', 'lastmod': '2024-02-09T21:45:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('be608b29-8288-4744-92c3-36ba622b2219'), tags=['map:key:context'], child_runs=[], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002720991796Zbe608b29-8288-4744-92c3-36ba622b2219.20240628T002721338786Ze796f43f-c6e2-42a0-8e8a-2058c5b4e318')], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d.20240628T002720991796Zbe608b29-8288-4744-92c3-36ba622b2219')], trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205'), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205.20240628T002651546918Z0b9878e7-06f0-4e38-97a1-d0951784c61d')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651324768Z6031057d-eda1-4e6b-a5df-053269d90205', trace_id=UUID('6031057d-eda1-4e6b-a5df-053269d90205')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What are the three major questions to consider when thinking about Retrieval Augmented Generation (RAG) as discussed in the context?'}, outputs={'answer': 'Retrieval augmented generation (RAG) is a method that involves retrieving information from various data sources and integrating it into the context window of a large language model (LLM) to enhance output generation. This approach is advantageous over complex fine-tuning for tasks requiring factual recall. RAG systems typically involve a user question that determines the information to retrieve, a retrieval process from data sources, and passing the retrieved information directly to the LLM as part of the prompt.'}, metadata={'dataset_split': ['base']}, id=UUID('22fba3eb-3089-451a-858e-961fbfe85a95'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 632844, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 632844, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('68cb4bfd-bc97-4660-9ea5-62bee4c33874'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9f74f5a1-faaa-4409-b368-271a838c013f'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=2, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7990e59a-41c8-4198-86bc-4135c0d28d13'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d1d3aab4-2a6a-481d-ada4-2c731984289d'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d58f81e1-3a3c-4c18-a67f-d039aa9b863f'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 812168, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 803436, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:19.268217+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 51.0, 'mem': {'rss': 1279164416.0}, 'cpu': {'time': {'sys': 300.3, 'user': 142.08}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, 'config': None}, outputs={'response': AIMessage(content='The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bca0e121-9dd1-4d20-91fa-312d106511e2-0', usage_metadata={'input_tokens': 737, 'output_tokens': 35, 'total_tokens': 772}), 'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=UUID('4354fff9-f111-49c6-bc40-e05acb66f545'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('9ca5547a-f2b4-4070-ac8a-e130a240c952'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 875581, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 800908, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 875581, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 800908, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'response': AIMessage(content='The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bca0e121-9dd1-4d20-91fa-312d106511e2-0', usage_metadata={'input_tokens': 737, 'output_tokens': 35, 'total_tokens': 772}), 'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), tags=[], child_runs=[Run(id=UUID('f5bd9849-b9cc-403e-9c53-ebd7879e5edd'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 925756, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 591456, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 925756, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 591456, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, reference_example_id=None, parent_run_id=UUID('9ca5547a-f2b4-4070-ac8a-e130a240c952'), tags=['seq:step:1'], child_runs=[Run(id=UUID('e490bf35-503a-4bbc-bdbc-cbb8cd3c63b8'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 995550, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 473685, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 995550, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 473685, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('f5bd9849-b9cc-403e-9c53-ebd7879e5edd'), tags=['map:key:context'], child_runs=[Run(id=UUID('067532e4-0e16-4669-b2e2-4f0f38e274e9'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 9, 31056, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 9, 58247, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 9, 31056, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 9, 58247, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, reference_example_id=None, parent_run_id=UUID('e490bf35-503a-4bbc-bdbc-cbb8cd3c63b8'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002708925756Zf5bd9849-b9cc-403e-9c53-ebd7879e5edd.20240628T002708995550Ze490bf35-503a-4bbc-bdbc-cbb8cd3c63b8.20240628T002709031056Z067532e4-0e16-4669-b2e2-4f0f38e274e9'), Run(id=UUID('e6e8aaea-c92b-47ec-a5ce-377c31e1b668'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 27, 9, 61779, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 386825, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 9, 61779, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 386825, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'documents': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('e490bf35-503a-4bbc-bdbc-cbb8cd3c63b8'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002708925756Zf5bd9849-b9cc-403e-9c53-ebd7879e5edd.20240628T002708995550Ze490bf35-503a-4bbc-bdbc-cbb8cd3c63b8.20240628T002709061779Ze6e8aaea-c92b-47ec-a5ce-377c31e1b668')], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002708925756Zf5bd9849-b9cc-403e-9c53-ebd7879e5edd.20240628T002708995550Ze490bf35-503a-4bbc-bdbc-cbb8cd3c63b8'), Run(id=UUID('f77ab35e-7ec4-48ee-b710-a23a95f03a5a'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 9, 19823, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 9, 69323, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 9, 19823, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 9, 69323, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, reference_example_id=None, parent_run_id=UUID('f5bd9849-b9cc-403e-9c53-ebd7879e5edd'), tags=['map:key:question'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002708925756Zf5bd9849-b9cc-403e-9c53-ebd7879e5edd.20240628T002709019823Zf77ab35e-7ec4-48ee-b710-a23a95f03a5a')], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002708925756Zf5bd9849-b9cc-403e-9c53-ebd7879e5edd'), Run(id=UUID('9ad827d2-55c0-4db5-94b3-32295281c378'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 820541, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 820541, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('9ca5547a-f2b4-4070-ac8a-e130a240c952'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002720820541Z9ad827d2-55c0-4db5-94b3-32295281c378'), Run(id=UUID('22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 441348, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 800119, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 441348, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 800119, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'response': AIMessage(content='The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bca0e121-9dd1-4d20-91fa-312d106511e2-0', usage_metadata={'input_tokens': 737, 'output_tokens': 35, 'total_tokens': 772}), 'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9ca5547a-f2b4-4070-ac8a-e130a240c952'), tags=['seq:step:3'], child_runs=[Run(id=UUID('3b32052f-299e-41bc-81be-720650d56a5b'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 761721, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 795881, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 761721, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 795881, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': AIMessage(content='The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bca0e121-9dd1-4d20-91fa-312d106511e2-0', usage_metadata={'input_tokens': 737, 'output_tokens': 35, 'total_tokens': 772})}, reference_example_id=None, parent_run_id=UUID('22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b'), tags=['map:key:response'], child_runs=[Run(id=UUID('f9b0becb-51d9-4e44-af70-798e7e2e5218'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 185297, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 368024, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 185297, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 368024, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'evaluate_comparative(\\\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\\\n    evaluators=[evaluate_pairwise],\\\\n)\\', metadata={\\'source\\': \\'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/\\', \\'loc\\': \\'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/\\', \\'lastmod\\': \\'2024-06-05T18:40:30.000Z\\'}), Document(page_content=\\'{question_summaries}\\', metadata={\\'source\\': \\'https://blog.langchain.dev/llms-to-improve-documentation/\\', \\'loc\\': \\'https://blog.langchain.dev/llms-to-improve-documentation/\\', \\'lastmod\\': \\'2023-08-18T21:55:31.000Z\\'}), Document(page_content=\\'logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to\\', metadata={\\'source\\': \\'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/\\', \\'loc\\': \\'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/\\', \\'lastmod\\': \\'2023-11-23T01:08:56.000Z\\'})]\\n\\nQuestion:\\nWhat function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?\\n')])}, reference_example_id=None, parent_run_id=UUID('3b32052f-299e-41bc-81be-720650d56a5b'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002722441348Z22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b.20240628T002722761721Z3b32052f-299e-41bc-81be-720650d56a5b.20240628T002723185297Zf9b0becb-51d9-4e44-af70-798e7e2e5218'), Run(id=UUID('bca0e121-9dd1-4d20-91fa-312d106511e2'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 442503, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 24, 795357, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 442503, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 24, 795357, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'evaluate_comparative(\\\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\\\n    evaluators=[evaluate_pairwise],\\\\n)\\', metadata={\\'source\\': \\'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/\\', \\'loc\\': \\'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/\\', \\'lastmod\\': \\'2024-06-05T18:40:30.000Z\\'}), Document(page_content=\\'{question_summaries}\\', metadata={\\'source\\': \\'https://blog.langchain.dev/llms-to-improve-documentation/\\', \\'loc\\': \\'https://blog.langchain.dev/llms-to-improve-documentation/\\', \\'lastmod\\': \\'2023-08-18T21:55:31.000Z\\'}), Document(page_content=\\'logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to\\', metadata={\\'source\\': \\'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/\\', \\'loc\\': \\'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/\\', \\'lastmod\\': \\'2023-11-23T01:08:56.000Z\\'})]\\n\\nQuestion:\\nWhat function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The function being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\" is evaluate_comparative.', 'response_metadata': {'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-bca0e121-9dd1-4d20-91fa-312d106511e2-0', 'usage_metadata': {'input_tokens': 737, 'output_tokens': 35, 'total_tokens': 772}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 35, 'prompt_tokens': 737, 'total_tokens': 772}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('3b32052f-299e-41bc-81be-720650d56a5b'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002722441348Z22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b.20240628T002722761721Z3b32052f-299e-41bc-81be-720650d56a5b.20240628T002723442503Zbca0e121-9dd1-4d20-91fa-312d106511e2')], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002722441348Z22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b.20240628T002722761721Z3b32052f-299e-41bc-81be-720650d56a5b'), Run(id=UUID('d66d709f-3096-4c84-b99e-354eb874a0b8'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 766052, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 137125, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 766052, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 137125, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})], 'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'output': [Document(page_content='evaluate_comparative(\\n    [\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"],\\n    evaluators=[evaluate_pairwise],\\n)', metadata={'source': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'loc': 'https://blog.langchain.dev/pairwise-evaluations-with-langsmith/', 'lastmod': '2024-06-05T18:40:30.000Z'}), Document(page_content='{question_summaries}', metadata={'source': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'loc': 'https://blog.langchain.dev/llms-to-improve-documentation/', 'lastmod': '2023-08-18T21:55:31.000Z'}), Document(page_content='logical collections and default retrieval schemes produce incomplete summariesDiscrete value lookup performance by the LLM on vectorized data is poorDefault data cleaning does not handle certain things like Excel numeric date encodingThe basic problem with summarization is that it is a reduction from many things to one statement.  The default configuration for a single document retrieval Q&A application is to find 4 similar parts of the document and “stuff” them into the context window before asking for a summary.To improve retrieval and summarization performance on spreadsheets, we need to consider other retrieval strategies.  Damien Benveniste recently posted the following graphic on LinkedIn that addresses handling summaries of multiple documents that are too large to fit into the context window:Adding options for chain type, search type, and k-documents settings to my Gradio app, I am now able to test each of these strategies in different configurations against different LLMs:Figure 8 - Adding Options for Chain Type, Search Type, and K-documentsSetting chain type to “map reduce” (the second strategy in Figure 7) and increasing the number of retrieved documents produces a much better result:Figure 9 - Map-reduced Summarization with 10 Table ElementsThe LLM hits on all major themes in the various extracted sub-tables, acknowledges the instrument type in the document (debt), and even mentions the amortization schedule.  All sans nonsensical data elements.Turning on chain verbosity, we can get an idea of what is happening behind the scenes (apologies for the small text):Figure 10 – The Final Prompt in the Map Reduce ChainEach extracted sub-table is being summarized by the LLM before it is injected into a final prompt for a collective summary at the end, just like the map-reduce diagram depicts.  Expanding the K-document size ensures that smaller nuances of the file are considered.You may be wondering about the “refine” strategy and perhaps wonder what happened to', metadata={'source': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'loc': 'https://blog.langchain.dev/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/', 'lastmod': '2023-11-23T01:08:56.000Z'})]}, reference_example_id=None, parent_run_id=UUID('22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b'), tags=['map:key:context'], child_runs=[], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002722441348Z22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b.20240628T002722766052Zd66d709f-3096-4c84-b99e-354eb874a0b8')], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952.20240628T002722441348Z22a226fe-a2ad-4ad7-88fc-b6dcac0bd23b')], trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595'), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595.20240628T002708875581Z9ca5547a-f2b4-4070-ac8a-e130a240c952')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002708812168Zc67c5660-7b87-4e10-bfde-b7d1893c1595', trace_id=UUID('c67c5660-7b87-4e10-bfde-b7d1893c1595')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What function is being called to evaluate the summaries \"summary-cmd-r-f692a55c\" and \"summary-opus-21590361\"?'}, outputs={'answer': 'evaluate_comparative([\"summary-cmd-r-f692a55c\", \"summary-opus-21590361\"], evaluators=[evaluate_pairwise])'}, metadata={'dataset_split': ['base']}, id=UUID('4354fff9-f111-49c6-bc40-e05acb66f545'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 268217, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 268217, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4bae48e4-0549-4414-ba2a-b858a4155728'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('74cfd6da-4e32-4e85-bc10-d9a46f4890a2'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=8, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bfd85b32-5619-406c-a5cd-c3b59dddeb98'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f6b5dbf1-fe3b-404b-8dcb-0ee373cb9d6e'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('295c8747-0273-44e3-b568-04c11573fd2a'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 50, 929975, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 810094, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:22.009035+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 93.0, 'mem': {'rss': 2243321856.0}, 'cpu': {'time': {'sys': 186.08, 'user': 126.82}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, 'config': None}, outputs={'response': AIMessage(content='Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c2acb30-8d69-4e25-92e5-e301ce783ca0-0', usage_metadata={'input_tokens': 1485, 'output_tokens': 74, 'total_tokens': 1559}), 'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=UUID('65154ac6-48ca-4aa9-8da2-6b3483afcd8d'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('f038026f-4d60-4a20-afea-4bd86b63d16d'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 48307, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 807433, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 48307, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 807433, tzinfo=datetime.timezone.utc)}], inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'response': AIMessage(content='Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c2acb30-8d69-4e25-92e5-e301ce783ca0-0', usage_metadata={'input_tokens': 1485, 'output_tokens': 74, 'total_tokens': 1559}), 'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), tags=[], child_runs=[Run(id=UUID('8d24bb16-872e-40e3-bb04-8acb5afc7a3e'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 207216, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 6, 809016, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 207216, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 6, 809016, tzinfo=datetime.timezone.utc)}], inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, reference_example_id=None, parent_run_id=UUID('f038026f-4d60-4a20-afea-4bd86b63d16d'), tags=['seq:step:1'], child_runs=[Run(id=UUID('5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 755614, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 6, 775660, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 755614, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 6, 775660, tzinfo=datetime.timezone.utc)}], inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('8d24bb16-872e-40e3-bb04-8acb5afc7a3e'), tags=['map:key:context'], child_runs=[Run(id=UUID('5b77614a-cee1-4f77-beae-cc4672e82db6'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 860424, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 226844, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 860424, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 226844, tzinfo=datetime.timezone.utc)}], inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, reference_example_id=None, parent_run_id=UUID('5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002651207216Z8d24bb16-872e-40e3-bb04-8acb5afc7a3e.20240628T002651755614Z5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4.20240628T002651860424Z5b77614a-cee1-4f77-beae-cc4672e82db6'), Run(id=UUID('1fdcf99c-4858-4de0-91c6-329210012c11'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 227817, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 6, 773964, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 227817, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 6, 773964, tzinfo=datetime.timezone.utc)}], inputs={'query': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'documents': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002651207216Z8d24bb16-872e-40e3-bb04-8acb5afc7a3e.20240628T002651755614Z5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4.20240628T002652227817Z1fdcf99c-4858-4de0-91c6-329210012c11')], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002651207216Z8d24bb16-872e-40e3-bb04-8acb5afc7a3e.20240628T002651755614Z5e70d4bf-dc8e-42e7-aa9b-fc120cfc7fe4'), Run(id=UUID('3c6c69b5-9fb9-44b7-be6d-3a135d84a8da'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 761860, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 924461, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 761860, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 924461, tzinfo=datetime.timezone.utc)}], inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, reference_example_id=None, parent_run_id=UUID('8d24bb16-872e-40e3-bb04-8acb5afc7a3e'), tags=['map:key:question'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002651207216Z8d24bb16-872e-40e3-bb04-8acb5afc7a3e.20240628T002651761860Z3c6c69b5-9fb9-44b7-be6d-3a135d84a8da')], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002651207216Z8d24bb16-872e-40e3-bb04-8acb5afc7a3e'), Run(id=UUID('008d506d-01d0-42a4-9c5d-8b10f2b00938'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 6, 947219, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 6, 947219, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('f038026f-4d60-4a20-afea-4bd86b63d16d'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002706947219Z008d506d-01d0-42a4-9c5d-8b10f2b00938'), Run(id=UUID('2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 114677, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 805366, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 114677, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 805366, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'response': AIMessage(content='Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c2acb30-8d69-4e25-92e5-e301ce783ca0-0', usage_metadata={'input_tokens': 1485, 'output_tokens': 74, 'total_tokens': 1559}), 'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('f038026f-4d60-4a20-afea-4bd86b63d16d'), tags=['seq:step:3'], child_runs=[Run(id=UUID('5649d1e3-5ced-4f50-b4ed-3073dbb48b62'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 192909, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 800442, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 192909, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 800442, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': AIMessage(content='Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c2acb30-8d69-4e25-92e5-e301ce783ca0-0', usage_metadata={'input_tokens': 1485, 'output_tokens': 74, 'total_tokens': 1559})}, reference_example_id=None, parent_run_id=UUID('2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86'), tags=['map:key:response'], child_runs=[Run(id=UUID('9c3bc737-2873-468f-afb5-c1e4daba620b'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 202561, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 206661, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 202561, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 206661, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton\\', metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'}), Document(page_content=\\'Editor\\\\\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will\\', metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they\\'d probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'})]\\n\\nQuestion:\\nHow can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?\\n')])}, reference_example_id=None, parent_run_id=UUID('5649d1e3-5ced-4f50-b4ed-3073dbb48b62'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002707114677Z2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86.20240628T002707192909Z5649d1e3-5ced-4f50-b4ed-3073dbb48b62.20240628T002707202561Z9c3bc737-2873-468f-afb5-c1e4daba620b'), Run(id=UUID('4c2acb30-8d69-4e25-92e5-e301ce783ca0'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 224635, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 8, 776410, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 224635, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 8, 776410, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton\\', metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'}), Document(page_content=\\'Editor\\\\\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will\\', metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they\\'d probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={\\'source\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'loc\\': \\'https://blog.langchain.dev/feature-stores-and-llms/\\', \\'lastmod\\': \\'2023-06-24T21:39:58.000Z\\'})]\\n\\nQuestion:\\nHow can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'Feature stores can be utilized to enhance prompt construction in language model applications by allowing for the inclusion of real-time, complex, and user-based information. This can personalize the prompts and make them more relevant to the specific user input. An example of an open source feature store mentioned in the context is Feast, which is one of the more popular feature stores used for this purpose.', 'response_metadata': {'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-4c2acb30-8d69-4e25-92e5-e301ce783ca0-0', 'usage_metadata': {'input_tokens': 1485, 'output_tokens': 74, 'total_tokens': 1559}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 74, 'prompt_tokens': 1485, 'total_tokens': 1559}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('5649d1e3-5ced-4f50-b4ed-3073dbb48b62'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002707114677Z2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86.20240628T002707192909Z5649d1e3-5ced-4f50-b4ed-3073dbb48b62.20240628T002707224635Z4c2acb30-8d69-4e25-92e5-e301ce783ca0')], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002707114677Z2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86.20240628T002707192909Z5649d1e3-5ced-4f50-b4ed-3073dbb48b62'), Run(id=UUID('6a02d327-82aa-4550-99c8-c29b946b9286'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 196583, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 7, 246814, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 196583, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 7, 246814, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})], 'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'output': [Document(page_content='There are two variables this prompt expects: `question` and `context`. Question is usually provided at run-time by the user, and context is data that is fetched (usually via a retrieval system) that is relevant to the user input.This already allows for a certain type of “personalization” - depending on the retriever that is used, that context can be used to “personalize” an application so that it can respond about a specific type of data.Feature stores <> PromptingThose are the four main prompt construction strategies that we see today. We think feature stores can supercharge prompt construction - by allowing for inclusion of real-time, complex, and often user-based, information.This is combining key ideas from existing prompt construction methods. When we insert things like the current date and time, we are injecting knowledge of the current state of the world into the prompt - but it is very simple and general knowledge. When we insert external data fetched based on user input, we are incorporating more personalized data - but it is largely based solely on user input.To see this in play, we can head over to the feast GitHub repository - one of the more popular open source feature stores. Let’s look at the example on the README. In this example we are fetching realtime, user specific information. In traditional ML models this information would be fed in as feature values into a model, but now we can feed it into the prompt!To see how this can be done, we’ve set up an example notebook showing how to connect a feature store to a prompt template. Putting it inside a prompt template allows us to nicely package up that logic and pass it through the chain. When we then call that chain, real-time information can be fetched via feast and used to construct a prompt which is then passed into the language model.See the example notebook for this here. We do this for a variety of feature stores: feast, Tecton, FeatureForm. Here’s an example of the code for doing it with Tecton', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content='Editor\\'s note: big thank you to Willem Pienaar (Feast), Mike Del Balso (Tecton), and Simba Khadder (FeatureForm) for their comments and help on this post.LLMs represent a new paradigm of AI. There is a big open question of how many of the tools and services that were useful for traditional machine learning are still relevant here. On one hand, there are very real new use cases and requirements for this new paradigm. On the other hand, existing tools and services have years of experience, development, and feature hardening. Are these tools still useful in this new paradigm?Feature stores overviewOne particularly interesting case study is feature stores. In traditional machine learning, the input to models is not raw text or an image, but rather a series of engineered “features” related to the datapoint at hand. A feature store is a A feature store is a system meant to centralize and serve ML features to models. There are usually two benefits:A way of keeping track of what features are present at a particular point in time to use in model trainingA real-time pipeline of features to use when doing inferenceHow might these apply to LLM applications?The first point seems not as relevant. Most folks are using pre-trained LLMs from OpenAI, Anthropic, etc and not training their own models from scratch.On the other hand, we believe that the second benefit (a real-time pipeline of features to use when doing inference) is still extremely relevant. We believe that a real-time pipeline of features (enabled by a feature store) can be used to achieve real-time personalization of LLM applications. Although we do not see a lot of this at the moment, we believe that this will become more popular as applications become more advanced. An LLM is a \"reasoning engine,\" just like a person. There are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will', metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'}), Document(page_content=\"are a whole set of tasks that would be much easier to reason for if you also had ready access to fresh data about your user or business. E.g. if you are asking a person to estimate how long will take to drive from a to b, they'd probably make a way better decision if they also knew which road segments there were between a and b and how much traffic is on those segments right now.This kind of added context or data enrichment has been widely used in traditional ML for a long time. Some models have 1000s of features. Although LLMs make it possible to solve some ML tasks with only user prompts, a large set of tasks can only be completed at competitive accuracy with a lot of context data.Prompting overviewLanguage model applications are largely defined by how their prompts are constructed. Therefore, before trying to think about how feature stores could impact language model applications, we should try to understand different ways of constructing prompts, and then think about how feature stores could impact prompt construction. Below are probably the three most common types of prompt construction, ordered from least complex to most complex.#1: Hard coded prompt stringThis is when the entire prompt that is passed into the language model is hard coded by the application. There is no customization done to the prompt, and therefore all the variation in output comes from the language model. This is an incredibly simple prompting strategy that honestly is likely so simple it’s rarely used.#2: Incorporating user inputThe next step is incorporating user input into the prompt string. This means having a prompt template, and filling in the variables with user input that they put in at run time. An example of an application like this could be a poem-generating application. This application could ask the user for a `subject` input, and then format a prompt like `Write me a poem about {subject}` before passing that to the language model. This can be done with multiple variables, but\", metadata={'source': 'https://blog.langchain.dev/feature-stores-and-llms/', 'loc': 'https://blog.langchain.dev/feature-stores-and-llms/', 'lastmod': '2023-06-24T21:39:58.000Z'})]}, reference_example_id=None, parent_run_id=UUID('2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86'), tags=['map:key:context'], child_runs=[], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002707114677Z2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86.20240628T002707196583Z6a02d327-82aa-4550-99c8-c29b946b9286')], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d.20240628T002707114677Z2c6dfa23-e641-40f4-b8fe-7e7fd0ad4a86')], trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442'), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442.20240628T002651048307Zf038026f-4d60-4a20-afea-4bd86b63d16d')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002650929975Z6374c1b8-d11e-44f8-babd-5561031bf442', trace_id=UUID('6374c1b8-d11e-44f8-babd-5561031bf442')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'How can feature stores be utilized to enhance prompt construction in language model applications, and what is an example of an open source feature store mentioned in the context?'}, outputs={'answer': 'Feature stores can supercharge prompt construction by allowing the inclusion of real-time, complex, and often user-based information. This approach combines key ideas from existing prompt construction methods by injecting knowledge of the current state of the world and incorporating more personalized data based on user input. Examples of feature stores that can be used include feast, Tecton, and FeatureForm. This method can enhance language model applications by providing a superior, personalized end-user experience through real-time context, personalized marketing content, and session-based recommendations.'}, metadata={'dataset_split': ['base']}, id=UUID('65154ac6-48ca-4aa9-8da2-6b3483afcd8d'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 22, 9035, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 22, 9035, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('16f737ac-d5de-4b07-87fb-1d717f5eb08f'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('84d132dc-4790-4b2c-8d67-6cf80c452aeb'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('29334cd4-61f0-4cc4-ac0c-6877fc24ef26'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c545cdb2-9c74-4254-bf9d-a169e6017b0e'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d1b4ee02-32d9-4a5c-abc0-ee1f15c08f9e'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 9768, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 123127, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.666032+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 87.0, 'mem': {'rss': 1290809344.0}, 'cpu': {'time': {'sys': 300.04, 'user': 140.98}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, 'config': None}, outputs={'response': AIMessage(content='After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab9ef32-3049-490b-95cf-948d05205b19-0', usage_metadata={'input_tokens': 1583, 'output_tokens': 26, 'total_tokens': 1609}), 'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=UUID('8415c903-d1d6-4c81-9de6-24bd86d3ce00'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('3e4a86c3-49de-4905-b25d-5274db0e1547'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 250353, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 113992, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 250353, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 19, 113992, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'response': AIMessage(content='After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab9ef32-3049-490b-95cf-948d05205b19-0', usage_metadata={'input_tokens': 1583, 'output_tokens': 26, 'total_tokens': 1609}), 'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=None, parent_run_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), tags=[], child_runs=[Run(id=UUID('09694c18-c41c-4c4c-a1b0-f4def8707032'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 460444, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 962507, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 460444, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 962507, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, reference_example_id=None, parent_run_id=UUID('3e4a86c3-49de-4905-b25d-5274db0e1547'), tags=['seq:step:1'], child_runs=[Run(id=UUID('06e96adc-6de2-41c7-a007-9a2635b5dec6'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 893296, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 911832, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 893296, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 911832, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=None, parent_run_id=UUID('09694c18-c41c-4c4c-a1b0-f4def8707032'), tags=['map:key:context'], child_runs=[Run(id=UUID('c765ae65-47bf-4cd1-a38c-7e40602f38d0'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 229966, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 324101, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 229966, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 324101, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, reference_example_id=None, parent_run_id=UUID('06e96adc-6de2-41c7-a007-9a2635b5dec6'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002651460444Z09694c18-c41c-4c4c-a1b0-f4def8707032.20240628T002651893296Z06e96adc-6de2-41c7-a007-9a2635b5dec6.20240628T002652229966Zc765ae65-47bf-4cd1-a38c-7e40602f38d0'), Run(id=UUID('666cdbc2-8b81-4b1b-9a64-3bb4cf95679c'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 326149, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 905632, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 326149, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 905632, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'documents': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=None, parent_run_id=UUID('06e96adc-6de2-41c7-a007-9a2635b5dec6'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002651460444Z09694c18-c41c-4c4c-a1b0-f4def8707032.20240628T002651893296Z06e96adc-6de2-41c7-a007-9a2635b5dec6.20240628T002652326149Z666cdbc2-8b81-4b1b-9a64-3bb4cf95679c')], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002651460444Z09694c18-c41c-4c4c-a1b0-f4def8707032.20240628T002651893296Z06e96adc-6de2-41c7-a007-9a2635b5dec6'), Run(id=UUID('cbe90bd1-b17b-4ba9-8167-c1ceaebbd9ed'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 114206, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 266182, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 114206, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 266182, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, reference_example_id=None, parent_run_id=UUID('09694c18-c41c-4c4c-a1b0-f4def8707032'), tags=['map:key:question'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002651460444Z09694c18-c41c-4c4c-a1b0-f4def8707032.20240628T002652114206Zcbe90bd1-b17b-4ba9-8167-c1ceaebbd9ed')], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002651460444Z09694c18-c41c-4c4c-a1b0-f4def8707032'), Run(id=UUID('1e431539-e0d7-4b80-9fc1-d12ab4d764fa'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 992710, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 992710, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('3e4a86c3-49de-4905-b25d-5274db0e1547'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002713992710Z1e431539-e0d7-4b80-9fc1-d12ab4d764fa'), Run(id=UUID('4e728b2f-8918-4583-be80-b6615d10bd58'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 422066, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 94852, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 422066, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 19, 94852, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'response': AIMessage(content='After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab9ef32-3049-490b-95cf-948d05205b19-0', usage_metadata={'input_tokens': 1583, 'output_tokens': 26, 'total_tokens': 1609}), 'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=None, parent_run_id=UUID('3e4a86c3-49de-4905-b25d-5274db0e1547'), tags=['seq:step:3'], child_runs=[Run(id=UUID('68ed5f5a-7293-4e4a-9e47-5df592f2abc8'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 653471, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 660254, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 653471, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 660254, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4e728b2f-8918-4583-be80-b6615d10bd58'), tags=['map:key:context'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002714422066Z4e728b2f-8918-4583-be80-b6615d10bd58.20240628T002714653471Z68ed5f5a-7293-4e4a-9e47-5df592f2abc8'), Run(id=UUID('dae65ea7-ac7d-42e9-bb16-c4b327b06d9e'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 665391, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 899988, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 665391, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 899988, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': AIMessage(content='After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab9ef32-3049-490b-95cf-948d05205b19-0', usage_metadata={'input_tokens': 1583, 'output_tokens': 26, 'total_tokens': 1609})}, reference_example_id=None, parent_run_id=UUID('4e728b2f-8918-4583-be80-b6615d10bd58'), tags=['map:key:response'], child_runs=[Run(id=UUID('7fbc4b6d-5de4-4e30-a4b6-6c8a87cbd50c'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 692788, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 744903, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 692788, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 744903, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='const ids = await vectorStore.addDocuments(docs);\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\nconst memory = new BufferMemory({\\n  chatHistory: new XataChatMessageHistory({\\n    table: \"memory\",\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\n    client,\\n    createTable: false,\\n  }),\\n  memoryKey: \"chat_history\",\\n});\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\nconst model = new ChatOpenAI({});\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\n  model,\\n  vectorStore.asRetriever(),\\n  {\\n    memory,\\n  }\\n);\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'loc': 'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/', 'lastmod': '2023-08-29T14:47:07.000Z'}), Document(page_content='evaluation_config = RunEvalConfig(\\n    custom_evaluators=[eval_chains.values()],\\n    prediction_key=\"result\",\\n)\\n    \\nresult = run_on_dataset(\\n    client,\\n    dataset_name,\\n    create_qa_chain,\\n    evaluation=evaluation_config,\\n    input_mapper=lambda x: x,\\n)\\n\\n# output\\n# View the evaluation results for project \\'2023-08-24-03-36-45-RetrievalQA\\' at:\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.', metadata={'source': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'loc': 'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/', 'lastmod': '2023-08-24T13:02:03.000Z'}), Document(page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/langchain-templates/', 'loc': 'https://blog.langchain.dev/langchain-templates/', 'lastmod': '2023-11-10T17:08:10.000Z'}), Document(page_content='We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\xa0code is available as a Langchain template\\xa0and as a\\xa0Jupyter notebook.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'loc': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'lastmod': '2024-02-20T17:24:29.000Z'})], 'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'const ids = await vectorStore.addDocuments(docs);\\\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\\\nconst memory = new BufferMemory({\\\\n  chatHistory: new XataChatMessageHistory({\\\\n    table: \"memory\",\\\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\\\n    client,\\\\n    createTable: false,\\\\n  }),\\\\n  memoryKey: \"chat_history\",\\\\n});\\\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\\\nconst model = new ChatOpenAI({});\\\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\\\n  model,\\\\n  vectorStore.asRetriever(),\\\\n  {\\\\n    memory,\\\\n  }\\\\n);\\\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'evaluation_config = RunEvalConfig(\\\\n    custom_evaluators=[eval_chains.values()],\\\\n    prediction_key=\"result\",\\\\n)\\\\n    \\\\nresult = run_on_dataset(\\\\n    client,\\\\n    dataset_name,\\\\n    create_qa_chain,\\\\n    evaluation=evaluation_config,\\\\n    input_mapper=lambda x: x,\\\\n)\\\\n\\\\n# output\\\\n# View the evaluation results for project \\\\\\'2023-08-24-03-36-45-RetrievalQA\\\\\\' at:\\\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\\', \\'loc\\': \\'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\\', \\'lastmod\\': \\'2023-08-24T13:02:03.000Z\\'}), Document(page_content=\\'Join our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024\\', metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-templates/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-templates/\\', \\'lastmod\\': \\'2023-11-10T17:08:10.000Z\\'}), Document(page_content=\\'We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\\\xa0code is available as a Langchain template\\\\xa0and as a\\\\xa0Jupyter notebook.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024\\', metadata={\\'source\\': \\'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/\\', \\'loc\\': \\'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/\\', \\'lastmod\\': \\'2024-02-20T17:24:29.000Z\\'})]\\n\\nQuestion:\\nWhat should you do after receiving the confirmation email for the LangChain newsletter subscription?\\n')])}, reference_example_id=None, parent_run_id=UUID('dae65ea7-ac7d-42e9-bb16-c4b327b06d9e'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002714422066Z4e728b2f-8918-4583-be80-b6615d10bd58.20240628T002714665391Zdae65ea7-ac7d-42e9-bb16-c4b327b06d9e.20240628T002714692788Z7fbc4b6d-5de4-4e30-a4b6-6c8a87cbd50c'), Run(id=UUID('5ab9ef32-3049-490b-95cf-948d05205b19'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 747766, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 876388, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 747766, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 876388, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'const ids = await vectorStore.addDocuments(docs);\\\\nNext, we create a chat memory store, again based on Xata. This stores the messages exchanged by the chat bots with the users in a Xata table. Each conversation gets a session ID, which is then used to retrieve the previous messages in the conversation, so that the context is not lost./* Create the chat memory store */\\\\nconst memory = new BufferMemory({\\\\n  chatHistory: new XataChatMessageHistory({\\\\n    table: \"memory\",\\\\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\\\\n    client,\\\\n    createTable: false,\\\\n  }),\\\\n  memoryKey: \"chat_history\",\\\\n});\\\\nThen we initialize the client for interrogating the model, in this case the OpenAI ChatGPT API:/* Initialize the LLM to use to answer the question */\\\\nconst model = new ChatOpenAI({});\\\\nAnd finally, put all of them together in a conversational QA chain:/* Create the chain */\\\\nconst chain = ConversationalRetrievalQAChain.fromLLM(\\\\n  model,\\\\n  vectorStore.asRetriever(),\\\\n  {\\\\n    memory,\\\\n  }\\\\n);\\\\nIf you look at the data via the Xata UI while the example is running, you will see two tables: docs and memory. The docs table is populated with the documents from the vector store, having a content column and an embedding column of type vector:The memory table is populated with the questions and answers from the user and from the AI:Content hackathonAs part of the launch week, Xata is also organizing a content hackathon, where you can win prizes and swag by creating apps, writing blogs, recording videos, and more. See the launch week blog post for details.If you have any questions or ideas or if you need help implementing Xata with LangChain, join us on Discord or reach out on Twitter.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'loc\\': \\'https://blog.langchain.dev/xata-x-langchain-new-vector-store-and-memory-store-integrations/\\', \\'lastmod\\': \\'2023-08-29T14:47:07.000Z\\'}), Document(page_content=\\'evaluation_config = RunEvalConfig(\\\\n    custom_evaluators=[eval_chains.values()],\\\\n    prediction_key=\"result\",\\\\n)\\\\n    \\\\nresult = run_on_dataset(\\\\n    client,\\\\n    dataset_name,\\\\n    create_qa_chain,\\\\n    evaluation=evaluation_config,\\\\n    input_mapper=lambda x: x,\\\\n)\\\\n\\\\n# output\\\\n# View the evaluation results for project \\\\\\'2023-08-24-03-36-45-RetrievalQA\\\\\\' at:\\\\n# https://smith.langchain.com/projects/p/9fb78371-150e-49cc-a927-b1247fdb9e8d?eval=trueOpen up the results and you will see something like thisThis shows the output for each example in the training dataset and the Feedback columns show the evaluation results. Now if you want to dive more into the reasons for the scores and how to improve them, click on any single example and open the Feedback tab.This will show you each score and if you open the pop-up icon it will point you to the evaluation run of the RagasEvaluatorChain.You can analyze each result to see why it was so and this will give you ideas on how to improve it.Now if your QA pipeline also uses LangSmith for logging, tracing, and monitoring you can leverage the add-to-dataset feature to set up a continuous evaluation pipeline that keeps adding interesting data points (based on human feedback of other indirect methods) into the test to keep the test dataset up to date with a more comprehensive dataset with wider coverage.ConclusionRagas enhances QA system evaluation by addressing limitations in traditional metrics and leveraging Large Language Models. LangSmith provides a platform for running evaluations and visualizing results.By using Ragas and LangSmith, you can ensure your QA systems are robust and ready for real-world applications, making the development process more efficient and reliable.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\\', \\'loc\\': \\'https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\\', \\'lastmod\\': \\'2023-08-24T13:02:03.000Z\\'}), Document(page_content=\\'Join our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024\\', metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-templates/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-templates/\\', \\'lastmod\\': \\'2023-11-10T17:08:10.000Z\\'}), Document(page_content=\\'We can simply copy this tool description in the system prompt and Mixtral will be able to use the defined tools, which is quite cool.ConclusionMost of the work to implement the JSON-based agent was done by Harrison Chase and the LangChain team, for which I am grateful. All I had to do was find the puzzle pieces and put them together. As mentioned, don’t expect the same level of agent performance as with GPT-4. However, I think the more powerful OSS LLMs like Mixtral could be used as agents today (with a bit more exception handling than GPT-4). I am looking forward to more open source LLMs being fine-tuned as agents.The\\\\xa0code is available as a Langchain template\\\\xa0and as a\\\\xa0Jupyter notebook.\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024\\', metadata={\\'source\\': \\'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/\\', \\'loc\\': \\'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/\\', \\'lastmod\\': \\'2024-02-20T17:24:29.000Z\\'})]\\n\\nQuestion:\\nWhat should you do after receiving the confirmation email for the LangChain newsletter subscription?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'After receiving the confirmation email for the LangChain newsletter subscription, you should check your inbox and click the link to confirm your subscription.', 'response_metadata': {'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-5ab9ef32-3049-490b-95cf-948d05205b19-0', 'usage_metadata': {'input_tokens': 1583, 'output_tokens': 26, 'total_tokens': 1609}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 26, 'prompt_tokens': 1583, 'total_tokens': 1609}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('dae65ea7-ac7d-42e9-bb16-c4b327b06d9e'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002714422066Z4e728b2f-8918-4583-be80-b6615d10bd58.20240628T002714665391Zdae65ea7-ac7d-42e9-bb16-c4b327b06d9e.20240628T002714747766Z5ab9ef32-3049-490b-95cf-948d05205b19')], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002714422066Z4e728b2f-8918-4583-be80-b6615d10bd58.20240628T002714665391Zdae65ea7-ac7d-42e9-bb16-c4b327b06d9e')], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547.20240628T002714422066Z4e728b2f-8918-4583-be80-b6615d10bd58')], trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa'), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa.20240628T002651250353Z3e4a86c3-49de-4905-b25d-5274db0e1547')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651009768Z27c478f0-613d-4954-826b-2bea2a9cf8aa', trace_id=UUID('27c478f0-613d-4954-826b-2bea2a9cf8aa')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What should you do after receiving the confirmation email for the LangChain newsletter subscription?'}, outputs={'answer': 'How can a user confirm their subscription to the LangChain newsletter?\\n\\nAnswer:\\nA user can confirm their subscription to the LangChain newsletter by checking their inbox and clicking the link provided in the confirmation email.'}, metadata={'dataset_split': ['base']}, id=UUID('8415c903-d1d6-4c81-9de6-24bd86d3ce00'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 666032, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 666032, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('062f5f3e-5611-43c3-972f-c6e642381edf'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('37c8ea6f-5177-479d-9a54-c139225556fe'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1fc267c6-bf38-46f5-8b21-cad7635cd9fc'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5889a930-f99e-4171-aced-2a42895805bc'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('964b9d51-0d91-4c4c-81a2-177ec0332f62'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 299717, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 432729, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.742449+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, 'config': None}, outputs={'response': AIMessage(content='The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a1185f7-0d55-491b-9f92-e2fd35d85178-0', usage_metadata={'input_tokens': 1496, 'output_tokens': 90, 'total_tokens': 1586}), 'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=UUID('9d7c053f-8ec7-449c-9c64-a0b2f3f9499f'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('7947b00d-79e0-4547-aac0-d2c511b5afa2'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 716450, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 430391, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 716450, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 430391, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'response': AIMessage(content='The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a1185f7-0d55-491b-9f92-e2fd35d85178-0', usage_metadata={'input_tokens': 1496, 'output_tokens': 90, 'total_tokens': 1586}), 'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), tags=[], child_runs=[Run(id=UUID('590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 40216, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 825466, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 40216, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 825466, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, reference_example_id=None, parent_run_id=UUID('7947b00d-79e0-4547-aac0-d2c511b5afa2'), tags=['seq:step:1'], child_runs=[Run(id=UUID('3fcc3266-899d-4a94-bea8-794bc99bfcb8'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 370936, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 744382, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 370936, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 744382, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc'), tags=['map:key:context'], child_runs=[Run(id=UUID('7f72cdb8-b925-49be-a6d4-d6e73cb4ca11'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 555476, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 792937, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 555476, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 792937, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, reference_example_id=None, parent_run_id=UUID('3fcc3266-899d-4a94-bea8-794bc99bfcb8'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002652040216Z590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc.20240628T002652370936Z3fcc3266-899d-4a94-bea8-794bc99bfcb8.20240628T002652555476Z7f72cdb8-b925-49be-a6d4-d6e73cb4ca11'), Run(id=UUID('67e80e0a-ff23-4d34-b97e-e0895f681a82'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 793654, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 731022, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 793654, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 731022, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'documents': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('3fcc3266-899d-4a94-bea8-794bc99bfcb8'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002652040216Z590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc.20240628T002652370936Z3fcc3266-899d-4a94-bea8-794bc99bfcb8.20240628T002652793654Z67e80e0a-ff23-4d34-b97e-e0895f681a82')], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002652040216Z590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc.20240628T002652370936Z3fcc3266-899d-4a94-bea8-794bc99bfcb8'), Run(id=UUID('33320681-cc93-4c3d-8378-328a986453b3'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 711288, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 103375, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 711288, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 103375, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, reference_example_id=None, parent_run_id=UUID('590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc'), tags=['map:key:question'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002652040216Z590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc.20240628T002652711288Z33320681-cc93-4c3d-8378-328a986453b3')], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002652040216Z590ba86d-4eea-4cf4-ada3-5c3b8a05b9fc'), Run(id=UUID('8f9ef603-f056-4c46-a437-94b491e61e3b'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 862912, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 862912, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('7947b00d-79e0-4547-aac0-d2c511b5afa2'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002718862912Z8f9ef603-f056-4c46-a437-94b491e61e3b'), Run(id=UUID('11557323-a7c4-4a3d-9b8d-c63c668addc7'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 177693, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 428263, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 177693, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 428263, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'response': AIMessage(content='The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a1185f7-0d55-491b-9f92-e2fd35d85178-0', usage_metadata={'input_tokens': 1496, 'output_tokens': 90, 'total_tokens': 1586}), 'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('7947b00d-79e0-4547-aac0-d2c511b5afa2'), tags=['seq:step:3'], child_runs=[Run(id=UUID('8bd66229-ccd4-427e-867c-84a8819f55f9'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 631524, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 979096, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 631524, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 979096, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})]}, reference_example_id=None, parent_run_id=UUID('11557323-a7c4-4a3d-9b8d-c63c668addc7'), tags=['map:key:context'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002721177693Z11557323-a7c4-4a3d-9b8d-c63c668addc7.20240628T002721631524Z8bd66229-ccd4-427e-867c-84a8819f55f9'), Run(id=UUID('0f70448a-f021-466f-b913-c348e005227d'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 21, 635282, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 420074, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 21, 635282, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 420074, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': AIMessage(content='The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a1185f7-0d55-491b-9f92-e2fd35d85178-0', usage_metadata={'input_tokens': 1496, 'output_tokens': 90, 'total_tokens': 1586})}, reference_example_id=None, parent_run_id=UUID('11557323-a7c4-4a3d-9b8d-c63c668addc7'), tags=['map:key:response'], child_runs=[Run(id=UUID('191cf658-77c3-4876-bd91-c6e59e16206a'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 57163, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 646013, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 57163, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 646013, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or', metadata={'source': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'loc': 'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/', 'lastmod': '2023-11-16T15:32:54.000Z'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we've now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={'source': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'loc': 'https://blog.langchain.dev/langsmith-production-logging-automations/', 'lastmod': '2024-04-02T15:41:23.000Z'}), Document(page_content='ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\xa0literacy rates are in decline.\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the', metadata={'source': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'loc': 'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/', 'lastmod': '2023-10-23T15:59:03.000Z'})], 'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/\\', \\'lastmod\\': \\'2023-11-16T15:32:54.000Z\\'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we\\'ve now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={\\'source\\': \\'https://blog.langchain.dev/langsmith-production-logging-automations/\\', \\'loc\\': \\'https://blog.langchain.dev/langsmith-production-logging-automations/\\', \\'lastmod\\': \\'2024-04-02T15:41:23.000Z\\'}), Document(page_content=\\'ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\\\xa0literacy rates are in decline.\\\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the\\', metadata={\\'source\\': \\'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/\\', \\'loc\\': \\'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/\\', \\'lastmod\\': \\'2023-10-23T15:59:03.000Z\\'})]\\n\\nQuestion:\\nWhat is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?\\n')])}, reference_example_id=None, parent_run_id=UUID('0f70448a-f021-466f-b913-c348e005227d'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002721177693Z11557323-a7c4-4a3d-9b8d-c63c668addc7.20240628T002721635282Z0f70448a-f021-466f-b913-c348e005227d.20240628T002722057163Z191cf658-77c3-4876-bd91-c6e59e16206a'), Run(id=UUID('5a1185f7-0d55-491b-9f92-e2fd35d85178'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 847838, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 410630, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 847838, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 410630, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'TLDR: We’re excited to announce a new LangChain template for helping with research, heavily inspired by and in collaboration with the GPT Researcher team. We are excited about this because it is one of the best performing, long-running, general, non-chat “cognitive architectures” that we’ve seen.Key Links:Access the “research assistant” template hereWatch us code the \"research assistant\" from scratch hereThe Downside of ChatMost LLM applications so far have been chat based. This isn’t terribly surprising - ChatGPT is the fastest growing consumer application, and chat is a reasonable UX (allows for back and forth, is a natural medium). However, chat as a UX also has some downsides, and can often make it harder to create highly performant applications.Latency ExpectationsThe main downside are the latency expectations that come with chat. When you are using a chat application, there is a certain expectation you have as a user around the latency of that application. Specifically, you expect a quick response. This is why streaming is an incredibly important functionality in LLM applications - it gives the illusion of a fast response and shows progress.These latency expectations mean that it is tough to do too much work behind the scenes - the more work you do, the longer it will take to generate a response. This is a bit problematic - the major issue with LLM application is their accuracy/performance, and one main way to increase is the spend more time breaking down the process into individual steps, running more checks, etc. However, all these additional steps greatly increase latency.We often talk to teams who try to balance these two considerations, which can at points be contradictory. While working in a chat UX, for complex reasoning tasks it’s tough to get a highly accurate and helpful response in the time that is expectedOutput FormatChat is great for messages. It’s less great for things that don’t really belong in messages - like long papers or code files, or\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-uxs-besides-chat-with-research-assistant/\\', \\'lastmod\\': \\'2023-11-16T15:32:54.000Z\\'}), Document(page_content=\"advanced features here is that you can group runs by metadata attributes. What this means is that you add a metadata tag to runs representing a particular configuration. A concrete example of this is with ChatLangChain, where we rotate between five different LLM providers. We insert as metadata a key tracking which LLM we chose. In LangSmith, you can then group the monitoring dashboards by this metadata key. This allows us to easily compare all those same stats (latency, feedback, etc) across the five different model providers.Another incredibly useful feature of these charts is that they are all interactive. What this means is that you can click into any particular point in the chart and that will bring you to the Runs page automatically filtered to only show datapoints in the timebin you just clicked on.ThreadsThe dominant UX for LLM applications is still chat. In chat applications, there is a back and forth between human messages and an AI response. Each AI response is a trace (and can consist of many sub-runs). With Threads, we\\'ve now introduced a way to view the whole back-and-forth of a conversation in a single view. This can be done by attaching a special metadata key to each trace with the unique identifier for that conversation. This makes it much easier to debug conversations as you can see the whole thread in one place.AutomationsThe previous features all make it easy to manually inspect datapoints. With Automations, you can now act upon datapoints of interest in an automated fashion. Automations consist of three points: a filter, a sampling rate, and an action. The filter determines which subset of datapoints you want to act on. We talked about filters above, and we can reuse the same UI components to create an automation. After constructing the desired filter, you can then click on the Add Rule button to create an automation.The sampling rate is the next thing to set. This is a rate between 0 and 1 which represents the fraction of the datapoints that\", metadata={\\'source\\': \\'https://blog.langchain.dev/langsmith-production-logging-automations/\\', \\'loc\\': \\'https://blog.langchain.dev/langsmith-production-logging-automations/\\', \\'lastmod\\': \\'2024-04-02T15:41:23.000Z\\'}), Document(page_content=\\'ChatGPT, Bard, etc., all use some form of basic chat window.But LLMs are not yet replacements for the personal touch of true human interaction. They tend to overexplain, creating a completely different experience from traditional service agents. So, if GenAI is the next great paradigm shift in digital interactions, it is crucial to solve for the UX/UI. Consumers reject complexity, even in the face of evolution. By nature, LLMs require reading and writing. Until we can advance the integration between LLM output and traditional UX interaction, the majority of your users will fall into the “stuck” zones.Furthermore, utilizing prompting as our primary interaction method relies on the customer to get it right. This path reduces development effort at the sacrifice of adoption and accessibility. In the early stages of advancement this was widely acceptable. ChatGPT was used by everyone but has since seen a steady decline. It’s not as if the model is somehow producing extra errors. The newness has worn off, and users feel more like, “OK, but what’s next?”. The outlook becomes even more troubling when you consider\\\\xa0literacy rates are in decline.\\\\xa0How can such a complex product that requires reading and writing at higher levels than standard web practice become the next great revolution?This post outlines a few UX strategies to help empower your users and bridge the gap between standard UX and LLM interfaces. None of these will break your roadmap in terms of effort, but they can go a long way in creating a more accessible, usable product. We will cover:Helping your users get up to speed fasterAccounting for AnthropomorphismUser Feedback and testingPreventing the Blank Canvas ProblemThe art world has a well-known concept of a blank canvas. Picture an artist with an empty easel, endlessly wondering where or how to start. The canvas will feel different for different artists. It can represent fear of the unknown or endless, infinite, possibilities. How an artist feels about the\\', metadata={\\'source\\': \\'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/\\', \\'loc\\': \\'https://blog.langchain.dev/beyond-text-making-genai-applications-accessible-to-all/\\', \\'lastmod\\': \\'2023-10-23T15:59:03.000Z\\'})]\\n\\nQuestion:\\nWhat is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context, is the latency expectations that come with chat. Users expect quick responses in a chat application, which can make it difficult to perform extensive work behind the scenes to increase accuracy and performance. The more work done behind the scenes, the longer it takes to generate a response, which can be problematic for complex reasoning tasks.', 'response_metadata': {'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-5a1185f7-0d55-491b-9f92-e2fd35d85178-0', 'usage_metadata': {'input_tokens': 1496, 'output_tokens': 90, 'total_tokens': 1586}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 90, 'prompt_tokens': 1496, 'total_tokens': 1586}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('0f70448a-f021-466f-b913-c348e005227d'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002721177693Z11557323-a7c4-4a3d-9b8d-c63c668addc7.20240628T002721635282Z0f70448a-f021-466f-b913-c348e005227d.20240628T002722847838Z5a1185f7-0d55-491b-9f92-e2fd35d85178')], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002721177693Z11557323-a7c4-4a3d-9b8d-c63c668addc7.20240628T002721635282Z0f70448a-f021-466f-b913-c348e005227d')], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2.20240628T002721177693Z11557323-a7c4-4a3d-9b8d-c63c668addc7')], trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3'), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3.20240628T002651716450Z7947b00d-79e0-4547-aac0-d2c511b5afa2')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651299717Z46aa3fb1-dc4a-41ca-a390-38ca612fe4d3', trace_id=UUID('46aa3fb1-dc4a-41ca-a390-38ca612fe4d3')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the primary downside of using chat as a user experience (UX) for complex reasoning tasks in LLM applications, as described in the context?'}, outputs={'answer': 'The main downside of chat-based LLM applications mentioned in the context is the latency expectations that come with chat.'}, metadata={'dataset_split': ['base']}, id=UUID('9d7c053f-8ec7-449c-9c64-a0b2f3f9499f'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 742449, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 742449, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('581ddf2f-9537-48e3-9c58-d7f2f9323965'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9f72a1df-d3c7-4539-aa54-2f72cde12a52'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('928a1f61-e2fb-43e2-9582-959f5954d331'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('cea9b189-a276-4049-bb7c-b7194bf069e0'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3a474a25-eaa8-4e31-9d73-f7caf1c9af08'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 13929, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 768933, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.543088+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 87.0, 'mem': {'rss': 1290809344.0}, 'cpu': {'time': {'sys': 300.04, 'user': 140.98}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, 'config': None}, outputs={'response': AIMessage(content='LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ef17578-441d-4ece-a8ed-fb34021a020c-0', usage_metadata={'input_tokens': 1617, 'output_tokens': 18, 'total_tokens': 1635}), 'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=UUID('99b7533d-891c-43ad-a206-27782e10e35f'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 166376, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 716311, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 166376, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 716311, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'response': AIMessage(content='LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ef17578-441d-4ece-a8ed-fb34021a020c-0', usage_metadata={'input_tokens': 1617, 'output_tokens': 18, 'total_tokens': 1635}), 'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=None, parent_run_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), tags=[], child_runs=[Run(id=UUID('29cf99d7-6812-4e29-9202-3af8a2e323d1'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 643167, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 454288, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 643167, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 454288, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, reference_example_id=None, parent_run_id=UUID('11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd'), tags=['seq:step:1'], child_runs=[Run(id=UUID('7d8c9e46-86f2-44f1-82ea-e8184bff8d4a'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 971311, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 449420, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 971311, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 449420, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=None, parent_run_id=UUID('29cf99d7-6812-4e29-9202-3af8a2e323d1'), tags=['map:key:context'], child_runs=[Run(id=UUID('6b5403a1-7f9e-45d3-8c1f-b43a9b595583'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 186668, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 362328, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 186668, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 362328, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, reference_example_id=None, parent_run_id=UUID('7d8c9e46-86f2-44f1-82ea-e8184bff8d4a'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002651643167Z29cf99d7-6812-4e29-9202-3af8a2e323d1.20240628T002651971311Z7d8c9e46-86f2-44f1-82ea-e8184bff8d4a.20240628T002652186668Z6b5403a1-7f9e-45d3-8c1f-b43a9b595583'), Run(id=UUID('b9ab6d7d-a343-4e66-b597-1b9e4f75cb62'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 363259, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 355201, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 363259, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 355201, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'documents': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=None, parent_run_id=UUID('7d8c9e46-86f2-44f1-82ea-e8184bff8d4a'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002651643167Z29cf99d7-6812-4e29-9202-3af8a2e323d1.20240628T002651971311Z7d8c9e46-86f2-44f1-82ea-e8184bff8d4a.20240628T002652363259Zb9ab6d7d-a343-4e66-b597-1b9e4f75cb62')], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002651643167Z29cf99d7-6812-4e29-9202-3af8a2e323d1.20240628T002651971311Z7d8c9e46-86f2-44f1-82ea-e8184bff8d4a'), Run(id=UUID('74acb5cc-cf30-43aa-8852-c8e6e4ad8a3c'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 147038, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 275066, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 147038, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 275066, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, reference_example_id=None, parent_run_id=UUID('29cf99d7-6812-4e29-9202-3af8a2e323d1'), tags=['map:key:question'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002651643167Z29cf99d7-6812-4e29-9202-3af8a2e323d1.20240628T002652147038Z74acb5cc-cf30-43aa-8852-c8e6e4ad8a3c')], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002651643167Z29cf99d7-6812-4e29-9202-3af8a2e323d1'), Run(id=UUID('8ba9bbfe-2851-4f67-aee5-30d936688125'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 456763, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 456763, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002714456763Z8ba9bbfe-2851-4f67-aee5-30d936688125'), Run(id=UUID('27e77bf3-424c-45ed-85ef-e3136f6b2be8'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 18, 773398, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 600276, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 18, 773398, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 600276, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'response': AIMessage(content='LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ef17578-441d-4ece-a8ed-fb34021a020c-0', usage_metadata={'input_tokens': 1617, 'output_tokens': 18, 'total_tokens': 1635}), 'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=None, parent_run_id=UUID('11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd'), tags=['seq:step:3'], child_runs=[Run(id=UUID('fbb14f9d-6dd4-450f-a307-b482ac82eb95'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 367492, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 394610, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 19, 367492, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 394610, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': AIMessage(content='LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4ef17578-441d-4ece-a8ed-fb34021a020c-0', usage_metadata={'input_tokens': 1617, 'output_tokens': 18, 'total_tokens': 1635})}, reference_example_id=None, parent_run_id=UUID('27e77bf3-424c-45ed-85ef-e3136f6b2be8'), tags=['map:key:response'], child_runs=[Run(id=UUID('4ddaae38-f29b-4849-b2b4-c3d4dcae77e6'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 350195, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 551149, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 350195, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 551149, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content=\"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\\\nRelease NotesBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]\\n\\nQuestion:\\nWhat recent announcement did LangChain make regarding its availability in the Azure Marketplace?\\n\")])}, reference_example_id=None, parent_run_id=UUID('fbb14f9d-6dd4-450f-a307-b482ac82eb95'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002718773398Z27e77bf3-424c-45ed-85ef-e3136f6b2be8.20240628T002719367492Zfbb14f9d-6dd4-450f-a307-b482ac82eb95.20240628T002720350195Z4ddaae38-f29b-4849-b2b4-c3d4dcae77e6'), Run(id=UUID('4ef17578-441d-4ece-a8ed-fb34021a020c'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 779051, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 277998, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 779051, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 277998, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\\\nRelease NotesBy LangChain\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\\\nBy LangChainRelease Notes\\\\n\\\\n\\\\nJoin our newsletter\\\\nUpdates from the LangChain team and community\\\\n\\\\n\\\\nEnter your email\\\\n\\\\nSubscribe\\\\n\\\\nProcessing your application...\\\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\\\nSorry, something went wrong. Please try again.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nYou might also like\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 6/10] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n5 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/27] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n7 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 5/13] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n6 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n[Week of 4/29] LangChain Release Notes\\\\n\\\\n\\\\nRelease Notes\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\\\n\\\\n\\\\nBy LangChain\\\\n4 min read\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nSign up\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]\\n\\nQuestion:\\nWhat recent announcement did LangChain make regarding its availability in the Azure Marketplace?\\n\", 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'LangChain recently announced that LangSmith is now a transactable offering in the Azure Marketplace.', 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-4ef17578-441d-4ece-a8ed-fb34021a020c-0', 'usage_metadata': {'input_tokens': 1617, 'output_tokens': 18, 'total_tokens': 1635}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 1617, 'total_tokens': 1635}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('fbb14f9d-6dd4-450f-a307-b482ac82eb95'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002718773398Z27e77bf3-424c-45ed-85ef-e3136f6b2be8.20240628T002719367492Zfbb14f9d-6dd4-450f-a307-b482ac82eb95.20240628T002720779051Z4ef17578-441d-4ece-a8ed-fb34021a020c')], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002718773398Z27e77bf3-424c-45ed-85ef-e3136f6b2be8.20240628T002719367492Zfbb14f9d-6dd4-450f-a307-b482ac82eb95'), Run(id=UUID('9bca7687-900d-48cb-a84d-01b3ad04138f'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 19, 739647, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 589909, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 19, 739647, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 589909, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})], 'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'output': [Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-1-22-24-langchain-release-notes/', 'lastmod': '2024-02-13T19:22:37.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-2-19-langchain-release-notes/', 'lastmod': '2024-05-17T17:31:42.000Z'}), Document(page_content='Tags\\nRelease NotesBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-16-langchain-release-notes/', 'lastmod': '2023-10-20T15:15:53.000Z'}), Document(page_content='Tags\\nBy LangChainRelease Notes\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow Factory used LangSmith to automate their feedback loop and improve iteration speed by 2x\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 6/10] LangChain Release Notes\\n\\n\\nRelease Notes\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/27] LangChain Release Notes\\n\\n\\nRelease Notes\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 5/13] LangChain Release Notes\\n\\n\\nRelease Notes\\n6 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[Week of 4/29] LangChain Release Notes\\n\\n\\nRelease Notes\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAnnouncing LangSmith is now a transactable offering in the Azure Marketplace\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2024', metadata={'source': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-10-2-langchain-release-notes/', 'lastmod': '2023-10-06T17:02:48.000Z'})]}, reference_example_id=None, parent_run_id=UUID('27e77bf3-424c-45ed-85ef-e3136f6b2be8'), tags=['map:key:context'], child_runs=[], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002718773398Z27e77bf3-424c-45ed-85ef-e3136f6b2be8.20240628T002719739647Z9bca7687-900d-48cb-a84d-01b3ad04138f')], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd.20240628T002718773398Z27e77bf3-424c-45ed-85ef-e3136f6b2be8')], trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc'), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc.20240628T002651166376Z11ca9f9c-4f78-4f8b-b2e9-dc53c55382dd')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651013929Zfb941117-cedc-42f0-b2f5-f3e837ec56cc', trace_id=UUID('fb941117-cedc-42f0-b2f5-f3e837ec56cc')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What recent announcement did LangChain make regarding its availability in the Azure Marketplace?'}, outputs={'answer': '© LangChain Blog 2024'}, metadata={'dataset_split': ['base']}, id=UUID('99b7533d-891c-43ad-a206-27782e10e35f'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 543088, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 543088, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('376a5991-5ba9-448e-b959-e1a225454445'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4042ebde-3f1f-45ce-b57d-4059b0297d46'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=4, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('940eec89-f664-45f6-8e36-ff3d74369943'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7d39fd71-2266-49de-9005-795642f3a445'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('f15332e0-606b-4762-9e0d-a53b246c7716'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 329383, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 279148, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.530415+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, 'config': None}, outputs={'response': AIMessage(content='The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c-0', usage_metadata={'input_tokens': 981, 'output_tokens': 71, 'total_tokens': 1052}), 'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=UUID('6f5b2e3e-487e-426c-b5ba-314e1c5925f9'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('da4c2b15-6ef7-4d38-b41f-3e27e548a9a4'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 750354, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 277445, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 750354, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 277445, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'response': AIMessage(content='The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c-0', usage_metadata={'input_tokens': 981, 'output_tokens': 71, 'total_tokens': 1052}), 'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), tags=[], child_runs=[Run(id=UUID('daf0a30c-ea2c-44e7-ad9a-3a56026ce593'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 926021, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 544234, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 926021, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 544234, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, reference_example_id=None, parent_run_id=UUID('da4c2b15-6ef7-4d38-b41f-3e27e548a9a4'), tags=['seq:step:1'], child_runs=[Run(id=UUID('4f2cfae2-4476-48ce-b6dc-4699767b1e9d'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 358796, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 450576, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 358796, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 450576, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('daf0a30c-ea2c-44e7-ad9a-3a56026ce593'), tags=['map:key:context'], child_runs=[Run(id=UUID('4ddcb324-4888-47ea-ba03-d9d895ff5b2e'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 525483, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 703624, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 525483, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 703624, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, reference_example_id=None, parent_run_id=UUID('4f2cfae2-4476-48ce-b6dc-4699767b1e9d'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002651926021Zdaf0a30c-ea2c-44e7-ad9a-3a56026ce593.20240628T002652358796Z4f2cfae2-4476-48ce-b6dc-4699767b1e9d.20240628T002652525483Z4ddcb324-4888-47ea-ba03-d9d895ff5b2e'), Run(id=UUID('0e201e70-5998-4864-9a43-2e93c8f6743b'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 705889, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 380533, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 705889, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 380533, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'documents': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4f2cfae2-4476-48ce-b6dc-4699767b1e9d'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002651926021Zdaf0a30c-ea2c-44e7-ad9a-3a56026ce593.20240628T002652358796Z4f2cfae2-4476-48ce-b6dc-4699767b1e9d.20240628T002652705889Z0e201e70-5998-4864-9a43-2e93c8f6743b')], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002651926021Zdaf0a30c-ea2c-44e7-ad9a-3a56026ce593.20240628T002652358796Z4f2cfae2-4476-48ce-b6dc-4699767b1e9d'), Run(id=UUID('fe3e9035-fee1-40ab-8876-c584d8579164'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 600371, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 847095, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 600371, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 847095, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, reference_example_id=None, parent_run_id=UUID('daf0a30c-ea2c-44e7-ad9a-3a56026ce593'), tags=['map:key:question'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002651926021Zdaf0a30c-ea2c-44e7-ad9a-3a56026ce593.20240628T002652600371Zfe3e9035-fee1-40ab-8876-c584d8579164')], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002651926021Zdaf0a30c-ea2c-44e7-ad9a-3a56026ce593'), Run(id=UUID('fc4aba24-8336-4b4a-922d-48b9ed06825a'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 744845, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 744845, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('da4c2b15-6ef7-4d38-b41f-3e27e548a9a4'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002720744845Zfc4aba24-8336-4b4a-922d-48b9ed06825a'), Run(id=UUID('860ea424-eb20-4732-a4cd-554207a8563a'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 246193, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 275953, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 246193, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 275953, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'response': AIMessage(content='The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c-0', usage_metadata={'input_tokens': 981, 'output_tokens': 71, 'total_tokens': 1052}), 'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('da4c2b15-6ef7-4d38-b41f-3e27e548a9a4'), tags=['seq:step:3'], child_runs=[Run(id=UUID('28da5271-7f3e-4602-b505-8909926fe056'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 811638, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 274276, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 811638, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 274276, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': AIMessage(content='The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c-0', usage_metadata={'input_tokens': 981, 'output_tokens': 71, 'total_tokens': 1052})}, reference_example_id=None, parent_run_id=UUID('860ea424-eb20-4732-a4cd-554207a8563a'), tags=['map:key:response'], child_runs=[Run(id=UUID('67865020-5ca9-42a0-acfd-c14f654beae0'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 147225, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 402794, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 147225, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 402794, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Videos must always have exactly 12 scenes.\\\\nToday\\'s date (UTC) is ${new Date().toLocaleDateString()}.`\\\\n  ],\\\\n  [\\'human\\', `The GitHub stats are as follows: ${stats}`]\\\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from \\'zod\\'\", metadata={\\'source\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'loc\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'lastmod\\': \\'2023-12-19T18:27:38.000Z\\'}), Document(page_content=\\'<People from={i * fps * 5} people={animation.people} />\\\\n        )\\\\n      // ...\\\\n      default:\\\\n        return (\\\\n            <Conclusion from={i * fps * 5} text={text} />\\\\n        )\\\\n    }\\\\n  })\\\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\\\xa0hello@rubriclabs.com.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'loc\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'lastmod\\': \\'2023-12-19T18:27:38.000Z\\'})]\\n\\nQuestion:\\nWhat approach did the team decide to take to ensure high-quality video generation within the given timeframe?\\n')])}, reference_example_id=None, parent_run_id=UUID('28da5271-7f3e-4602-b505-8909926fe056'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002722246193Z860ea424-eb20-4732-a4cd-554207a8563a.20240628T002722811638Z28da5271-7f3e-4602-b505-8909926fe056.20240628T002723147225Z67865020-5ca9-42a0-acfd-c14f654beae0'), Run(id=UUID('3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 472861, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 273675, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 472861, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 273675, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Videos must always have exactly 12 scenes.\\\\nToday\\'s date (UTC) is ${new Date().toLocaleDateString()}.`\\\\n  ],\\\\n  [\\'human\\', `The GitHub stats are as follows: ${stats}`]\\\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from \\'zod\\'\", metadata={\\'source\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'loc\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'lastmod\\': \\'2023-12-19T18:27:38.000Z\\'}), Document(page_content=\\'<People from={i * fps * 5} people={animation.people} />\\\\n        )\\\\n      // ...\\\\n      default:\\\\n        return (\\\\n            <Conclusion from={i * fps * 5} text={text} />\\\\n        )\\\\n    }\\\\n  })\\\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\\\xa0hello@rubriclabs.com.\\', metadata={\\'source\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'loc\\': \\'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/\\', \\'lastmod\\': \\'2023-12-19T18:27:38.000Z\\'})]\\n\\nQuestion:\\nWhat approach did the team decide to take to ensure high-quality video generation within the given timeframe?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The team decided to strike a middle ground by creating a bank of \"scenes\" and parametrizing them as much as possible. This allowed the AI to pick the most relevant scenes based on the user\\'s data and descriptions for each scene. By using AI-selected scenes and passing user-specific data, they were able to generate a unique sequence of personalized frames.', 'response_metadata': {'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c-0', 'usage_metadata': {'input_tokens': 981, 'output_tokens': 71, 'total_tokens': 1052}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 71, 'prompt_tokens': 981, 'total_tokens': 1052}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('28da5271-7f3e-4602-b505-8909926fe056'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002722246193Z860ea424-eb20-4732-a4cd-554207a8563a.20240628T002722811638Z28da5271-7f3e-4602-b505-8909926fe056.20240628T002723472861Z3e1ff7ad-56e8-4971-99cb-18a6f4d30f3c')], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002722246193Z860ea424-eb20-4732-a4cd-554207a8563a.20240628T002722811638Z28da5271-7f3e-4602-b505-8909926fe056'), Run(id=UUID('6a66194f-4a38-4191-8e69-ee9139ab5484'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 845327, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 220543, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 845327, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 220543, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})], 'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'output': [Document(page_content=\"Videos must always have exactly 12 scenes.\\nToday's date (UTC) is ${new Date().toLocaleDateString()}.`\\n  ],\\n  ['human', `The GitHub stats are as follows: ${stats}`]\\n]);Full link to prompt hereGiven user stats, the AI generates a video_manifest which is similar to a script for the video. The manifest tells a unique story in 12 sequences (as defined in the prompt). Assuming each sequence lasts 5 seconds, this results in a 60 second video consistently.Here we ran into a challenging problem: do we give the AI complete creative freedom or do we template as guardrails for the AI?After running some experiments, we quickly realized that in the given timeframe, we couldn’t generate high quality video by giving AI the complete creative freedom. While the output was decent and could have been improved, it wasn’t good enough to have that nostalgic moment, especially in the engineering time that we had.So instead we struck middle ground by creating a bank of “scenes” and parametrized them as much as possible. This allowed the AI pick the most relevant scenes based on the user’s data based on descriptions for each scene that the AI could match with. Using these AI-selected scenes, and passing user-specific data, we are able to generate a unique sequence of personalized frames.This was possible using OpenAI’s Function Calling which enabled the AI to output parsable text, conforming to a Zod schema. The schema uses a Zod discriminated union (not the name of a rockband) to distinguish scenes:import z from 'zod'\", metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'}), Document(page_content='<People from={i * fps * 5} people={animation.people} />\\n        )\\n      // ...\\n      default:\\n        return (\\n            <Conclusion from={i * fps * 5} text={text} />\\n        )\\n    }\\n  })\\n}Playing the video in the clientHere, the from prop determines the first frame when this scene will appear.To generate 3D objects, we leveraged Three.js. For example, to mould this wormhole effect from a flat galaxy image, we pushed Three’s TubeGeometry to its limits with high polygon count and low radius.Wormhole effectNow, we want this experience to scale by being as lightweight as possible. By saving the video_manifest, instead of the actual video, we trim the bulk of the project’s bandwidth and storage by 100x. Another benefit of this approach is that the video is actually interactive.Rendering the videoSince we map over a manifest in the client using React components, to download the video as .mp4, we have to render the video first. This is achieved using Remotion lambda leveraging 10,000 concurrent AWS Lambda instances and storing the file in an S3 bucket. Each user only has to render their video once, after which we store their download URL in Supabase for subsequent downloads.This step is the most expensive in the entire process and we intentionally added some friction to this step so that only the users that care the most about sharing their video end up executing this step.ConclusionThis project makes use of all the latest tech: server-side rendering, an open-source database, LLMs, 3D, generative video. These sound like buzzwords but each is used very intentionally in this project. We hope it inspires you to build something new in 2024!Ready for takeoff? Give Year in code a try. Translate your keystrokes into stardust. Find solace in your retrospection, let others join you in your journey, and connect with starfarers alike.Your chronicle awaits.Thanks for reading! If you have feedback on this post, please reach out to us at\\xa0hello@rubriclabs.com.', metadata={'source': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'loc': 'https://blog.langchain.dev/rubric-labs-graphite-personalized-video-at-scale/', 'lastmod': '2023-12-19T18:27:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('860ea424-eb20-4732-a4cd-554207a8563a'), tags=['map:key:context'], child_runs=[], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002722246193Z860ea424-eb20-4732-a4cd-554207a8563a.20240628T002722845327Z6a66194f-4a38-4191-8e69-ee9139ab5484')], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4.20240628T002722246193Z860ea424-eb20-4732-a4cd-554207a8563a')], trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766'), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766.20240628T002651750354Zda4c2b15-6ef7-4d38-b41f-3e27e548a9a4')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651329383Z735bce82-7959-4768-bfc5-618fbae8e766', trace_id=UUID('735bce82-7959-4768-bfc5-618fbae8e766')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What approach did the team decide to take to ensure high-quality video generation within the given timeframe?'}, outputs={'answer': 'The goal of the video is to make the end user feel seen, valued, and have a nostalgic moment of review.'}, metadata={'dataset_split': ['base']}, id=UUID('6f5b2e3e-487e-426c-b5ba-314e1c5925f9'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 530415, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 530415, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('14f2f0bb-a7e8-47a3-bd85-12a72505ed58'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3da34274-e6e5-475e-81ea-1543c6cecb9b'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=2, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a761a58c-6013-4e59-90e8-a8173d367957'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('248d420a-bad9-4666-a6fb-b72c6ce67d2d'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3dab052b-6030-46f8-83e5-e5419dda5e7f'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 747418, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 204389, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:19.948885+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 51.0, 'mem': {'rss': 1279164416.0}, 'cpu': {'time': {'sys': 300.3, 'user': 142.08}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, 'config': None}, outputs={'response': AIMessage(content='The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-00ec86a3-4d50-4d44-89e1-ba38a8bed717-0', usage_metadata={'input_tokens': 1050, 'output_tokens': 73, 'total_tokens': 1123}), 'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=UUID('9554553c-c33d-4b91-9878-7527a562d2b3'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('670196dd-510e-4119-805f-02c1a582d4ba'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 80669, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 202588, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 80669, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 202588, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'response': AIMessage(content='The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-00ec86a3-4d50-4d44-89e1-ba38a8bed717-0', usage_metadata={'input_tokens': 1050, 'output_tokens': 73, 'total_tokens': 1123}), 'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=None, parent_run_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), tags=[], child_runs=[Run(id=UUID('c75a9de1-58c5-4042-bdbf-a9af319fde4b'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 259200, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 523238, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 259200, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 523238, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, reference_example_id=None, parent_run_id=UUID('670196dd-510e-4119-805f-02c1a582d4ba'), tags=['seq:step:1'], child_runs=[Run(id=UUID('edb9eea5-fb10-42cd-802d-9bf6b64404bc'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 783879, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 434743, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 783879, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 434743, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=None, parent_run_id=UUID('c75a9de1-58c5-4042-bdbf-a9af319fde4b'), tags=['map:key:context'], child_runs=[Run(id=UUID('8e13183b-a859-4956-9356-b4e9627a6f7b'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 858545, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 227935, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 858545, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 227935, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, reference_example_id=None, parent_run_id=UUID('edb9eea5-fb10-42cd-802d-9bf6b64404bc'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002652259200Zc75a9de1-58c5-4042-bdbf-a9af319fde4b.20240628T002652783879Zedb9eea5-fb10-42cd-802d-9bf6b64404bc.20240628T002652858545Z8e13183b-a859-4956-9356-b4e9627a6f7b'), Run(id=UUID('b83eb5b0-33d0-45ce-ba07-6d53c44e7dc7'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 230857, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 349123, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 230857, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 349123, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'documents': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=None, parent_run_id=UUID('edb9eea5-fb10-42cd-802d-9bf6b64404bc'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002652259200Zc75a9de1-58c5-4042-bdbf-a9af319fde4b.20240628T002652783879Zedb9eea5-fb10-42cd-802d-9bf6b64404bc.20240628T002653230857Zb83eb5b0-33d0-45ce-ba07-6d53c44e7dc7')], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002652259200Zc75a9de1-58c5-4042-bdbf-a9af319fde4b.20240628T002652783879Zedb9eea5-fb10-42cd-802d-9bf6b64404bc'), Run(id=UUID('1a2ca212-814b-4f07-ba7b-3ceef3add243'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 191405, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 470013, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 191405, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 470013, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, reference_example_id=None, parent_run_id=UUID('c75a9de1-58c5-4042-bdbf-a9af319fde4b'), tags=['map:key:question'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002652259200Zc75a9de1-58c5-4042-bdbf-a9af319fde4b.20240628T002653191405Z1a2ca212-814b-4f07-ba7b-3ceef3add243')], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002652259200Zc75a9de1-58c5-4042-bdbf-a9af319fde4b'), Run(id=UUID('1c08b023-20bf-41ff-9588-5a3cbf7709bc'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 716892, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 716892, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('670196dd-510e-4119-805f-02c1a582d4ba'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002720716892Z1c08b023-20bf-41ff-9588-5a3cbf7709bc'), Run(id=UUID('40caefbf-8b1b-4a16-b4dc-6b2ffda11e35'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 317894, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 200468, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 317894, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 200468, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'response': AIMessage(content='The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-00ec86a3-4d50-4d44-89e1-ba38a8bed717-0', usage_metadata={'input_tokens': 1050, 'output_tokens': 73, 'total_tokens': 1123}), 'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=None, parent_run_id=UUID('670196dd-510e-4119-805f-02c1a582d4ba'), tags=['seq:step:3'], child_runs=[Run(id=UUID('48215d1a-c746-40a3-a6e7-5adc1c413fb7'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 923754, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 199001, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 923754, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 199001, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': AIMessage(content='The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-00ec86a3-4d50-4d44-89e1-ba38a8bed717-0', usage_metadata={'input_tokens': 1050, 'output_tokens': 73, 'total_tokens': 1123})}, reference_example_id=None, parent_run_id=UUID('40caefbf-8b1b-4a16-b4dc-6b2ffda11e35'), tags=['map:key:response'], child_runs=[Run(id=UUID('51c9810a-bdc9-4a08-b360-e3cb92d3379e'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 398160, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 548007, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 398160, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 548007, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don\\'t have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we\\'ve obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={\\'source\\': \\'https://blog.langchain.dev/self-learning-gpts/\\', \\'loc\\': \\'https://blog.langchain.dev/self-learning-gpts/\\', \\'lastmod\\': \\'2024-03-21T03:50:34.000Z\\'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere\\'s New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner\\'s Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/\\', \\'lastmod\\': \\'2024-05-17T17:30:30.000Z\\'})]\\n\\nQuestion:\\nWhat are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?\\n')])}, reference_example_id=None, parent_run_id=UUID('48215d1a-c746-40a3-a6e7-5adc1c413fb7'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002722317894Z40caefbf-8b1b-4a16-b4dc-6b2ffda11e35.20240628T002722923754Z48215d1a-c746-40a3-a6e7-5adc1c413fb7.20240628T002723398160Z51c9810a-bdc9-4a08-b360-e3cb92d3379e'), Run(id=UUID('00ec86a3-4d50-4d44-89e1-ba38a8bed717'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 660459, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 198362, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 660459, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 198362, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don\\'t have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we\\'ve obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={\\'source\\': \\'https://blog.langchain.dev/self-learning-gpts/\\', \\'loc\\': \\'https://blog.langchain.dev/self-learning-gpts/\\', \\'lastmod\\': \\'2024-03-21T03:50:34.000Z\\'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere\\'s New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner\\'s Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={\\'source\\': \\'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/\\', \\'loc\\': \\'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/\\', \\'lastmod\\': \\'2024-05-17T17:30:30.000Z\\'})]\\n\\nQuestion:\\nWhat are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The main similarities between DSPy and LangChain in the context of optimizing LLM systems are that they both deal with optimizations of LLM systems, not just of a single LLM call. Additionally, a key component of both DSPy and LangChain is tracing, which is emphasized in both frameworks to underpin a lot of the optimizations done for LLM systems.', 'response_metadata': {'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-00ec86a3-4d50-4d44-89e1-ba38a8bed717-0', 'usage_metadata': {'input_tokens': 1050, 'output_tokens': 73, 'total_tokens': 1123}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 73, 'prompt_tokens': 1050, 'total_tokens': 1123}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('48215d1a-c746-40a3-a6e7-5adc1c413fb7'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002722317894Z40caefbf-8b1b-4a16-b4dc-6b2ffda11e35.20240628T002722923754Z48215d1a-c746-40a3-a6e7-5adc1c413fb7.20240628T002723660459Z00ec86a3-4d50-4d44-89e1-ba38a8bed717')], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002722317894Z40caefbf-8b1b-4a16-b4dc-6b2ffda11e35.20240628T002722923754Z48215d1a-c746-40a3-a6e7-5adc1c413fb7'), Run(id=UUID('9c087d6d-764b-4ca9-9360-376915c32460'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 140329, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 407006, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 140329, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 407006, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})], 'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'output': [Document(page_content=\"move runs (according to some filter and sampling rate) into a dataset (or annotation queue). We will be rolling this out to everyone in a few weeks. In the meantime, if you are interested in early access please sign up here. ImportanceWe are excited to showcase this for two reasons.Mainly: this is a reliable way to automatically construct datasets of good examples. When talking with teams building LLM applications, they often don't have a dataset of good input/output pairs. They were often doing this process of collecting feedback and then moving to a dataset manually. This automates that and makes it easy for anyone to do.Secondarily: incorporating them as few shot examples is a cheap and practical way to use this feedback to improve your application. Previously when collecting feedback, all we could use it for was helping to highlight datapoints you should look at. This is far more actionable.Note that this is not the only way to use these datapoints. You could use them as test set to do regression testing against. You could use them to fine-tune a model. You could do dynamic selection of few shot examples. You could use these examples in a DSPy-like manner to optimize your LLM system.Optimization of LLM SystemsDSPy is a framework for algorithmically optimizing LLM prompts and weights. We have chatted in depth with Omar about optimization of LLM systems and done an initial integration with them. We build off of a lot of ideas in DSPy to create this example.There are actually a lot of similarities between DSPy and LangChain. Mainly - they both deal with optimizations of LLM systems, not just of a single LLM call. A key component of this is tracing, a concept that both DSPy and LangChain place a lot of emphasis on. Tracing underpins a lot of the optimizations that DSPy does, and with LangSmith we've obviously placed a lot of emphasis on it as well.Tracing the full system is particularly important because you will usually gather feedback at the system level, but\", metadata={'source': 'https://blog.langchain.dev/self-learning-gpts/', 'loc': 'https://blog.langchain.dev/self-learning-gpts/', 'lastmod': '2024-03-21T03:50:34.000Z'}), Document(page_content=\"series breaks down important RAG concepts in short videos with accompanying code. We’ve recently released two more concepts:Indexing w/ RAPTOR: Learn more about hierarchical indexing in this video about RAPTOR, a paper that tackles the challenge of handling both lower and higher level user questions in RAG systems.Feedback + Self-Reflection: RAG systems can suffer from low-quality retrieval and hallucinations. In this video, we introduce the concept of Flow Engineering using LangGraph to orchestrate checks and feedback for more reliable performance.👀 In Case You Missed It from LangChain🔁 Optimization of LLM Systems with DSPy – Webinar: Relying on prompt engineering to improve complex LLM systems can be time consuming. DSPy helps with optimization of these systems using automatic evaluation, and we’re starting to incorporate these techniques in LangChain’s suite of products. Learn more in this webinar replay with our CEO Harrison Chase and creator of DSPy Omar Khattab.💻 Adaptive RAG w/ Cohere's New Command-R+: Adaptive-RAG is a recent paper that combines query analysis and iterative answer construction to handle queries of varying complexity. Watch this video to see us implement these ideas from scratch with LangGraph. LangGraph Code. ReACT Agent Code.🎙️📹 Audio & Video Structured Extraction with Gemini: Gemini 1.5 Pro now has support for audio and video inputs, opening up the gates for new use cases. In this video, we show you how to perform structured extraction on YouTube videos and audio clips using LangChain JS/TS. Docs.🦜🧑\\u200d🏫 How to Use LangChain to Build With LLMs – A Beginner's Guide: Learn the up-to-date fundamentals of building with LLMs in our new guide on freeCodeCamp. You’ll learn how to use LCEL, streaming, LangSmith tracing, and more. This is a great resource if you’re just starting out!🤝 From the CommunityHow Physics Wallah Uses LangChain, DataStax Astra DB, Vector Search and RAG to Revolutionize Education by Jauneet Singh at DataStax.LangChain\", metadata={'source': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'loc': 'https://blog.langchain.dev/week-of-4-15-langchain-release-notes/', 'lastmod': '2024-05-17T17:30:30.000Z'})]}, reference_example_id=None, parent_run_id=UUID('40caefbf-8b1b-4a16-b4dc-6b2ffda11e35'), tags=['map:key:context'], child_runs=[], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002722317894Z40caefbf-8b1b-4a16-b4dc-6b2ffda11e35.20240628T002723140329Z9c087d6d-764b-4ca9-9360-376915c32460')], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba.20240628T002722317894Z40caefbf-8b1b-4a16-b4dc-6b2ffda11e35')], trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82'), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82.20240628T002652080669Z670196dd-510e-4119-805f-02c1a582d4ba')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651747418Z962016e4-1272-49b0-b821-f72bfb9c1e82', trace_id=UUID('962016e4-1272-49b0-b821-f72bfb9c1e82')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What are the main similarities between DSPy and LangChain in the context of optimizing LLM systems?'}, outputs={'answer': 'The answer must only rely on the provided context.\\n\\nQuestion:\\nWhat is the main reason tracing is important in optimizing LLM systems according to the provided context?\\n\\nAnswer:\\nTracing is important in optimizing LLM systems because it allows the association of high-level feedback with the lower-level LLM calls, which is crucial for system-level feedback and optimizations.'}, metadata={'dataset_split': ['base']}, id=UUID('9554553c-c33d-4b91-9878-7527a562d2b3'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 948885, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 948885, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8b6c9a98-bf18-4b29-bb94-0527f02bf2c5'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('da3053cc-8705-4e25-86a2-61fe34599659'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1d119dd4-3034-4fdf-a25e-330f30b60af1'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('50b1939b-aa6f-4a17-ae55-8877534bbc91'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('854cb3a3-0114-4c34-bb1d-4f1475c76d57'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 599285, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 384903, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.064081+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, 'config': None}, outputs={'response': AIMessage(content=\"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6880ac5-cf78-40b7-8c01-d96eb8dc4125-0', usage_metadata={'input_tokens': 2146, 'output_tokens': 88, 'total_tokens': 2234}), 'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=UUID('b0b807e2-37cb-40f7-921c-eb8bec0176c9'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('d46f6d89-2dc5-43b3-a7a2-f471a8d59182'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 29073, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 383263, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 29073, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 383263, tzinfo=datetime.timezone.utc)}], inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'response': AIMessage(content=\"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6880ac5-cf78-40b7-8c01-d96eb8dc4125-0', usage_metadata={'input_tokens': 2146, 'output_tokens': 88, 'total_tokens': 2234}), 'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=None, parent_run_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), tags=[], child_runs=[Run(id=UUID('514d7478-832a-44e0-97d0-9beeddd85e00'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 216873, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 653145, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 216873, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 653145, tzinfo=datetime.timezone.utc)}], inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, reference_example_id=None, parent_run_id=UUID('d46f6d89-2dc5-43b3-a7a2-f471a8d59182'), tags=['seq:step:1'], child_runs=[Run(id=UUID('f73755aa-5a7b-47d3-a9b7-cd9e36e15778'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 670278, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 482401, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 670278, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 482401, tzinfo=datetime.timezone.utc)}], inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=None, parent_run_id=UUID('514d7478-832a-44e0-97d0-9beeddd85e00'), tags=['map:key:context'], child_runs=[Run(id=UUID('89e8a436-12c9-45bc-853f-7e2b587825ca'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 869590, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 354045, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 869590, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 354045, tzinfo=datetime.timezone.utc)}], inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, reference_example_id=None, parent_run_id=UUID('f73755aa-5a7b-47d3-a9b7-cd9e36e15778'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002652216873Z514d7478-832a-44e0-97d0-9beeddd85e00.20240628T002652670278Zf73755aa-5a7b-47d3-a9b7-cd9e36e15778.20240628T002652869590Z89e8a436-12c9-45bc-853f-7e2b587825ca'), Run(id=UUID('8147e7f7-d892-4a29-8a3e-17140e5f2f71'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 356091, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 393959, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 356091, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 393959, tzinfo=datetime.timezone.utc)}], inputs={'query': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'documents': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=None, parent_run_id=UUID('f73755aa-5a7b-47d3-a9b7-cd9e36e15778'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002652216873Z514d7478-832a-44e0-97d0-9beeddd85e00.20240628T002652670278Zf73755aa-5a7b-47d3-a9b7-cd9e36e15778.20240628T002653356091Z8147e7f7-d892-4a29-8a3e-17140e5f2f71')], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002652216873Z514d7478-832a-44e0-97d0-9beeddd85e00.20240628T002652670278Zf73755aa-5a7b-47d3-a9b7-cd9e36e15778'), Run(id=UUID('1baa8961-7016-4a68-862a-cae8f405e428'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 999548, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 703323, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 999548, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 703323, tzinfo=datetime.timezone.utc)}], inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, reference_example_id=None, parent_run_id=UUID('514d7478-832a-44e0-97d0-9beeddd85e00'), tags=['map:key:question'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002652216873Z514d7478-832a-44e0-97d0-9beeddd85e00.20240628T002652999548Z1baa8961-7016-4a68-862a-cae8f405e428')], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002652216873Z514d7478-832a-44e0-97d0-9beeddd85e00'), Run(id=UUID('3676d685-0db8-491d-ab1d-84cf8d900708'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 938658, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 938658, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs=None, reference_example_id=None, parent_run_id=UUID('d46f6d89-2dc5-43b3-a7a2-f471a8d59182'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002720938658Z3676d685-0db8-491d-ab1d-84cf8d900708'), Run(id=UUID('04967f41-590f-4914-9a15-ff442e9f3e38'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 479937, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 382247, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 479937, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 382247, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'response': AIMessage(content=\"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6880ac5-cf78-40b7-8c01-d96eb8dc4125-0', usage_metadata={'input_tokens': 2146, 'output_tokens': 88, 'total_tokens': 2234}), 'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=None, parent_run_id=UUID('d46f6d89-2dc5-43b3-a7a2-f471a8d59182'), tags=['seq:step:3'], child_runs=[Run(id=UUID('8e30ca2a-b4f4-4c57-8a5d-ad4b31e4244e'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 271217, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 477853, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 271217, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 477853, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]}, reference_example_id=None, parent_run_id=UUID('04967f41-590f-4914-9a15-ff442e9f3e38'), tags=['map:key:context'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002722479937Z04967f41-590f-4914-9a15-ff442e9f3e38.20240628T002723271217Z8e30ca2a-b4f4-4c57-8a5d-ad4b31e4244e'), Run(id=UUID('a0c7bef7-ccb0-408a-aa3e-08a0d5460847'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 290703, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 381006, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 290703, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 381006, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': AIMessage(content=\"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a6880ac5-cf78-40b7-8c01-d96eb8dc4125-0', usage_metadata={'input_tokens': 2146, 'output_tokens': 88, 'total_tokens': 2234})}, reference_example_id=None, parent_run_id=UUID('04967f41-590f-4914-9a15-ff442e9f3e38'), tags=['map:key:response'], child_runs=[Run(id=UUID('f5d03cc1-dc9f-4931-a88f-0d73996405aa'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 438586, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 597023, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 438586, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 597023, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})], 'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content=\"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]\\n\\nQuestion:\\nWhat is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\\n\")])}, reference_example_id=None, parent_run_id=UUID('a0c7bef7-ccb0-408a-aa3e-08a0d5460847'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002722479937Z04967f41-590f-4914-9a15-ff442e9f3e38.20240628T002723290703Za0c7bef7-ccb0-408a-aa3e-08a0d5460847.20240628T002723438586Zf5d03cc1-dc9f-4931-a88f-0d73996405aa'), Run(id=UUID('a6880ac5-cf78-40b7-8c01-d96eb8dc4125'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 670977, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 380446, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 670977, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 380446, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n[Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='Prov(y): ‘the sentence with Gödel number y is provable in P’\\\\nwhich without being primitive recursive, is however obtained from Prf(x, y) by existentially quantifying x. (Prov(y) satisfies only the ‘positive’ part of numeralwise expressibility, and not the negative part; but the negative part is not needed.)\\\\nIn Theorem V of his paper, Gödel proves that any number theoretic predicate which is primitive recursive is numeralwise expressible in P. Thus since Prf(x, y) and substitution are primitive recursive, these are decided by P when closed terms are substituted for the free variables x and y. This is the heart of the matter as we will see. Another key point about numeralwise expressibility is that although we informally interpret, for example, Prov(Sb(ru1…unZ(x1)…Z(xn))), by: ‘the formula with Gödel number r is provable if the Gödel number for the xi th numeral is substituted in place of the i th variable,’ neither the formal statement within the theory P nor anything we prove about it appeals to such meanings. On the contrary Prov(Sb(ru1…unZ(x1)…Z(xn))), is a meaningless string of logical and arithmetical symbols. As Gödel puts it in his introduction to his theorem V, ‘The fact that can be formulated vaguely by saying that every recursive relation is definable in the system P (if the usual meaning is given to the formulas of this system) is expressed in precise language, without reference to any interpretation of the formulas of P, by the following Theorem (V) (Gödel 1986, p. 171, italics Gödel’s).\\\\nGödel in his incompleteness theorems uses a method given in what is called nowadays Gödel’s Fixed Point Theorem. Although Gödel constructs a fixed point in the course of proving the incompleteness theorem, he does not state the fixed point theorem explicitly. The fixed point theorem is as follows:', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'}), Document(page_content='An essential point here is that when a formula is construed as a natural number, then the numeral corresponding to that natural number can occur as the argument of a formula, thus enabling the syntax to “refer” to itself, so to speak (i.e., when a numeral is substituted into a formula the Gödel number of which the numeral represents). This will eventually allow Gödel to formalize the Liar paradox (with “provability” in place of “truth”) by substituting into the formula which says, ‘the formula, whose code is x, is unprovable,’ its own natural number code (or more precisely the corresponding numeral).\\\\nAnother concept required to carry out the formalization is the concept of numeralwise expressibility of number theoretic predicates. A number-theoretic formula φ(n1, …, nk) is numeralwise expressible in P if for each tuple of natural numbers (n1, …, nk):\\\\nN ⊨ φ(n1, …, nk) ⇒ P ⊢ φ(n1, …, nk) N ⊨ ¬φ(n1, …, nk) ⇒ P ⊢ ¬φ(n1, …, nk)\\\\nwhere n is the formal term which denotes the natural number n. (In P, this is S(S(…S(0)…), where n is the number of iterations of the successor function applied to the constant symbol 0.) One of the principal goals is to numeralwise express the predicate\\\\nPrf(x, y): ‘the sequence with Gödel number x is a proof of the sentence with Gödel number y.’\\\\nReaching this goal involves defining forty-five relations, each defined in terms of the preceding ones. These relations are all primitive recursive.[10] Relations needed are, among others, those which assert of a natural number that it codes a sequence, or a formula, or an axiom, or that it is the code, denoted by Sb(ru1…unZ(x1)…Z(xn)), of a formula obtained from a formula with code r by substituting for its free variable ui the xi th numeral for i = 1, …, n. The forty-fifth primitive recursive relation defined is Prf(x, y), and the forty-sixth is\\\\nProv(y): ‘the sentence with Gödel number y is provable in P’', metadata={'source': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'loc': 'https://blog.langchain.dev/a-chunk-by-any-other-name/', 'lastmod': '2024-03-14T20:34:52.000Z'})]\\n\\nQuestion:\\nWhat is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\\n\", 'type': 'human'}}]]}, outputs={'generations': [[{'text': \"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': \"The forty-fifth primitive recursive relation Prf(x, y) is significant in the context of Gödel's incompleteness theorems because it represents the concept that 'the sequence with Gödel number x is a proof of the sentence with Gödel number y.' This relation plays a crucial role in the formalization and understanding of Gödel's incompleteness theorems by defining the connection between proofs and sentences within the system.\", 'response_metadata': {'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-a6880ac5-cf78-40b7-8c01-d96eb8dc4125-0', 'usage_metadata': {'input_tokens': 2146, 'output_tokens': 88, 'total_tokens': 2234}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 88, 'prompt_tokens': 2146, 'total_tokens': 2234}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('a0c7bef7-ccb0-408a-aa3e-08a0d5460847'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002722479937Z04967f41-590f-4914-9a15-ff442e9f3e38.20240628T002723290703Za0c7bef7-ccb0-408a-aa3e-08a0d5460847.20240628T002723670977Za6880ac5-cf78-40b7-8c01-d96eb8dc4125')], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002722479937Z04967f41-590f-4914-9a15-ff442e9f3e38.20240628T002723290703Za0c7bef7-ccb0-408a-aa3e-08a0d5460847')], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182.20240628T002722479937Z04967f41-590f-4914-9a15-ff442e9f3e38')], trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56'), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56.20240628T002652029073Zd46f6d89-2dc5-43b3-a7a2-f471a8d59182')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651599285Z10f0e514-8379-408d-a893-6368a6d53a56', trace_id=UUID('10f0e514-8379-408d-a893-6368a6d53a56')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': \"What is the significance of the forty-fifth primitive recursive relation Prf(x, y) in the context of Gödel's incompleteness theorems?\"}, outputs={'answer': \"Gödel's First Incompleteness Theorem states that if P is ω-consistent, then there is a sentence which is neither provable nor refutable from P.\"}, metadata={'dataset_split': ['base']}, id=UUID('b0b807e2-37cb-40f7-921c-eb8bec0176c9'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 64081, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 64081, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('584a4612-3cd8-40fd-9c65-aec4fb711a9c'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c26b2aca-efee-42ee-880d-316226b7f098'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=7, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('55b9b46d-8107-499a-a0c8-3ed6da638196'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b4c8e5f3-b469-4c17-8dc3-efe314f646e4'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5c671dda-69e4-4acd-88a1-a1fc258be8d0'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 50, 934460, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 971983, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.887610+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 87.0, 'mem': {'rss': 1290809344.0}, 'cpu': {'time': {'sys': 300.04, 'user': 140.98}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, 'config': None}, outputs={'response': AIMessage(content='The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a57476d6-8abd-4d19-a85a-2bf57971f037-0', usage_metadata={'input_tokens': 1014, 'output_tokens': 98, 'total_tokens': 1112}), 'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=UUID('d596b86f-f16b-48bc-87f1-3e3a88a70898'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('385466dc-44b3-41d8-9626-5e7971df2754'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 40690, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 853846, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 40690, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 853846, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'response': AIMessage(content='The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a57476d6-8abd-4d19-a85a-2bf57971f037-0', usage_metadata={'input_tokens': 1014, 'output_tokens': 98, 'total_tokens': 1112}), 'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=None, parent_run_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), tags=[], child_runs=[Run(id=UUID('1476975a-ffc3-42f8-8de5-8472d771b693'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 386988, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 924348, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 386988, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 924348, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, reference_example_id=None, parent_run_id=UUID('385466dc-44b3-41d8-9626-5e7971df2754'), tags=['seq:step:1'], child_runs=[Run(id=UUID('a9f7ade1-94a7-407e-84de-badf3580c32b'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 772833, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 840383, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 772833, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 840383, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=None, parent_run_id=UUID('1476975a-ffc3-42f8-8de5-8472d771b693'), tags=['map:key:context'], child_runs=[Run(id=UUID('0d48ddf2-b5b0-4eb5-8979-24741a6936ef'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 149698, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 280938, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 149698, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 280938, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, reference_example_id=None, parent_run_id=UUID('a9f7ade1-94a7-407e-84de-badf3580c32b'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002651386988Z1476975a-ffc3-42f8-8de5-8472d771b693.20240628T002651772833Za9f7ade1-94a7-407e-84de-badf3580c32b.20240628T002652149698Z0d48ddf2-b5b0-4eb5-8979-24741a6936ef'), Run(id=UUID('9b61c814-b16d-4875-a295-a18ef9427d30'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 282319, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 838029, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 282319, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 838029, tzinfo=datetime.timezone.utc)}], inputs={'query': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'documents': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=None, parent_run_id=UUID('a9f7ade1-94a7-407e-84de-badf3580c32b'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002651386988Z1476975a-ffc3-42f8-8de5-8472d771b693.20240628T002651772833Za9f7ade1-94a7-407e-84de-badf3580c32b.20240628T002652282319Z9b61c814-b16d-4875-a295-a18ef9427d30')], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002651386988Z1476975a-ffc3-42f8-8de5-8472d771b693.20240628T002651772833Za9f7ade1-94a7-407e-84de-badf3580c32b'), Run(id=UUID('ef8f0094-84e7-4663-a243-1fa8d5fd65ff'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 856349, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 225547, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 856349, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 225547, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, reference_example_id=None, parent_run_id=UUID('1476975a-ffc3-42f8-8de5-8472d771b693'), tags=['map:key:question'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002651386988Z1476975a-ffc3-42f8-8de5-8472d771b693.20240628T002651856349Zef8f0094-84e7-4663-a243-1fa8d5fd65ff')], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002651386988Z1476975a-ffc3-42f8-8de5-8472d771b693'), Run(id=UUID('1c7e1f7f-5b95-415f-8c53-0885da266cf6'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 13, 949408, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 13, 949408, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('385466dc-44b3-41d8-9626-5e7971df2754'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002713949408Z1c7e1f7f-5b95-415f-8c53-0885da266cf6'), Run(id=UUID('33359ee1-a70c-4c94-907b-8315b182f19c'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 617803, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 712203, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 617803, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 712203, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'response': AIMessage(content='The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a57476d6-8abd-4d19-a85a-2bf57971f037-0', usage_metadata={'input_tokens': 1014, 'output_tokens': 98, 'total_tokens': 1112}), 'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=None, parent_run_id=UUID('385466dc-44b3-41d8-9626-5e7971df2754'), tags=['seq:step:3'], child_runs=[Run(id=UUID('c2ccab39-e740-4341-9c74-19f66d0b7360'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 717999, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 489508, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 717999, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 489508, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': AIMessage(content='The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a57476d6-8abd-4d19-a85a-2bf57971f037-0', usage_metadata={'input_tokens': 1014, 'output_tokens': 98, 'total_tokens': 1112})}, reference_example_id=None, parent_run_id=UUID('33359ee1-a70c-4c94-907b-8315b182f19c'), tags=['map:key:response'], child_runs=[Run(id=UUID('ec832506-2cb7-49c7-8bee-d01c095cf6e2'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 941682, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 946938, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 941682, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 946938, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl\\', metadata={\\'source\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'loc\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'lastmod\\': \\'2023-07-12T14:58:10.000Z\\'}), Document(page_content=\\'print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\\\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!\\', metadata={\\'source\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'loc\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'lastmod\\': \\'2023-07-12T14:58:10.000Z\\'})]\\n\\nQuestion:\\nWhy did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?\\n')])}, reference_example_id=None, parent_run_id=UUID('c2ccab39-e740-4341-9c74-19f66d0b7360'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002714617803Z33359ee1-a70c-4c94-907b-8315b182f19c.20240628T002714717999Zc2ccab39-e740-4341-9c74-19f66d0b7360.20240628T002714941682Zec832506-2cb7-49c7-8bee-d01c095cf6e2'), Run(id=UUID('a57476d6-8abd-4d19-a85a-2bf57971f037'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 14, 981952, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 430143, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 14, 981952, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 430143, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl\\', metadata={\\'source\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'loc\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'lastmod\\': \\'2023-07-12T14:58:10.000Z\\'}), Document(page_content=\\'print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\\\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!\\', metadata={\\'source\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'loc\\': \\'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/\\', \\'lastmod\\': \\'2023-07-12T14:58:10.000Z\\'})]\\n\\nQuestion:\\nWhy did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The Neon team added the PGEmbedding extension to LangChain for vector similarity search in Postgres because it offers faster execution time and scalability for LLM applications compared to PGVector. While PGVector provides exact similarity search with 100% accuracy, it can be costly at scale. PGEmbedding, on the other hand, performs 20x faster for 99% accuracy due to its use of the Hierarchical Navigable Small World (HNSW) index graph-based approach.', 'response_metadata': {'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-a57476d6-8abd-4d19-a85a-2bf57971f037-0', 'usage_metadata': {'input_tokens': 1014, 'output_tokens': 98, 'total_tokens': 1112}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 98, 'prompt_tokens': 1014, 'total_tokens': 1112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('c2ccab39-e740-4341-9c74-19f66d0b7360'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002714617803Z33359ee1-a70c-4c94-907b-8315b182f19c.20240628T002714717999Zc2ccab39-e740-4341-9c74-19f66d0b7360.20240628T002714981952Za57476d6-8abd-4d19-a85a-2bf57971f037')], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002714617803Z33359ee1-a70c-4c94-907b-8315b182f19c.20240628T002714717999Zc2ccab39-e740-4341-9c74-19f66d0b7360'), Run(id=UUID('671ae4cc-d99b-47bd-81c5-aec5950e7d7e'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 15, 170984, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 16, 977379, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 15, 170984, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 16, 977379, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})], 'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'output': [Document(page_content='Editor’s Note: This blog post was written in collaboration with the Neon team (Raouf Chebri in particular). The vectorstore space is on fire, and we’re excited to highlight new implementations and options. We’re also really excited by the detailed analysis done here, bringing some solid stats and insights to a novel space.We’re very excited to announce Neon’s collaboration with LangChain to release the pg_embedding extension and PGEmbedding integration in LangChain for vector similarity search in Postgres.But wait. Aren’t they already two other vector stores in LangChain using Postgres and PGVector? Why did the Neon team add another?The short answer is: the Neon team built and added it for faster execution time and scalable LLM applications. PGVector is great, it does exact similarity search by default, which results in 100% accuracy (recall). At scale, however, exact search is costly. Neon found that you can use PGVector with the IVFFlat index to improve query execution time, but that often comes at the cost of accuracy, which increases the chance of hallucination.The Neon team carried out benchmark tests to compare the performance of pgvector and PGEmbedding, and they found out that PGEmbedding performs 20x faster for 99% accuracy.Read the full article to learn more about the benchmark here.Why is PGEmbedding faster?The PGEmbedding integration uses the Hierarchical Navigable Small World (HNSW) index graph-based approach to indexing high-dimensional data. It constructs a hierarchy of graphs, where each layer is a subset of the previous one, which results in a time complexity of O(log(rows)). Search with IVFFlat optimal parameters, however, often has a time complexity of O(sqrt(rows)).How to get started with PGEmbeddingThe first step is to login to your Neon account and create a project:npx neonctl authThe command above will direct you to the sign-up if you do not already have a Neon account.Once logged in, create a project using the following command:npx neonctl', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'}), Document(page_content='print(\"-\" * 80)PGEmbedding vs PGVector: Which vector store should you pick?The Neon team compared both indexes using five criteria:Search speedAccuracyMemory usageIndex construction speedDistance metricsPGVector / SupabaseVectorStorePGEmbeddingSearch SpeedFast, but the search speed depends on the number of clusters examined. More clusters mean higher accuracy but slower search times.Typically faster than IVFFlat, especially in high-dimensional spaces, thanks to its graph-based nature.AccuracyCan achieve high accuracy but at the cost of examining more clusters and hence longer search times.Generally achieves higher accuracy for the same memory footprint compared to IVFFlat.Memory UsageIt uses relatively less memory since it only stores the centroids of clusters and the lists of vectors within these clusters.Generally uses more memory because it maintains a graph structure with multiple layers.Index Construction SpeedIndex building process is relatively fast. The data points are assigned to the nearest centroid, and inverted lists are constructed.\\xa0Index construction involves building multiple layers of graphs, which can be computationally intensive, especially if you choose high values for the parameter ef_constructionDistance MetricsTypically used for L2 distances, but pgvector supports inner product and cosine distance as well.Only uses L2 distance metrics at the moment.ConclusionWith the introduction of the PGEmbedding integration, you now have a powerful new tool at your disposal for your LLM apps. \\xa0PGVector remains a viable choice for applications with stringent memory constraints but at the expense of recall.Ultimately, the choice between PGEmbedding and other vector stores should be informed by the specific demands of your application. We encourage you to experiment with both approaches to find the one that best meets your needs.We are excited to see what you are going to build with PGEmbedding and look forward to your feedback!Happy coding!', metadata={'source': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'loc': 'https://blog.langchain.dev/neon-x-langchainhnsw-in-postgres-with-pg_embedding/', 'lastmod': '2023-07-12T14:58:10.000Z'})]}, reference_example_id=None, parent_run_id=UUID('33359ee1-a70c-4c94-907b-8315b182f19c'), tags=['map:key:context'], child_runs=[], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002714617803Z33359ee1-a70c-4c94-907b-8315b182f19c.20240628T002715170984Z671ae4cc-d99b-47bd-81c5-aec5950e7d7e')], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754.20240628T002714617803Z33359ee1-a70c-4c94-907b-8315b182f19c')], trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75'), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75.20240628T002651040690Z385466dc-44b3-41d8-9626-5e7971df2754')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002650934460Zaaae1772-46c0-450c-b4f9-f995abd05b75', trace_id=UUID('aaae1772-46c0-450c-b4f9-f995abd05b75')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'Why did the Neon team add the PGEmbedding extension to LangChain for vector similarity search in Postgres, despite already having PGVector?'}, outputs={'answer': 'The Neon team added a new vector store implementation using PGEmbedding for faster execution time and scalable LLM applications.'}, metadata={'dataset_split': ['base']}, id=UUID('d596b86f-f16b-48bc-87f1-3e3a88a70898'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 887610, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 887610, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5a189e53-d04f-4159-824c-dbaa12a38f6d'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d2363291-69f0-4eb1-8dfe-ee69b44c9a62'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=9, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c79aa885-35d4-4dd5-8fad-2ec925907c8f'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('52998438-92d4-422a-990b-905ed606923c'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3ed8d989-f146-4d8e-b9a2-d789775501e7'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 201151, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 466340, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:20.978732+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 59.0, 'mem': {'rss': 1286721536.0}, 'cpu': {'time': {'sys': 300.2, 'user': 141.68}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, 'config': None}, outputs={'response': AIMessage(content='The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-04d904f3-280e-41b6-be7e-d30bac4af539-0', usage_metadata={'input_tokens': 1403, 'output_tokens': 81, 'total_tokens': 1484}), 'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=UUID('300b7276-3a72-4f5c-aafd-a4df75ef643c'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('13984e05-87ee-42bf-b894-d08bdf080074'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 532968, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 462202, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 532968, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 462202, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'response': AIMessage(content='The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-04d904f3-280e-41b6-be7e-d30bac4af539-0', usage_metadata={'input_tokens': 1403, 'output_tokens': 81, 'total_tokens': 1484}), 'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=None, parent_run_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), tags=[], child_runs=[Run(id=UUID('4a0873b2-2df4-4277-96e0-8f5378ca9236'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 822528, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 475051, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 822528, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 475051, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, reference_example_id=None, parent_run_id=UUID('13984e05-87ee-42bf-b894-d08bdf080074'), tags=['seq:step:1'], child_runs=[Run(id=UUID('99931be1-dc36-4dd1-b96c-2877a4b2f287'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 219449, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 354197, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 219449, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 354197, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4a0873b2-2df4-4277-96e0-8f5378ca9236'), tags=['map:key:context'], child_runs=[Run(id=UUID('40ad3ec6-48a8-4a3e-867e-7a25dde6e41c'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 434084, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 625926, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 434084, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 625926, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, reference_example_id=None, parent_run_id=UUID('99931be1-dc36-4dd1-b96c-2877a4b2f287'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002651822528Z4a0873b2-2df4-4277-96e0-8f5378ca9236.20240628T002652219449Z99931be1-dc36-4dd1-b96c-2877a4b2f287.20240628T002652434084Z40ad3ec6-48a8-4a3e-867e-7a25dde6e41c'), Run(id=UUID('6c9fbfd5-efb1-4a92-86ce-09bc9680cc7b'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 627827, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 263579, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 627827, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 263579, tzinfo=datetime.timezone.utc)}], inputs={'query': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'documents': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=None, parent_run_id=UUID('99931be1-dc36-4dd1-b96c-2877a4b2f287'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002651822528Z4a0873b2-2df4-4277-96e0-8f5378ca9236.20240628T002652219449Z99931be1-dc36-4dd1-b96c-2877a4b2f287.20240628T002652627827Z6c9fbfd5-efb1-4a92-86ce-09bc9680cc7b')], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002651822528Z4a0873b2-2df4-4277-96e0-8f5378ca9236.20240628T002652219449Z99931be1-dc36-4dd1-b96c-2877a4b2f287'), Run(id=UUID('97b5efcc-8457-45f4-a31b-7289a237035f'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 426588, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 673767, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 426588, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 673767, tzinfo=datetime.timezone.utc)}], inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, reference_example_id=None, parent_run_id=UUID('4a0873b2-2df4-4277-96e0-8f5378ca9236'), tags=['map:key:question'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002651822528Z4a0873b2-2df4-4277-96e0-8f5378ca9236.20240628T002652426588Z97b5efcc-8457-45f4-a31b-7289a237035f')], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002651822528Z4a0873b2-2df4-4277-96e0-8f5378ca9236'), Run(id=UUID('4c86bb8b-bfd2-4e20-a226-e86b416172d3'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 597916, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 597916, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs=None, reference_example_id=None, parent_run_id=UUID('13984e05-87ee-42bf-b894-d08bdf080074'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002720597916Z4c86bb8b-bfd2-4e20-a226-e86b416172d3'), Run(id=UUID('9b61a861-7438-4c80-99d8-828d17354aa2'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 55427, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 460601, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 55427, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 460601, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'response': AIMessage(content='The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-04d904f3-280e-41b6-be7e-d30bac4af539-0', usage_metadata={'input_tokens': 1403, 'output_tokens': 81, 'total_tokens': 1484}), 'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=None, parent_run_id=UUID('13984e05-87ee-42bf-b894-d08bdf080074'), tags=['seq:step:3'], child_runs=[Run(id=UUID('bfda06de-3924-43d0-b74e-9fbd0bdfa869'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 446603, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 458376, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 446603, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 458376, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': AIMessage(content='The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-04d904f3-280e-41b6-be7e-d30bac4af539-0', usage_metadata={'input_tokens': 1403, 'output_tokens': 81, 'total_tokens': 1484})}, reference_example_id=None, parent_run_id=UUID('9b61a861-7438-4c80-99d8-828d17354aa2'), tags=['map:key:response'], child_runs=[Run(id=UUID('a1c1fdc4-2bb0-4a09-99a3-71943ee969ee'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 889036, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 222968, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 889036, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 222968, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'lastmod\\': \\'2023-10-23T18:43:28.000Z\\'}), Document(page_content=\\'a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered\\', metadata={\\'source\\': \\'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/\\', \\'loc\\': \\'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/\\', \\'lastmod\\': \\'2023-06-24T21:34:28.000Z\\'}), Document(page_content=\\'Editor\\\\\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\\\\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\\\\\'s thought process is broken down into multiple Brains appropriate\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'lastmod\\': \\'2023-10-23T18:43:28.000Z\\'})]\\n\\nQuestion:\\nExplain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.\\n')])}, reference_example_id=None, parent_run_id=UUID('bfda06de-3924-43d0-b74e-9fbd0bdfa869'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002722055427Z9b61a861-7438-4c80-99d8-828d17354aa2.20240628T002722446603Zbfda06de-3924-43d0-b74e-9fbd0bdfa869.20240628T002722889036Za1c1fdc4-2bb0-4a09-99a3-71943ee969ee'), Run(id=UUID('04d904f3-280e-41b6-be7e-d30bac4af539'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 305895, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 450622, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 305895, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 450622, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\\'...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'lastmod\\': \\'2023-10-23T18:43:28.000Z\\'}), Document(page_content=\\'a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered\\', metadata={\\'source\\': \\'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/\\', \\'loc\\': \\'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/\\', \\'lastmod\\': \\'2023-06-24T21:34:28.000Z\\'}), Document(page_content=\\'Editor\\\\\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\\\\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\\\\\'s thought process is broken down into multiple Brains appropriate\\', metadata={\\'source\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'loc\\': \\'https://blog.langchain.dev/exploring-genworlds/\\', \\'lastmod\\': \\'2023-10-23T18:43:28.000Z\\'})]\\n\\nQuestion:\\nExplain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The \"Simulation Socket\" in GenWorlds is a websocket server that serves as the communication backbone, enabling parallel operation of the World, Agents, and Objects. It allows these entities to communicate by sending events, supporting the connection of frontends or other services to the World, running Agents on external servers, and more. This architecture facilitates the interaction and coordination between different components within the GenWorlds framework.', 'response_metadata': {'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-04d904f3-280e-41b6-be7e-d30bac4af539-0', 'usage_metadata': {'input_tokens': 1403, 'output_tokens': 81, 'total_tokens': 1484}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1403, 'total_tokens': 1484}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('bfda06de-3924-43d0-b74e-9fbd0bdfa869'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002722055427Z9b61a861-7438-4c80-99d8-828d17354aa2.20240628T002722446603Zbfda06de-3924-43d0-b74e-9fbd0bdfa869.20240628T002723305895Z04d904f3-280e-41b6-be7e-d30bac4af539')], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002722055427Z9b61a861-7438-4c80-99d8-828d17354aa2.20240628T002722446603Zbfda06de-3924-43d0-b74e-9fbd0bdfa869'), Run(id=UUID('09393645-5f27-4304-bb6f-65806daa2502'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 596598, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 996904, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 596598, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 996904, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})], 'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'output': [Document(page_content='...Simulation SocketThe “Simulation Socket” is a websocket server which serves as the communication backbone. It enables parallel operation of the World, Agents, and Objects, and allows them to communicate by sending events. This architecture supports connecting frontends or other services to the World, running Agents on external servers, and much more.AgentsAgents in GenWorlds are autonomous entities built on top of LangChain that perceive their surroundings and make decisions to achieve specific goals as set by the user. The Agents interact with their environment by sending events through the Simulation Socket server. They dynamically learn about the World and the Objects around them, figuring out how to utilize these Objects to achieve their goals.There can be many different types of Agents, as long as each one of them understands the event protocol used to communicate with the World.The Agent’s Mental ModelAgents follow a specific mental model at each step of their interaction with the World:Review World state: The Agent assesses the environment to understand the context before planning any actions.Review new events: The Agent evaluates any new occurrences in the World. These could be actions taken by other Agents or changes in the World state due to Object interactions.Remember: Using its stored memories and awareness of previous state changes, the Agent recalls past experiences and data that might affect its current decision-making process.Create a plan: The Agent creates and updates action plans based on World state changes. An Agent can execute multiple actions in one step, improving overall efficiency.Execution:  Agent implements its plan, influencing the World state and potentially triggering responses from other Agents.This interactive process fosters the emergence of complex, autonomous behavior, making each Agent an active participant in the World.The Thinking ProcessThe think() method in the code is the central function where an Agent’s thinking', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'}), Document(page_content='a high level, the architecture of the simulation looks like this. The actual code contains a lot more nuance and additional helper classes, but this is the rough sketch of how things work together.The World class serves the the top level wrapper for everything else. When we run the simulation, we are running world.run( ), which triggers each agent in the world to begin its agent loop.The Agent LoopThe agent loop is the main driver of activity within GPTeam. As the world runs, each agent repeats this loop again and again until the world stops. You can view the code for the agent loop here.As we dive into how the agent loop works, it’s helpful to understand that there is no discrete agentic AI to be found in this repository. The appearence of an agentic human-like entity is an illusion created by our memories system and a string of distinct Language Model prompts.Agent.observeAn agent starts their loop by first observing activity in its location. This function, observe( ), gets the latest events from the agent’s current location and adds each one to the agents memory. When a new memory is created, it’s assigned an importance score, which aims to quantify the poignancy of the memory. This allows more critical events to be “remembered” more easily later on. We assign the importance score by simply asking an LLM to generate one:FYI: You can view all of our LLM prompts in this file.Agent.planNext, the agent makes plans if they don’t have any, although they usually do, since they make 5 plans at a time. Every action an agent takes must be part of some plan, so planning is critical. To make plans, we take the agent’s personal details and situational context, then pass them into an LLM call with the prompt shown below.An important part of this prompt is the agent’s directives, which act as their compass when deciding what to do. Without them, they can only be reactionary. You can think of their directives as their default activity.The result from this LLM call is an ordered', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'}), Document(page_content='Editor\\'s Note: This is another edition of our series of guest posts highlighting the powerful applications of LangChain. We have been working with the Yeager.ai team for several months now, and they have built some really impressive applications of LangChain agents. We\\'re excited to highlight this guest blog on their GenWorlds framework for multi-agent systems.  We are especially excited to see their plan for making it seamless for the LangChain/GenWorlds community to monetize their projects.🧬🌍GenWorlds is an open-source development framework for multi-agent systems. Core to the framework are dedicated environments or \"Worlds\" with specialized AI Agents and shared Objects. In these Worlds, Agents use shared Objects to coordinate to execute complex tasks and achieve common goals.  Our research has shown that Agents perform better with narrow focus, so Agent coordination is essential for GenAI systems to perform complex operations.  Developers Can Easily MonetizeYeager.ai is building a robust ecosystem for GenAI applications. The framework is designed with composability and interoperability in mind. In addition to the framework, GenWorlds will offer a platform for hosting GenAI applications with minimal maintenance and a marketplace where developers can easily monetize their builds.The Power of ModularityModularity is a cornerstone of the GenWorlds framework. In software engineering, breaking things down into smaller self-contained components can improve reusability and reliability of systems. The same is true for systems using LLMs - the more specific and narrow each call to the language model, the higher the reliability of the output. The GenWorlds framework applies this principle at multiple levels:Instead of a single agent trying to do everything, GenWorlds allows you to create multiple agents, each with a much narrower mission, that can work together towards a common goalFurthermore, each Agent\\'s thought process is broken down into multiple Brains appropriate', metadata={'source': 'https://blog.langchain.dev/exploring-genworlds/', 'loc': 'https://blog.langchain.dev/exploring-genworlds/', 'lastmod': '2023-10-23T18:43:28.000Z'})]}, reference_example_id=None, parent_run_id=UUID('9b61a861-7438-4c80-99d8-828d17354aa2'), tags=['map:key:context'], child_runs=[], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002722055427Z9b61a861-7438-4c80-99d8-828d17354aa2.20240628T002722596598Z09393645-5f27-4304-bb6f-65806daa2502')], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074.20240628T002722055427Z9b61a861-7438-4c80-99d8-828d17354aa2')], trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111'), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111.20240628T002651532968Z13984e05-87ee-42bf-b894-d08bdf080074')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651201151Z36ebb055-9a06-4595-a7b4-8ec718fe2111', trace_id=UUID('36ebb055-9a06-4595-a7b4-8ec718fe2111')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'Explain the function and purpose of the \"Simulation Socket\" in the context of GenWorlds.'}, outputs={'answer': \"The think() method is the central function where an Agent’s thinking process is carried out. The function first acquires the initial state of the World and potential actions to be performed. It then enters a loop, where it processes events and evaluates entities in the Agent's proximity to inform its decision-making. Depending on the current state and goals of the Agent, the think() function may choose to wait, respond to user input, or interact with entities. If the Agent selects an action, it executes it and updates its memory accordingly. The think() function continually updates the Agent's state in the World and repeats the process until it decides to exit.\"}, metadata={'dataset_split': ['base']}, id=UUID('300b7276-3a72-4f5c-aafd-a4df75ef643c'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 978732, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 20, 978732, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('def4e2e4-ffe5-4f45-ad95-68981c751e25'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d1ec9a2f-2dd2-4678-88d2-a0caf487b4d7'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ab838755-0ccc-4b6c-9cca-047a3ac484e8'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6fd7c771-d6dc-4cf4-8e98-07d99bb39de0'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('63a3ca49-9c2c-4e4a-b17f-3ad128c48b59'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 810070, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 698994, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:19.833340+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 51.0, 'mem': {'rss': 1279164416.0}, 'cpu': {'time': {'sys': 300.3, 'user': 142.08}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, 'config': None}, outputs={'response': AIMessage(content='The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb-0', usage_metadata={'input_tokens': 1920, 'output_tokens': 30, 'total_tokens': 1950}), 'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=UUID('b9c0afc2-271e-4ce3-b0fb-864bbbd39fc7'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('4c2b00f9-f081-4934-a7bd-2b02bda8416f'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 137894, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 697077, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 137894, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 697077, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'response': AIMessage(content='The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb-0', usage_metadata={'input_tokens': 1920, 'output_tokens': 30, 'total_tokens': 1950}), 'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=None, parent_run_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), tags=[], child_runs=[Run(id=UUID('b10a5ee7-5bee-4fd9-a30e-aeb873cac19b'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 427991, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 680129, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 427991, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 680129, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, reference_example_id=None, parent_run_id=UUID('4c2b00f9-f081-4934-a7bd-2b02bda8416f'), tags=['seq:step:1'], child_runs=[Run(id=UUID('d071df12-c010-4292-b38c-d57cadcaa410'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 797004, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 478264, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 797004, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 478264, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=None, parent_run_id=UUID('b10a5ee7-5bee-4fd9-a30e-aeb873cac19b'), tags=['map:key:context'], child_runs=[Run(id=UUID('70d794d2-8681-4f70-aaf8-d223d7f5d423'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 104308, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 451302, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 104308, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 451302, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, reference_example_id=None, parent_run_id=UUID('d071df12-c010-4292-b38c-d57cadcaa410'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002652427991Zb10a5ee7-5bee-4fd9-a30e-aeb873cac19b.20240628T002652797004Zd071df12-c010-4292-b38c-d57cadcaa410.20240628T002653104308Z70d794d2-8681-4f70-aaf8-d223d7f5d423'), Run(id=UUID('5f7dc90a-0f88-46fe-85cc-7eff9cb533c6'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 452576, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 388234, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 452576, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 388234, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'documents': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=None, parent_run_id=UUID('d071df12-c010-4292-b38c-d57cadcaa410'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002652427991Zb10a5ee7-5bee-4fd9-a30e-aeb873cac19b.20240628T002652797004Zd071df12-c010-4292-b38c-d57cadcaa410.20240628T002653452576Z5f7dc90a-0f88-46fe-85cc-7eff9cb533c6')], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002652427991Zb10a5ee7-5bee-4fd9-a30e-aeb873cac19b.20240628T002652797004Zd071df12-c010-4292-b38c-d57cadcaa410'), Run(id=UUID('daea0187-0a48-4f6f-975b-bd76e7cadd15'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 275477, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 53, 500031, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 275477, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 53, 500031, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, reference_example_id=None, parent_run_id=UUID('b10a5ee7-5bee-4fd9-a30e-aeb873cac19b'), tags=['map:key:question'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002652427991Zb10a5ee7-5bee-4fd9-a30e-aeb873cac19b.20240628T002653275477Zdaea0187-0a48-4f6f-975b-bd76e7cadd15')], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002652427991Zb10a5ee7-5bee-4fd9-a30e-aeb873cac19b'), Run(id=UUID('bd604d68-ae84-4935-a31f-2221a6d2db5a'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 898296, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 898296, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('4c2b00f9-f081-4934-a7bd-2b02bda8416f'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002720898296Zbd604d68-ae84-4935-a31f-2221a6d2db5a'), Run(id=UUID('5a4c51db-d39d-41a3-a05a-1447ef593425'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 449029, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 695437, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 449029, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 695437, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'response': AIMessage(content='The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb-0', usage_metadata={'input_tokens': 1920, 'output_tokens': 30, 'total_tokens': 1950}), 'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=None, parent_run_id=UUID('4c2b00f9-f081-4934-a7bd-2b02bda8416f'), tags=['seq:step:3'], child_runs=[Run(id=UUID('aa93e9c5-86ec-4f73-9843-668ceec93853'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 216096, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 693287, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 216096, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 693287, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': AIMessage(content='The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb-0', usage_metadata={'input_tokens': 1920, 'output_tokens': 30, 'total_tokens': 1950})}, reference_example_id=None, parent_run_id=UUID('5a4c51db-d39d-41a3-a05a-1447ef593425'), tags=['map:key:response'], child_runs=[Run(id=UUID('4f421536-0353-4429-90e0-040f52534a00'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 468540, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 740913, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 468540, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 740913, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We\\'re not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we\\'ve built (embeddings, vectorstores) are aimed at facilitating this process.But we\\'ve noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user\\'s interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we\\'ve:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That\\'s the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={\\'source\\': \\'https://blog.langchain.dev/retrieval/\\', \\'loc\\': \\'https://blog.langchain.dev/retrieval/\\', \\'lastmod\\': \\'2023-08-18T22:03:20.000Z\\'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There\\'s a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\\\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={\\'source\\': \\'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/\\', \\'loc\\': \\'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/\\', \\'lastmod\\': \\'2023-10-03T14:35:38.000Z\\'}), Document(page_content=\\'building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\\\\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\\\\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\\\\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\\\\\'s worth looking something up or not. Another way to do\\', metadata={\\'source\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'loc\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'lastmod\\': \\'2023-10-15T19:19:25.000Z\\'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we\\'ve implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user\\'s questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-state-of-ai-2023/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-state-of-ai-2023/\\', \\'lastmod\\': \\'2023-12-21T19:21:35.000Z\\'})]\\n\\nQuestion:\\nWhat is the two-step process described in the context that increases the accuracy and relevance of search results?\\n')])}, reference_example_id=None, parent_run_id=UUID('aa93e9c5-86ec-4f73-9843-668ceec93853'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002722449029Z5a4c51db-d39d-41a3-a05a-1447ef593425.20240628T002723216096Zaa93e9c5-86ec-4f73-9843-668ceec93853.20240628T002723468540Z4f421536-0353-4429-90e0-040f52534a00'), Run(id=UUID('42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 754684, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 691984, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 754684, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 691984, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We\\'re not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we\\'ve built (embeddings, vectorstores) are aimed at facilitating this process.But we\\'ve noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user\\'s interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we\\'ve:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That\\'s the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={\\'source\\': \\'https://blog.langchain.dev/retrieval/\\', \\'loc\\': \\'https://blog.langchain.dev/retrieval/\\', \\'lastmod\\': \\'2023-08-18T22:03:20.000Z\\'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There\\'s a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\\\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={\\'source\\': \\'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/\\', \\'loc\\': \\'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/\\', \\'lastmod\\': \\'2023-10-03T14:35:38.000Z\\'}), Document(page_content=\\'building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\\\\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\\\\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\\\\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\\\\\'s worth looking something up or not. Another way to do\\', metadata={\\'source\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'loc\\': \\'https://blog.langchain.dev/weblangchain/\\', \\'lastmod\\': \\'2023-10-15T19:19:25.000Z\\'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we\\'ve implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user\\'s questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={\\'source\\': \\'https://blog.langchain.dev/langchain-state-of-ai-2023/\\', \\'loc\\': \\'https://blog.langchain.dev/langchain-state-of-ai-2023/\\', \\'lastmod\\': \\'2023-12-21T19:21:35.000Z\\'})]\\n\\nQuestion:\\nWhat is the two-step process described in the context that increases the accuracy and relevance of search results?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The two-step process described in the context that increases the accuracy and relevance of search results is \"Retrieval Augmented Generation\" (RAG).', 'response_metadata': {'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb-0', 'usage_metadata': {'input_tokens': 1920, 'output_tokens': 30, 'total_tokens': 1950}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 30, 'prompt_tokens': 1920, 'total_tokens': 1950}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('aa93e9c5-86ec-4f73-9843-668ceec93853'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002722449029Z5a4c51db-d39d-41a3-a05a-1447ef593425.20240628T002723216096Zaa93e9c5-86ec-4f73-9843-668ceec93853.20240628T002723754684Z42bdbf2e-7333-44eb-85ed-ad65dd7bfdbb')], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002722449029Z5a4c51db-d39d-41a3-a05a-1447ef593425.20240628T002723216096Zaa93e9c5-86ec-4f73-9843-668ceec93853'), Run(id=UUID('13c3f69d-acd6-413a-99c4-5632d70945f1'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 293952, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 561721, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 293952, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 561721, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})], 'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'output': [Document(page_content=\"and the documents retrieved are those who are closest to the query in embedding space. We're not going to go into too much detail on that here - but here is a more in depth tutorial on the topic, and below is a diagram which nicely summarizes this.Diagram of typical retrieval stepProblemsThis process works pretty well, and a lot of the components and abstractions we've built (embeddings, vectorstores) are aimed at facilitating this process.But we've noticed two problems.First: there a lot of different variations in how you do this retrieval step. People want to do things beyond semantic search. To be concrete:We support two different query methods: one that just optimizes similarity, another with optimizes for maximal marginal relevance.Users often want to specify metadata filters to filter results before doing semantic searchOther types of indexes, like graphs, have piqued user's interestsSecond: we also realized that people may construct a retriever outside of LangChain - for example OpenAI released their ChatGPT Retrieval Plugin. We want to make it as easy as possible for people to use whatever retriever they created within LangChain.We realized we made a mistake - by making our abstractions centered around VectorDBQA we were limiting to use of our chains, making them hard to use (1) for users who wanted to experiment with other retrieval methods, (2) for users who created a retriever outside the LangChain ecosystem.SolutionSo how did we fix this?In our most recent Python and TypeScript releases, we've:Introduced the concept of a Retriever. Retrievers are expected to expose a get_relevant_documents method with the following signature: def get_relevant_documents(self, query: str) -> List[Document]. That's the only assumption we make about Retrievers. See more about this interface below.Changed all our chains that used VectorDBs to now use Retrievers. VectorDBQA is now RetrievalQA, ChatVectorDBChain is now ConversationalRetrievalChain, etc. Note that, moving\", metadata={'source': 'https://blog.langchain.dev/retrieval/', 'loc': 'https://blog.langchain.dev/retrieval/', 'lastmod': '2023-08-18T22:03:20.000Z'}), Document(page_content=\"been easy to set up, does the job, and has reliable support. It enabled us to chunk and embed over 20 million embeddings in under 15 minutes.Finally, we push the generated embeddings into a vector database. We use HNSW for vector indexing and BM25 for text indexing, which is fairly standard for these data types. One notable observation was that several vector stores perform simultaneous write and indexing operations, resulting in a significant decrease in write throughput. To address this issue, we highly recommend transitioning to incremental indexing.Optimizing RetrievalRetrieval goes beyond making a default call to your vector store. There's a lot we can do to decrease the candidate set and rank chunks better. In this component, we aim to make the retrieval process as accurate and efficient as possible.First, we build our own Query Intent Classifier (QIC) powered by LLMs. Given a query, this extracts all the entities that can either help us filter chunks on our metadata or get us some new information that validates hybrid search. This is where we force the LLM for structured output (s/o Instructor). \\xa0We also run sanity checks on the final response with string matching libraries to not miss out on entities that might exist in our metadata, for example. nike and nike inc. Turns out normalization itself for each metadata is a massive internal effort to do right.In addition to the classifier, we have also begun generating hallucinated search queries to conduct retrieval over, and to vote on the most relevant answers. While this has latency costs, this is an ongoing experiment to see how this improves accuracy.After obtaining a collection of search queries along with relevant filters, we initiate the retrieval process by narrowing down potential candidates using these filters. Next, we dynamically select between vector search and hybrid search, depending on the presence of any significant entities identified by QIC that warrant a keyword search. For vector search, we\", metadata={'source': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'loc': 'https://blog.langchain.dev/kay-x-cybersyn-x-langchain/', 'lastmod': '2023-10-03T14:35:38.000Z'}), Document(page_content='building LLM applications of your own.Retrieval Augmented GenerationUnder the hood, these web research tools use a technique known as \"Retrieval Augmented Generation\" (often called RAG). See this article for a good deep dive on the topic. A high level description of RAG involves two steps:Retrieval: retrieve some informationAugmented Generation: generate a response to the question using the retrieved informationWhile these two steps may seem simple, there is actually a good amount of complexity that goes into these steps.RetrievalThe first thing these web researchers do is look up things from the internet. Although this may seem simple, there\\'s actually MANY interesting decisions to be made here. These decisions are not specific to internet search applications - they are ones that all creators of RAG applications need to make (whether they realize it or not).Do we ALWAYS look something up?Do we look up the raw user search query or a derived one?What do we do for follow up questions?Do we look up multiple search terms or just one?Can we look things up multiple times?There are also some decisions that are more specific to web research in general. We will spend less time here because these are less generalizable.What search engine should we use?How do we get information from that search engine?Do we ALWAYS look something up?One decision you\\'ll have to make in your RAG application is whether you always want to look something up. Why would you NOT want to always look something up? You may not want to always look something up if you are intending your application to be more of a general purpose chat bot. In this situation, if users are interacting with your application and saying \"Hi\" you don\\'t need to do any retrieval, and doing so is just a waste of time and tokens. You could implement this logic of whether to look things up in a few ways. First, you could have a simple classification layer to classify whether it\\'s worth looking something up or not. Another way to do', metadata={'source': 'https://blog.langchain.dev/weblangchain/', 'loc': 'https://blog.langchain.dev/weblangchain/', 'lastmod': '2023-10-15T19:19:25.000Z'}), Document(page_content=\"for pieces of text. So how are developers doing that?Similar to LLMs, OpenAI reigns supreme - but we see more diversity after that. Open source providers are more used, with Hugging Face coming in 2nd most used, and GPT4All and Ollama also in the top 8. On the hosted side, we see that Vertex AI actually beats out AzureOpenAI, and Cohere and Amazon Bedrock are not far behind.Top Advanced Retrieval StrategiesJust doing cosine similarity between embeddings only gets you so far in retrieval. We see a lot of people relying on advanced retrieval strategies - a lot of which we've implemented and documented in LangChain.Even still - the most common retrieval strategy we see is not a built-in one but rather a custom one. This speaks to:The ease of implementing a custom retrieval strategy in LangChainThe need to implement custom logic in order to achieve the best performanceAfter that, we see more familiar names popping up:Self Query - which extracts metadata filters from user's questionsHybrid Search - mainly through provider specific integrations like Supabase and PineconeContextual Compression - which is postprocessing of base retrieval resultsMulti Query - transforming a single query into multiple, and then retrieving results for allTimeWeighted VectorStore - give more preference to recent documentsHow are people testing?Evaluation and testing has emerged as one of the largest pain points developers run into when building LLM applications, and LangSmith has emerged as one of the best ways to do this.We see that most users are able to formulate some metrics to evaluate their LLM apps - 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.Of the feedback logged, the majority of them use an LLM to evaluate the outputs. While some have\", metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z'})]}, reference_example_id=None, parent_run_id=UUID('5a4c51db-d39d-41a3-a05a-1447ef593425'), tags=['map:key:context'], child_runs=[], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002722449029Z5a4c51db-d39d-41a3-a05a-1447ef593425.20240628T002723293952Z13c3f69d-acd6-413a-99c4-5632d70945f1')], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f.20240628T002722449029Z5a4c51db-d39d-41a3-a05a-1447ef593425')], trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39'), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39.20240628T002652137894Z4c2b00f9-f081-4934-a7bd-2b02bda8416f')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002651810070Zbe369e4b-54a4-4c28-819b-c78867ba5e39', trace_id=UUID('be369e4b-54a4-4c28-819b-c78867ba5e39')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the two-step process described in the context that increases the accuracy and relevance of search results?'}, outputs={'answer': 'Metadata filtering using the pre-filtering approach is important because it allows users to narrow their search results according to specific attributes, such as dates or categories, before applying vector similarity search. This increases the accuracy and relevance of the search results by first filtering the documents based on structured criteria and then finding the most contextually relevant documents within this filtered subset.'}, metadata={'dataset_split': ['base']}, id=UUID('b9c0afc2-271e-4ce3-b0fb-864bbbd39fc7'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 833340, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 19, 833340, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c7032cb0-f918-45fa-b731-1ce484a9e0a7'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('adfc603c-226e-4d8f-a821-61ddb9381fa0'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('dbdce1f1-51ba-446b-853b-5a2cf7d5d998'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d9d6e630-6df6-4563-a1d9-03c490c1cad9'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3bcee40b-3fcc-4cec-95be-a4dc900be556'), target_run_id=None)]}},\n",
              " {'run': RunTree(id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), name='Target', start_time=datetime.datetime(2024, 6, 28, 0, 26, 50, 996270, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 617984, tzinfo=datetime.timezone.utc), extra={'metadata': {'revision_id': 'a61a7b1-dirty', 'num_repetitions': 1, 'example_version': '2024-06-28T00:09:21.769249+00:00', 'ls_method': 'traceable'}, 'runtime': {'sdk': 'langsmith-py', 'sdk_version': '0.1.82', 'library': 'langsmith', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31', 'runtime': 'python', 'py_implementation': 'CPython', 'runtime_version': '3.10.14', 'langchain_version': '0.2.6', 'langchain_core_version': '0.2.10', 'thread_count': 51.0, 'mem': {'rss': 1279164416.0}, 'cpu': {'time': {'sys': 300.3, 'user': 142.08}, 'ctx_switches': {'voluntary': 10593.0, 'involuntary': 281.0}, 'percent': 0.0}}}, error=None, serialized={'name': 'Target', 'signature': \"(input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Output'\", 'doc': None}, events=[], inputs={'input': {'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, 'config': None}, outputs={'response': AIMessage(content='The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b86400a6-9fa1-4061-8916-f0650695cd92-0', usage_metadata={'input_tokens': 1464, 'output_tokens': 79, 'total_tokens': 1543}), 'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=UUID('e64ebb5a-cc9d-43ff-a0cd-86220b01a819'), parent_run_id=None, tags=[], parent_run=None, child_runs=[Run(id=UUID('81594625-4738-4743-9a51-9f5d80f800e3'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 252981, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 616711, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, 'middle': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}], 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 5, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 6, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 7, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 8, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}, {'id': 9, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 10, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 11, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 12, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 13, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}, {'source': 5, 'target': 7}, {'source': 7, 'target': 6}, {'source': 5, 'target': 8}, {'source': 8, 'target': 6}, {'source': 1, 'target': 5}, {'source': 11, 'target': 12}, {'source': 9, 'target': 11}, {'source': 12, 'target': 10}, {'source': 9, 'target': 13}, {'source': 13, 'target': 10}, {'source': 6, 'target': 9}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 252981, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 616711, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'response': AIMessage(content='The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b86400a6-9fa1-4061-8916-f0650695cd92-0', usage_metadata={'input_tokens': 1464, 'output_tokens': 79, 'total_tokens': 1543}), 'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), tags=[], child_runs=[Run(id=UUID('b212ca2e-b7a9-49b2-8505-f4d4164a64b0'), name='RunnableParallel<context,question>', start_time=datetime.datetime(2024, 6, 28, 0, 26, 51, 608404, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 558330, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'question': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}}}, 'name': 'RunnableParallel<context,question>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context,question>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context,question>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 51, 608404, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 558330, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, reference_example_id=None, parent_run_id=UUID('81594625-4738-4743-9a51-9f5d80f800e3'), tags=['seq:step:1'], child_runs=[Run(id=UUID('67b76fe5-4b09-4820-8df8-3f94072c2faf'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 66933, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 467244, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, 'last': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'LambdaInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 3, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 66933, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 467244, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('b212ca2e-b7a9-49b2-8505-f4d4164a64b0'), tags=['map:key:context'], child_runs=[Run(id=UUID('8e0a9315-2f7a-443d-bae6-b9a2bedb9e4a'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 276348, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 353713, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 276348, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 353713, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, reference_example_id=None, parent_run_id=UUID('67b76fe5-4b09-4820-8df8-3f94072c2faf'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002651608404Zb212ca2e-b7a9-49b2-8505-f4d4164a64b0.20240628T002652066933Z67b76fe5-4b09-4820-8df8-3f94072c2faf.20240628T002652276348Z8e0a9315-2f7a-443d-bae6-b9a2bedb9e4a'), Run(id=UUID('7c30287b-cc77-47a1-b5d8-6408145766bd'), name='Retriever', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 355979, tzinfo=datetime.timezone.utc), run_type='retriever', end_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 383704, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'repr': 'ParentDocumentRetriever(vectorstore=<langchain_qdrant.vectorstores.Qdrant object at 0x7f59e8bf7f10>, docstore=<langchain_core.stores.InMemoryStore object at 0x7f59ebe6e5c0>, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6ed10>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x7f59ebe6cac0>)', 'name': 'ParentDocumentRetriever', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ParentDocumentRetrieverInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'retrievers', 'parent_document_retriever', 'ParentDocumentRetriever'], 'name': 'ParentDocumentRetriever'}}, {'id': 2, 'type': 'schema', 'data': 'ParentDocumentRetrieverOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 355979, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 383704, tzinfo=datetime.timezone.utc)}], inputs={'query': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'documents': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('67b76fe5-4b09-4820-8df8-3f94072c2faf'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002651608404Zb212ca2e-b7a9-49b2-8505-f4d4164a64b0.20240628T002652066933Z67b76fe5-4b09-4820-8df8-3f94072c2faf.20240628T002652355979Z7c30287b-cc77-47a1-b5d8-6408145766bd')], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002651608404Zb212ca2e-b7a9-49b2-8505-f4d4164a64b0.20240628T002652066933Z67b76fe5-4b09-4820-8df8-3f94072c2faf'), Run(id=UUID('1aedf9d6-e50d-4d1c-8ad2-07c563c73cfc'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 188125, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 26, 52, 431010, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('question'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 188125, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 26, 52, 431010, tzinfo=datetime.timezone.utc)}], inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, reference_example_id=None, parent_run_id=UUID('b212ca2e-b7a9-49b2-8505-f4d4164a64b0'), tags=['map:key:question'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002651608404Zb212ca2e-b7a9-49b2-8505-f4d4164a64b0.20240628T002652188125Z1aedf9d6-e50d-4d1c-8ad2-07c563c73cfc')], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002651608404Zb212ca2e-b7a9-49b2-8505-f4d4164a64b0'), Run(id=UUID('fd26682f-9b90-4c43-b6f5-bce0a5068a6c'), name='RunnableAssign<context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 20, 826211, tzinfo=datetime.timezone.utc), run_type='chain', end_time=None, extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableAssign'], 'kwargs': {'mapper': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}]}}}, 'name': 'RunnableAssign<context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'schema', 'runnable', 'RunnablePassthrough'], 'name': 'RunnablePassthrough'}}], 'edges': [{'source': 0, 'target': 2}, {'source': 2, 'target': 1}, {'source': 0, 'target': 3}, {'source': 3, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 20, 826211, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs=None, reference_example_id=None, parent_run_id=UUID('81594625-4738-4743-9a51-9f5d80f800e3'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002720826211Zfd26682f-9b90-4c43-b6f5-bce0a5068a6c'), Run(id=UUID('0d22cbdb-a5ac-4837-8b98-e500a3656619'), name='RunnableParallel<response,context>', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 90949, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 615875, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableParallel'], 'kwargs': {'steps__': {'response': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, 'context': {'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}}}, 'name': 'RunnableParallel<response,context>', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'Parallel<response,context>Input'}, {'id': 1, 'type': 'schema', 'data': 'Parallel<response,context>Output'}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 3, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 4, 'type': 'runnable', 'data': {'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'name': 'RunnableLambda'}}], 'edges': [{'source': 2, 'target': 3}, {'source': 0, 'target': 2}, {'source': 3, 'target': 1}, {'source': 0, 'target': 4}, {'source': 4, 'target': 1}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 90949, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 615875, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'response': AIMessage(content='The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b86400a6-9fa1-4061-8916-f0650695cd92-0', usage_metadata={'input_tokens': 1464, 'output_tokens': 79, 'total_tokens': 1543}), 'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('81594625-4738-4743-9a51-9f5d80f800e3'), tags=['seq:step:3'], child_runs=[Run(id=UUID('0ebe026f-508d-47d7-bffd-c7d39e349905'), name='RunnableSequence', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 517310, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 614327, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'], 'kwargs': {'first': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, 'last': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}, 'name': 'RunnableSequence', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 3, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 2, 'target': 3}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 517310, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 614327, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': AIMessage(content='The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b86400a6-9fa1-4061-8916-f0650695cd92-0', usage_metadata={'input_tokens': 1464, 'output_tokens': 79, 'total_tokens': 1543})}, reference_example_id=None, parent_run_id=UUID('0d22cbdb-a5ac-4837-8b98-e500a3656619'), tags=['map:key:response'], child_runs=[Run(id=UUID('1e7be736-b8d2-4019-9bb3-b57742cc1ea6'), name='ChatPromptTemplate', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 859720, tzinfo=datetime.timezone.utc), run_type='prompt', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 334018, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'template': \"Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\", 'template_format': 'f-string'}, 'name': 'PromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'name': 'PromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'PromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}}}]}, 'name': 'ChatPromptTemplate', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'PromptInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'name': 'ChatPromptTemplate'}}, {'id': 2, 'type': 'schema', 'data': 'ChatPromptTemplateOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 859720, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 334018, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': ChatPromptValue(messages=[HumanMessage(content='Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Editor\\'s Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We\\'re really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run\", metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'}), Document(page_content=\\'as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\\\\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\\\\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\\\\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for\\', metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'})]\\n\\nQuestion:\\nWhat is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?\\n')])}, reference_example_id=None, parent_run_id=UUID('0ebe026f-508d-47d7-bffd-c7d39e349905'), tags=['seq:step:1'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002722090949Z0d22cbdb-a5ac-4837-8b98-e500a3656619.20240628T002722517310Z0ebe026f-508d-47d7-bffd-c7d39e349905.20240628T002722859720Z1e7be736-b8d2-4019-9bb3-b57742cc1ea6'), Run(id=UUID('b86400a6-9fa1-4061-8916-f0650695cd92'), name='ChatOpenAI', start_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 405110, tzinfo=datetime.timezone.utc), run_type='llm', end_time=datetime.datetime(2024, 6, 28, 0, 27, 25, 613610, tzinfo=datetime.timezone.utc), extra={'invocation_params': {'model': 'gpt-3.5-turbo', 'model_name': 'gpt-3.5-turbo', 'stream': False, 'n': 1, 'temperature': 0.0, '_type': 'openai-chat', 'stop': None}, 'options': {'stop': None}, 'batch_size': 1, 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-3.5-turbo', 'ls_model_type': 'chat', 'ls_temperature': 0.0}}, error=None, serialized={'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'gpt-3.5-turbo', 'temperature': 0.0, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_proxy': '', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI', 'graph': {'nodes': [{'id': 0, 'type': 'schema', 'data': 'ChatOpenAIInput'}, {'id': 1, 'type': 'runnable', 'data': {'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'name': 'ChatOpenAI'}}, {'id': 2, 'type': 'schema', 'data': 'ChatOpenAIOutput'}], 'edges': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]}}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 405110, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 25, 613610, tzinfo=datetime.timezone.utc)}], inputs={'messages': [[{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'Using the provided context, please answer the user\\'s question. If you don\\'t know the answer based on the context, say you don\\'t know.\\n\\nContext:\\n[Document(page_content=\"Editor\\'s Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We\\'re really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run\", metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'}), Document(page_content=\\'as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\\\\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\\\\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\\\\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for\\', metadata={\\'source\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'loc\\': \\'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/\\', \\'lastmod\\': \\'2023-08-17T22:10:38.000Z\\'})]\\n\\nQuestion:\\nWhat is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?\\n', 'type': 'human'}}]]}, outputs={'generations': [[{'text': 'The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', 'generation_info': {'finish_reason': 'stop', 'logprobs': None}, 'type': 'ChatGeneration', 'message': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'AIMessage'], 'kwargs': {'content': 'The main strength of Qdrant when used in combination with LangChain for LLM applications is its ability to consistently support the user beyond the prototyping and launch phases while maximizing resource usage and data connection. Qdrant only requires a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors, making it efficient and cost-effective for users.', 'response_metadata': {'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'id': 'run-b86400a6-9fa1-4061-8916-f0650695cd92-0', 'usage_metadata': {'input_tokens': 1464, 'output_tokens': 79, 'total_tokens': 1543}, 'tool_calls': [], 'invalid_tool_calls': []}}}]], 'llm_output': {'token_usage': {'completion_tokens': 79, 'prompt_tokens': 1464, 'total_tokens': 1543}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None}, 'run': None}, reference_example_id=None, parent_run_id=UUID('0ebe026f-508d-47d7-bffd-c7d39e349905'), tags=['seq:step:2'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002722090949Z0d22cbdb-a5ac-4837-8b98-e500a3656619.20240628T002722517310Z0ebe026f-508d-47d7-bffd-c7d39e349905.20240628T002723405110Zb86400a6-9fa1-4061-8916-f0650695cd92')], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002722090949Z0d22cbdb-a5ac-4837-8b98-e500a3656619.20240628T002722517310Z0ebe026f-508d-47d7-bffd-c7d39e349905'), Run(id=UUID('309027e3-44f3-4255-99d9-9e01fef0d446'), name='RunnableLambda', start_time=datetime.datetime(2024, 6, 28, 0, 27, 22, 653809, tzinfo=datetime.timezone.utc), run_type='chain', end_time=datetime.datetime(2024, 6, 28, 0, 27, 23, 96845, tzinfo=datetime.timezone.utc), extra={}, error=None, serialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_core', 'runnables', 'base', 'RunnableLambda'], 'repr': \"RunnableLambda(itemgetter('context'))\"}, events=[{'name': 'start', 'time': datetime.datetime(2024, 6, 28, 0, 27, 22, 653809, tzinfo=datetime.timezone.utc)}, {'name': 'end', 'time': datetime.datetime(2024, 6, 28, 0, 27, 23, 96845, tzinfo=datetime.timezone.utc)}], inputs={'context': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})], 'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'output': [Document(page_content=\"Editor's Note: This post was written by the Qdrant team and cross-posted from their blog. As more LLM applications move into production, speed, stability and costs are going to become even more important features of the LLM tech stack. And, as more LLM applications take advantage of RAG (and longterm memory), this becomes even more of a challenge. We're really excited about what Qdrant is doing to help with that–their async support is particularly helpful!LangChain currently supports 40+ vector stores, each offering their own features and capabilities. When it comes to crafting a prototype, some truly stellar options are at your disposal. However, while some may outshine others in terms of performance and suitability, selecting the best option for your application’s production scenario requires careful consideration.If you are looking to scale up and keep the same level of performance, Qdrant and LangChain are a rock-solid combination. Getting started with both is a breeze and the documentation covers a broad number of cases. However, the main strength of Qdrant is that it can consistently support the user way past the prototyping and launch phases. For example, you only need a maximum of 18GB RAM, and a minimum of 2GB to support 1 million OpenAI Vectors! This makes Qdrant the best vector store for maximizing resource usage and data connection.At its core, Qdrant vector database excels at semantic search. When supported by LangChain, Qdrant can help you set up effective QA systems, detection systems and chatbots that leverage Retrieval Augmented Generation (RAG) to its full potential. Qdrant streamlines the process of retrieval augmentation, making it faster, easier to scale and efficient. Adding relevant context to LLMs can vastly improve user experience especially in most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content=\"most business cases, where LLMs haven’t accessed such data before. Vector search is better at sorting through relevant context, when the available data is vast, at times in hundreds or thousands of documents.How Does Qdrant Work With LangChain?Qdrant vector database functions as long-term memory for AI models. As a vector store, it manages the efficient storage and retrieval of vectors, which represent user data.In terms of RAG, LangChain receives a query, dispatches it to a vector database such as Qdrant, retrieves relevant documents, and then sends both the query and the retrieved documents into the large language model to generate an answer.Augmenting your AI application with retrieval systems reduces hallucinations, a situation where AI models produce legitimate-sounding but made-up responses. When it comes to long-term memory storage for LLM applications, developers can easily add relevant documents, chat history memory & rich user data to LLM app prompts. Qdrant takes care of document and chat history storage, embedding, enrichment, and more.Optimizing Resource UseRetrieval Augmented Generation is not without its challenges and limitations. One of the main setbacks for app developers is managing the complexity of the model. The integration of a retriever and a generator into a single model can lead to a raised level of complexity, thus increasing the computational resources required.Qdrant’s is completely optimized for performance and continually adds new features that reduce the computational load required to run your application. In particular, Qdrant is the only vector store offered by LangChain that supports asynchronous operations. Qdrant supports full async API based on GRPC protocol.This functionality is available with our open source Qdrant vector database as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don't waste time waiting for responses from external services. Vector stores run\", metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'}), Document(page_content='as well as the Qdrant Cloud SaaS product. This causes performance benefits as applications maximize compute use and don\\'t waste time waiting for responses from external services. Vector stores run as separate services, which makes them I/O bound from the perspective of an LLM-based application. Using `async` lets you utilize the resources better, primarily if the LangChain is combined with an `async` framework, such as FastAPI. Using async API is easy - all the methods have their counterpart async definitions (similarity_search -> asimilarity_search, etc.). FastAPI describes asynchronous operations quite well in their documentation.The application doesn\\'t wait for I/O operations and that pays off when applications interact with external systems, such as any database. In this way, compute power does not sit idle and is used to its fullest potential. The implementation of io_uring is a testament to Qdrant’s focus on performance and resource usage. One of the great optimizations Qdrant offers is quantization (either scalar or product-based). Uring complements these by mitigating the use of disk IO, via improved async throughput wherever the OS syscall overhead gets too high, which tends to occur in situations where software becomes IO bound.What is Your Endgame?The wise adage of \"trying before buying\" holds true in the realm of vector store selection. With numerous options available on LangChain, it\\'s imperative to try whether this option fits your use case the best.The best way to get started is to sign up for our Qdrant Cloud Free Tier. Join the official Discord community for tech support and integration advice.“We are all-in on performance and reliability. Every release we make Qdrant faster, more stable and cost-effective for the user. When others focus on prototyping, we are already ready for production. Very soon, our users will build successful products and go to market. At this point, I anticipate a great need for a reliable vector store. Qdrant is there for', metadata={'source': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'loc': 'https://blog.langchain.dev/qdrant-x-langchain-endgame-performance/', 'lastmod': '2023-08-17T22:10:38.000Z'})]}, reference_example_id=None, parent_run_id=UUID('0d22cbdb-a5ac-4837-8b98-e500a3656619'), tags=['map:key:context'], child_runs=[], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002722090949Z0d22cbdb-a5ac-4837-8b98-e500a3656619.20240628T002722653809Z309027e3-44f3-4255-99d9-9e01fef0d446')], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3.20240628T002722090949Z0d22cbdb-a5ac-4837-8b98-e500a3656619')], trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7'), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7.20240628T002651252981Z81594625-4738-4743-9a51-9f5d80f800e3')], session_name='Parent Document Retrieval RAG Evaluation-11fe2fe7', session_id=None, client=Client (API URL: https://api.smith.langchain.com), dotted_order='20240628T002650996270Z61625cf4-3a4e-40c4-aa50-e458267649e7', trace_id=UUID('61625cf4-3a4e-40c4-aa50-e458267649e7')),\n",
              "  'example': Example(dataset_id=UUID('28966892-6a7d-4b2d-924f-65c67f75bbe4'), inputs={'question': 'What is the main strength of Qdrant when used in combination with LangChain for LLM applications, particularly concerning resource usage and data connection?'}, outputs={'answer': 'Qdrant’s main strength is its ability to consistently support users beyond the prototyping and launch phases, requiring only a maximum of 18GB RAM and a minimum of 2GB to support 1 million OpenAI Vectors.'}, metadata={'dataset_split': ['base']}, id=UUID('e64ebb5a-cc9d-43ff-a0cd-86220b01a819'), created_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 769249, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 6, 28, 0, 9, 21, 769249, tzinfo=datetime.timezone.utc), runs=[], source_run_id=None),\n",
              "  'evaluation_results': {'results': [EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bf9506a9-7dd5-4185-8ebf-9448aa36a5fa'), target_run_id=None),\n",
              "    EvaluationResult(key='dopeness', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c633931a-13db-46aa-9a86-5dde6a6ba80b'), target_run_id=None),\n",
              "    EvaluationResult(key='score_string:accuracy', score=10, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ed2b4c9c-68a5-4e34-8a79-6766ee1da99c'), target_run_id=None),\n",
              "    EvaluationResult(key='coherence', score=1, value='Y', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7a478624-f817-4789-869e-307b89ea104b'), target_run_id=None),\n",
              "    EvaluationResult(key='relevance', score=0, value='N', comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d71024f2-f510-4080-ac85-4980e5826911'), target_run_id=None)]}}]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdr_rag_results._results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing.\n",
        "\n",
        "`Performance comparison of retirevers across the following metrics as tracked via langsmith is as follows:`\n",
        "\n",
        "\n",
        "    1) coherence (Well structured and organized response): Base = Hybrid model  > PDR\n",
        "\n",
        "    2) cot contextual accuraccy (is the provided context being correctly referenced) : Base > Hybrid model  > PDR\n",
        "\n",
        "    3) dopeness (The criterion for this assessment is \"dopeness\", which is defined as being cool, awesome, and legit) : Hybrid model > PDR > Base\n",
        "\n",
        "    4) relevance (Is the answer hallucinated or real?): Base = Hybrid model  = PDR\n",
        "\n",
        "    5) accuracy (Is the generated answer the same as the reference answer): Hybrid model >= Base > PDR\n",
        "\n",
        "    6) P50Latency: PDR >> Hybrid > Base\n",
        "    \n",
        "    7) P99Latency: PDR >> Base > Hybrid\n",
        "\n",
        "`To balance out metrics improvement and latency, select the Hybrid model sacrificing some contextual referencing consistency and minor latency degradation with improvements in coherence, dopeness and accuracy.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02e958d09185453587b3fb6106c6a19d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11355ee61067447ab6f9f640f03e5f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c75ca5ee9d42eb8476a307f30b7528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142e5756a6c840aeb25b733ece015ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4777a17d7d9c444a8d44e070faef0ad0",
              "IPY_MODEL_dad645f3259c41b5a50e0e94906ca3e3",
              "IPY_MODEL_c67428191d074e4d80cb0f28fb65cd41"
            ],
            "layout": "IPY_MODEL_8db8082c5ab741b5b4fd78027e65135d"
          }
        },
        "1ad9f6314e06446286368195168e394d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0589c63fd1461b93b403c594b6ccfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e2bff47a084ed6bc7d96ac09af9ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "283761def08545f2a8edc6f3ab8a0d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab8452345f24b46b172515ddc77fdb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42bfd25ca7a14d629e4070b093a197d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0589c63fd1461b93b403c594b6ccfa",
            "placeholder": "​",
            "style": "IPY_MODEL_5e500672dafc4529806e4e5f72fe97fa",
            "value": ""
          }
        },
        "46a7a454e9bb486588ac33156abddecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4777a17d7d9c444a8d44e070faef0ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab8452345f24b46b172515ddc77fdb0",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ad942ac5ed41c6a581b4022f177114",
            "value": ""
          }
        },
        "48ea78bdc6f145f49315f01981af8706": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "533af6b3e80c4fb7a8a4fb3dd9efe2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11355ee61067447ab6f9f640f03e5f8d",
            "placeholder": "​",
            "style": "IPY_MODEL_283761def08545f2a8edc6f3ab8a0d47",
            "value": " 22/? [01:07&lt;00:00,  1.69s/it]"
          }
        },
        "5e500672dafc4529806e4e5f72fe97fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7448f6f5d9dc431fba689d8821285ca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db8082c5ab741b5b4fd78027e65135d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a179a48558994e3c94b1484cf1f7310f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b46fc6940b004268ac685e7b1d2c4ad6",
              "IPY_MODEL_df0dc6074ebb4cbcb887b1bd123491be",
              "IPY_MODEL_533af6b3e80c4fb7a8a4fb3dd9efe2a6"
            ],
            "layout": "IPY_MODEL_02e958d09185453587b3fb6106c6a19d"
          }
        },
        "a603cf6ec7d544a1a951fb8defccb976": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f87ac933b45fa9e2c17871d4f2e44",
            "placeholder": "​",
            "style": "IPY_MODEL_a888bb92387549fea8843f20e3b4c50f",
            "value": " 22/? [01:19&lt;00:00,  2.61s/it]"
          }
        },
        "a888bb92387549fea8843f20e3b4c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8afc185bfc245f58ba2388559ca69c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2f76d283333446abcbf4ceb7f761f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b46fc6940b004268ac685e7b1d2c4ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ad9f6314e06446286368195168e394d",
            "placeholder": "​",
            "style": "IPY_MODEL_a8afc185bfc245f58ba2388559ca69c8",
            "value": ""
          }
        },
        "c67428191d074e4d80cb0f28fb65cd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c75ca5ee9d42eb8476a307f30b7528",
            "placeholder": "​",
            "style": "IPY_MODEL_e8143f8ceee6468c98433734ff59b835",
            "value": " 22/? [01:16&lt;00:00,  2.24s/it]"
          }
        },
        "d097ee02256d40d5b395d37cd0face05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ad942ac5ed41c6a581b4022f177114": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad645f3259c41b5a50e0e94906ca3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e2bff47a084ed6bc7d96ac09af9ebc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d097ee02256d40d5b395d37cd0face05",
            "value": 1
          }
        },
        "df0dc6074ebb4cbcb887b1bd123491be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ea78bdc6f145f49315f01981af8706",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2f76d283333446abcbf4ceb7f761f44",
            "value": 1
          }
        },
        "e8143f8ceee6468c98433734ff59b835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99b5317081e42c199a297f3a483bbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42bfd25ca7a14d629e4070b093a197d5",
              "IPY_MODEL_fa9f9fc79b344271adc94acc487aef59",
              "IPY_MODEL_a603cf6ec7d544a1a951fb8defccb976"
            ],
            "layout": "IPY_MODEL_7448f6f5d9dc431fba689d8821285ca3"
          }
        },
        "ece363bdc8b54631beba2628e3175e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f79f87ac933b45fa9e2c17871d4f2e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9f9fc79b344271adc94acc487aef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece363bdc8b54631beba2628e3175e0b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46a7a454e9bb486588ac33156abddecc",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
